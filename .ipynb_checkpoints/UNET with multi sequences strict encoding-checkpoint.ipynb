{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Unet CNN Pixel classification model for grouping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version of the UNET model uses the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import timeit\n",
    "import statistics\n",
    "import time\n",
    "import torch\n",
    "import torch_geometric\n",
    "import importlib\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "\n",
    "from data_utils import synthetic_data\n",
    "from data_utils import graph_constructors\n",
    "from data_utils import group_to_image_constructors\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import importlib\n",
    "\n",
    "from Unet import helper\n",
    "from Unet import simulation\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from torchvision import transforms, datasets, models\n",
    "import torchvision.utils\n",
    "\n",
    "from Unet import loss as Unet_loss\n",
    "\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'data_utils.group_to_image_constructors' from '/home/a/Documents/GNN-for-trans-grouping/GNN-for-trans-grouping/data_utils/group_to_image_constructors.py'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(synthetic_data)\n",
    "importlib.reload(graph_constructors)\n",
    "importlib.reload(group_to_image_constructors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "colour_list = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'tab:brown', 'tab:pink', 'tab:gray', 'tab:olive', 'tab:cyan']\n",
    "\n",
    "for i in range(2):\n",
    "    d_lst, a_lst, g_lst, t_lst = synthetic_data.make_a_group()\n",
    "    \n",
    "    d_arr = np.array(d_lst)\n",
    "    a_arr = np.array(a_lst)\n",
    "    g_arr = np.array(g_lst)    \n",
    "    \n",
    "    fig, (ax1) = plt.subplots(nrows=1, ncols=1, figsize=(10,8), sharex=True)\n",
    "    for g in g_lst:\n",
    "        mask = (g_arr == g)\n",
    "        \n",
    "        ax1.scatter(d_arr[mask], a_arr[mask], s=10, c=colour_list[g%10], marker='x')\n",
    "        ax1.set_title(str(i))\n",
    "        #ax1.legend(loc=\"upper right\")\n",
    "    \n",
    "    for ax1 in fig.axes:\n",
    "        matplotlib.pyplot.sca(ax1)\n",
    "        plt.xticks(rotation=90)\n",
    "        #plt.legend(bbox_to_anchor=(1.02, 1), loc=2, borderaxespad=0.)\n",
    "    \n",
    "    plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_lst, a_lst, g_lst, t_lst = synthetic_data.make_a_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "223"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_lst = []\n",
    "\n",
    "for _ in range(1000):\n",
    "    d_lst, a_lst, g_lst, t_lst = synthetic_data.make_a_group()\n",
    "    max_lst = max_lst+ [max(d_lst)]\n",
    "    \n",
    "max(max_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_arr = np.array(d_lst)\n",
    "a_arr = np.array(a_lst)\n",
    "g_arr = np.array(g_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_a_arr = graph_constructors.normalise_amounts(a_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unet demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 192, 192, 3)\n",
      "0 255\n",
      "(3, 6, 192, 192)\n",
      "0.0 1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdsAAAKvCAYAAAAiIWV+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3df4xddZ3/8ddr28VkWTaADKTfwmyBVFwxu6PcdCUEA7JqIcTCGt0232hXyQ4kkOx+d7+JIN+sZjckfl2RxLiiQ2havtEC2kUb0l1piF9hN7Aw1VqLUGmxyrRNO1CjZDH4Lby/f8wZPQx35v445zPnxzwfyc2993PPued9ZubTVz+fe+45jggBAIB0fqfqAgAAaDvCFgCAxAhbAAASI2wBAEiMsAUAIDHCFgCAxJKFre21tvfZ3m/75lTbAQCg7pzie7a2l0n6saT3SpqS9KSkDRHxo9I3BgBAzaUa2a6RtD8inouIX0u6V9K6RNsCAKDWlid635WSns89n5L0p/MtbJvTWGEpeyEiRqouoixnnHFGrFq1quoygErs2rWra39OFbbu0va6QLU9Lmk80faBJvlp1QUUle/Po6OjmpycrLgioBq2u/bnVNPIU5LOyT0/W9Lh/AIRMRERnYjoJKoBwCLJ9+eRkdYM0oHSpArbJyWttn2u7ZMkrZe0PdG2AACotSTTyBFxwvZNkr4taZmkTRHxVIptAQBQd6k+s1VE7JC0I9X7AwDQFJxBCgCAxAhbAAASI2wBAEiMsAUAIDHCFgCAxAhbAAASI2wBAEiMsAUAILFkJ7UAyjbMtZftbtfEAFC1Jy65fOB11vzHdxJUsjgY2QIAkBhhCwBAYoQtAACJEbYAACRG2AIAkBhhCwBAYoQtAACJDR22ts+x/R3bT9t+yvZfZ+2ftn3I9u7sdlV55QIA0DxFTmpxQtLfRcT3bJ8iaZftndlrd0TE54qXBwBA8w0dthFxRNKR7PFLtp+WtLKswgAAaItSPrO1vUrSOyT9Z9Z0k+09tjfZPq2MbQAA0FSFw9b270vaJulvIuKXku6UdL6kMc2MfG+fZ71x25O2J4vWAKBa+f48PT1ddTlA7XiYk7v/ZmX7dyU9KOnbEfH5Lq+vkvRgRLy9x/sMXwTQfLsiolN1EWXpdDoxOcn/obE02e7an4scjWxJd0t6Oh+0tlfkFrtW0t5htwEAQBsUORr5EkkfkfRD27uztk9K2mB7TFJIOijp+kIVAgDQcEWORv53Sd0uFrpj+HIAAGgfziAFAEBihC0AAIkRtgAAJEbYAgCQGGELAEBihC0AAIkRtgAAJEbYAgCQGGELAEBihC0AAIm1NmwjQkWuaASgPrY8erG2PHpx1WUAQ2tt2AIAUBeELQAAiRG2AAAkRtgCAJAYYQsAQGJDXzx+lu2Dkl6S9KqkExHRsX26pPskrZJ0UNKHI+LnRbcFAEATlTWyvTwixiKikz2/WdLDEbFa0sPZcwAAlqRU08jrJG3JHm+RdE2i7QAAUHtlhG1Iesj2LtvjWdtZEXFEkrL7M0vYDgAAjVT4M1tJl0TEYdtnStpp+5l+VsqCebzngnMMelaofpe3PWgpADL5/jw6Otr3eoOeFarf5Tde+thA7wukVnhkGxGHs/tjkh6QtEbSUdsrJCm7P9ZlvYmI6OQ+5wXQUPn+PDIyUnU5QO0UGtnaPlnS70TES9nj90n6B0nbJW2U9Jns/ltFC81ts6/lZke0jFiB+up3BDo7omXEiqYqOo18lqQHskBbLulrEfFvtp+UdL/t6yT9TNKHCm4HAIDGKhS2EfGcpD/p0v6ipCuKvDcAAG3BGaQAAEiMsAUAIDHCFgCAxAhbAAASI2wBAEiMsAUAIDHCFgCAxMo4N3ItceYooD04cxSajpEtAACJEbYAACRG2AIAkBhhCwBAYoQtAACJEbYAACRG2AIAkBhhCwBAYkOf1ML2BZLuyzWdJ+nvJZ0q6a8kTWftn4yIHUNXCABAww0dthGxT9KYJNleJumQpAckfUzSHRHxuVIqBACg4cqaRr5C0oGI+GlJ7wcAQGuUFbbrJW3NPb/J9h7bm2yfVtI2AABopMJha/skSR+Q9PWs6U5J52tmivmIpNvnWW/c9qTtyaI1AKhWvj9PT0/3XgFYYsoY2V4p6XsRcVSSIuJoRLwaEa9JukvSmm4rRcRERHQiolNCDQAqlO/PIyMjVZcD1E4ZYbtBuSlk2ytyr10raW8J2wAAoLEKXc/W9u9Jeq+k63PNn7U9JikkHZzzGgAAS06hsI2IlyW9eU7bRwpVhMaJCNmuugwAJfBXrlZc/2DVZbQOZ5BCKSKi6hIAlMRfubrqElqHsEVpCFygPQjcchG2KBWBC7QHgVsewhalI3CB9iBwy0HYIgkCF2gPArc4whbJELhAexC4xRC2SIrABdqDwB0eYYvkCFygPQjc4RC2WBQELtAeBO7gCFssGgIXaA8CdzCELRYVgQu0B4HbP8IWi47ABdqDwO0PYYtKELhAexC4vRG2qAyBC7QHgbswwhaVInCB9iBw50fYonIELtAeBG53fYWt7U22j9nem2s73fZO289m96dl7bb9Bdv7be+x/c5UxaM9CFygPQjcN+p3ZLtZ0to5bTdLejgiVkt6OHsuSVdKWp3dxiXdWbxMLAUELtAeBO7r9RW2EfGIpONzmtdJ2pI93iLpmlz7PTHjcUmn2l5RRrFoPwIXaA8C97eKfGZ7VkQckaTs/sysfaWk53PLTWVtQF8IXKA9CNwZKQ6Qcpe2N/zraXvc9qTtyQQ1oOEI3GbJ9+fp6emqy0HNELjS8gLrHrW9IiKOZNPEx7L2KUnn5JY7W9LhuStHxISkCUmyzb+sDWZ3+/8VlpJ8f+50OvTnBovrH6y6hFYqMrLdLmlj9nijpG/l2j+aHZX8Lkm/mJ1uBgBgKeprZGt7q6TLJJ1he0rSpyR9RtL9tq+T9DNJH8oW3yHpKkn7Jb0s6WMl1wwAQKP0FbYRsWGel67osmxIurFIUQAAtAlnkAIAIDHCFgCAxAhbAAASI2wBAEisyPdsMaB+TtTAd1aBZnhpd++xyiljry1CJWgCRraLpN8zInHmJKD++gnaQZZD+/GXsAgGDVACF6ivQQOUwIVE2CY3bHASuED9DBucBC74C0ioaGASuEB9FA1MAndp47cPAEBihC0AAIkRtgAAJEbYAgCQGGELAEBihC0AAIkRtgAAJNYzbG1vsn3M9t5c2z/Zfsb2HtsP2D41a19l+1e2d2e3L6csHgCAJuhnZLtZ0to5bTslvT0i/ljSjyXdknvtQESMZbcbyimzmYpeVICLEgD1UfSiAlyUYGnrGbYR8Yik43PaHoqIE9nTxyWdnaC2Vhg2MAlaoH6GDUyCFmV8ZvtxSf+ae36u7e/b/q7tS0t4/8YbNDgJWqC+Bg1OghZSwevZ2r5V0glJX82ajkgajYgXbV8k6Zu2L4yIX3ZZd1zSeJHtN4ntga9nO8i5kQloVCnfn0dHRyuuJr1Txl4b+Hq249sO9f3+Ex9cOVRdqC/3GQCrJD0YEW/PtW2UdIOkKyLi5XnW+7+S/mdETPZ4f864n1PkAgSEbiPtiohO1UWUpdPpxOTkgl1+SRkkZOcidJvHdtf+PNQ0su21kj4h6QP5oLU9YntZ9vg8SaslPTdcyUsTVwoC2qNI0JaxPuqjn6/+bJX0mKQLbE/Zvk7SFyWdImnnnK/4vFvSHts/kPQNSTdExPGub4w3KCsoCVygemUFJYHbDn1NIycvgmnkBQNyoanhYddDrTCN3DILBeRCU8PDrof6KHUaGeWaLzBt9wzMhZapw3+kgKVmvsCc+ODKnoG50DKMcJuNsK3YQkE7CAIXqN5CQTsIArd9CNsa4kQYQHsMO/3LtHG7ELYV6jbqTHGKR0a3QHrdRp1FA7Pb+oxum4mwBQAgMcK2RsqaBmY6GaheWdPATCe3A2ELAEBihC0AAIkRtgAAJEbYAgCQWKFL7LVBRPzm8ncL3QOot4vPv63vZR87cGvCSoA3WvIj29kg7XUPAMCwlnzYzp7wodf9YtZSl/cBMDyu+oO8JR+2jGwBAKkt+bCtcmSb4tSKKU4BCaC3FKdWTHEKSFRjyYdtHUe2wwYu08dA/QwbuEwft0vPsLW9yfYx23tzbZ+2fcj27ux2Ve61W2zvt73P9vtTFV6Wqj+zLevSeGVdqg/A8Mq6NF5Zl+pDffQzst0saW2X9jsiYiy77ZAk22+TtF7Shdk6X7K9rKxiU6jDyHahwO0VugstQ9ACi2+hwO0VugstQ9A2W8/v2UbEI7ZX9fl+6yTdGxGvSPqJ7f2S1kh6bOgKE6vL92xntzVfjcO8H4BqTHxw5byhOcz0MEHbfEU+s73J9p5smvm0rG2lpOdzy0xlbbVVh5Ht3Frq8j4AhsdVf5A3bNjeKel8SWOSjki6PWvv9q9812GZ7XHbk7Ynh6yhFFV/ZjtXiovHA6nl+/P09HTV5dRGiovHo5mGOl1jRBydfWz7LkkPZk+nJJ2TW/RsSYfneY8JSRPZe1R2GG2dRrZzaxok6AlZVCnfnzudTiX9ua6nYJwNzEGmjwnZ9hkqbG2viIgj2dNrJc0eqbxd0tdsf17Sf5O0WtIThatcoghQoD0I0KWtZ9ja3irpMkln2J6S9ClJl9ke08wU8UFJ10tSRDxl+35JP5J0QtKNEfFqmtIBAGiGfo5G3tCl+e4Flr9NUv+X3wAAoOWW/BmkAABIjbAFACAxwhYAgMQIWwAAEiNsAQBIjLAFACAxwhYAgMQIWwAAEiNsAQBIjLAFACAxwhYAgMQIWwAAEiNsAQBIjLAFACAxwhYAgMQIWwAAEusZtrY32T5me2+u7T7bu7PbQdu7s/ZVtn+Ve+3LKYsHAKAJlvexzGZJX5R0z2xDRPzF7GPbt0v6RW75AxExVlaBAAA0Xc+wjYhHbK/q9pptS/qwpPeUWxYAAO1R9DPbSyUdjYhnc23n2v6+7e/avrTg+wMA0Hj9TCMvZIOkrbnnRySNRsSLti+S9E3bF0bEL+euaHtc0njB7QOogXx/Hh0drbgaoH6GHtnaXi7pzyXdN9sWEa9ExIvZ412SDkh6S7f1I2IiIjoR0Rm2BgD1kO/PIyMjVZcD1E6RaeQ/k/RMREzNNtgesb0se3yepNWSnitWIgAAzdbPV3+2SnpM0gW2p2xfl720Xq+fQpakd0vaY/sHkr4h6YaIOF5mwQAANE0/RyNvmKf9L7u0bZO0rXhZAAC0B2eQAgAgMcIWAIDECFsAABIjbAEASIywBQAgMcIWAIDECFsAABIjbAEASIywBQAgMcIWAIDECFsAABJzRFRdg2xPS/ovSS9UXUsJzhD7USdN2I8/jIjWXJfO9kuS9lVdRwma8LfTD/ZjcXXtz7UIW0myPdmGa9uyH/XSlv1okrb8zNmPemn6fjCNDABAYoQtAACJ1SlsJ6ouoCTsR720ZT+apC0/c/ajXhq9H7X5zBYAgLaq08gWAIBWImwBAEiMsAUAIDHCFgCAxAhbAAASI2wBAEiMsAUAIDHCFgCAxAhbAAASI2wBAEiMsAUAIDHCFgCAxAhbAAASI2wBAEiMsAUAIDHCFgCAxAhbAAASI2wBAEiMsAUAIDHCFgCAxAhbAAASI2wBAEiMsAUAIDHCFgCAxAhbAAASI2wBAEiMsAUAIDHCFgCAxAhbAAASSxa2ttfa3md7v+2bU20HAIC6c0SU/6b2Mkk/lvReSVOSnpS0ISJ+VPrGAACouVQj2zWS9kfEcxHxa0n3SlqXaFsAANRaqrBdKen53POprA0AgCVneaL3dZe2181X2x6XNJ49vShRHUATvBARI1UXUUS+P5988skXvfWtb624IqAau3bt6tqfU4XtlKRzcs/PlnQ4v0BETEiakCTb5X9wDDTHT6suoKh8f+50OjE5OVlxRUA1bHftz6mmkZ+UtNr2ubZPkrRe0vZE2wIAoNaSjGwj4oTtmyR9W9IySZsi4qkU2wIAoO5STSMrInZI2pHq/QEAaArOIAUAQGKELQAAiRG2AAAkRtgCAJAYYQsAQGKELQAAiRG2AAAkRtgCAJAYYQsAQGKELQAAiRG2AAAkRtgCAJAYYQsAQGKELQAAiRG2AAAkRtgCAJDY0GFr+xzb37H9tO2nbP911v5p24ds785uV5VXLgAAzbO8wLonJP1dRHzP9imSdtnemb12R0R8rnh5AAA039BhGxFHJB3JHr9k+2lJK8sqDACAtijlM1vbqyS9Q9J/Zk032d5je5Pt08rYBgAATVU4bG3/vqRtkv4mIn4p6U5J50sa08zI9/Z51hu3PWl7smgNAKqV78/T09NVlwPUTqGwtf27mgnar0bEv0hSRByNiFcj4jVJd0la023diJiIiE5EdIrUAKB6+f48MjJSdTlA7RQ5GtmS7pb0dER8Pte+IrfYtZL2Dl8eAADNV+Ro5EskfUTSD23vzto+KWmD7TFJIemgpOsLVQgAQMMVORr53yW5y0s7hi8HAID24QxSAAAkRtgCAJAYYQsAQGKELQAAiRG2AAAkRtgCAJAYYQsAQGKELQAAiRG2AAAkRtgCAJAYYQsAQGKELQAAiRG2AAAkRtgCAJAYYQsAQGKELQAAiQ198fhZtg9KeknSq5JORETH9umS7pO0StJBSR+OiJ8X3RYAAE1U1sj28ogYi4hO9vxmSQ9HxGpJD2fPAQBYklJNI6+TtCV7vEXSNYm2AwBA7ZURtiHpIdu7bI9nbWdFxBFJyu7PLGE7AAA0UuHPbCVdEhGHbZ8paaftZ/pZKQvm8Z4LAqi9fH8eHR2tuBqgfgqPbCPicHZ/TNIDktZIOmp7hSRl98e6rDcREZ3c57wAGirfn0dGRqouB6idQmFr+2Tbp8w+lvQ+SXslbZe0MVtso6RvFdkOAABNVnQa+SxJD9iefa+vRcS/2X5S0v22r5P0M0kfKrgdAAAaq1DYRsRzkv6kS/uLkq4o8t4AALQFZ5ACACAxwhYAgMQIWwAAEiNsAQBIjLAFACAxwhYAgMQIWwAAEiNsAQBIjLAFACAxwhYAgMQIWwAAEiNsAQBIjLAFACAxwhYAgMQIWwAAEiNsAQBIbOiLx9u+QNJ9uabzJP29pFMl/ZWk6az9kxGxY+gKAQBouKHDNiL2SRqTJNvLJB2S9ICkj0m6IyI+V0qFAAA0XFnTyFdIOhARPy3p/QAAaI2ywna9pK255zfZ3mN7k+3TStoGAACNVDhsbZ8k6QOSvp413SnpfM1MMR+RdPs8643bnrQ9WbQGANXK9+fp6eneKwBLTBkj2yslfS8ijkpSRByNiFcj4jVJd0la022liJiIiE5EdEqoAUCF8v15ZGSk6nKA2ikjbDcoN4Vse0XutWsl7S1hGwAANNbQRyNLku3fk/ReSdfnmj9re0xSSDo457VkImLe12wvRgkASjK+7dC8r018cOUiVgKUo1DYRsTLkt48p+0jhSoavIa+lyF0Z34Wtvu+BxbTQiE7d5mlHroXn3/bwOs8duDWBJWgH4XCtmpzg7ZbOOSXIUB++zPq9x5YLHODtluY5pcZ33ZoyQcumqM1p2ucLxwIjdeb/c9Hv/dAFeYLUcIVTdXYsM2HQa9Azb++1EOEkS3qKD9i7RWo+df7mXYG6qCxYTur31AgPGYwskWd9TtyZYSLpml82GIwjGwBYPERtksMI1sAWHyE7RLDyBYAFh9hu8QwsgWAxdf4sO03FAiPGYxsUWf9Hl3MUchomsaG7SBf5xnka0Jtx8gWdTTI13kG+ZoQUBeNDdu55gsHQuP1GNmiCeYLXEa0aKpGn65x9hy+s3oFKwHCuZFRXxMfXPmG0zH2Wh5oikaHrfTbAOWqP/1hZIs6mw1QrvrTGxcVaJbGh+0swgFoDwIVbdOaz2wBAKgrwhYAgMT6Clvbm2wfs70313a67Z22n83uT8vabfsLtvfb3mP7namKBwCgCfod2W6WtHZO282SHo6I1ZIezp5L0pWSVme3cUl3Fi8TAIDm6itsI+IRScfnNK+TtCV7vEXSNbn2e2LG45JOtb2ijGIBAGiiIp/ZnhURRyQpuz8za18p6fncclNZGwAAS1KKA6S6fQfnDV+CtT1ue9L2ZIIaACyifH+enp6uuhygdoqE7dHZ6eHs/ljWPiXpnNxyZ0s6PHfliJiIiE5EdArUAKAG8v15ZGSk6nKA2ikSttslbcweb5T0rVz7R7Ojkt8l6Rez080AACxFfZ1ByvZWSZdJOsP2lKRPSfqMpPttXyfpZ5I+lC2+Q9JVkvZLelnSx0quGQCARukrbCNiwzwvXdFl2ZB0Y5GiAABoE84gBQBAYoQtAACJEbYAACRG2AIAkBhhCwBAYoQtAACJEbYAACRG2JYkIjTzFWMATbfl0Yu15dGLqy4DLULYAgCQGGELAEBihC0AAIkRtgAAJEbYAgCQGGELAEBihC0AAIkRtgAAJNYzbG1vsn3M9t5c2z/Zfsb2HtsP2D41a19l+1e2d2e3L6csHgCAJljexzKbJX1R0j25tp2SbomIE7b/t6RbJH0ie+1ARIyVWmWFBj0rVL/L2x6mHAAFDHpWqH6X33jpY8OUgyWk58g2Ih6RdHxO20MRcSJ7+riksxPUBgBAK/Qzsu3l45Luyz0/1/b3Jf1S0v+KiEdL2EZl+h2Bzo5oGbEC9dXvCHR2RMuIFWUpFLa2b5V0QtJXs6YjkkYj4kXbF0n6pu0LI+KXXdYdlzReZPsA6iHfn0dHRyuuBqifocPW9kZJV0u6IrJhXUS8IumV7PEu2wckvUXS5Nz1I2JC0kT2XrW/XA5X9AHml+/PnU6n9p3lpd0Lf4L2Ryet1NO/5tMxlGeor/7YXquZA6I+EBEv59pHbC/LHp8nabWk58ootEoELdAevYJ21h+dNJW4Eiwl/Xz1Z6ukxyRdYHvK9nWaOTr5FEk753zF592S9tj+gaRvSLohIo53feOGSHU0MoDF12/QDrs8MJ+e08gRsaFL893zLLtN0raiRdXFsMEZERwoBdTMsMH50u7f0Sljr5VcDZYa/ts2j6IjVEa4QH0UHaEywkVR/AUBAJAYYQsAQGKELQAAiZVxBikAaJU/etOhqktAyzCyBQAgMcIWAIDECFsAABIjbAEASIywnUfRM0BxBimgPoqeAYozSKEownYBwwYmQQvUz7CBSdCiDIRtD4MGJ0EL1NegwUnQoiyEbR/6DVCCFqi/fgOUoEWZOKlFnwjSduIKTUsTQdpO/srViusfrLqMrhjZYsnjCk1Ae/grV1ddQleELSACF2iTOgZuz7C1vcn2Mdt7c22ftn3I9u7sdlXutVts77e9z/b7UxUOlI3ABdqjboHbz8h2s6S1XdrviIix7LZDkmy/TdJ6SRdm63zJ9rKyigVSI3CB9qhT4PYM24h4RNLxPt9vnaR7I+KViPiJpP2S1hSoD1h0BC7QHnUJ3CKf2d5ke082zXxa1rZS0vO5ZaayNqBRCFygPeoQuMN+9edOSf8oKbL72yV9XFK371B0/VfL9rik8SG3j4TKCJo2fJ2GrwX1L9+fR0dHK64GeU9ccnnh91jzH98poZJqVf21oKFGthFxNCJejYjXJN2l304VT0k6J7fo2ZIOz/MeExHRiYjOMDUAi4ERbn/y/XlkZKTqcoCuqhzhDhW2tlfknl4rafZI5e2S1tt+k+1zJa2W9ESxEoFqEbhAe1QVuD2nkW1vlXSZpDNsT0n6lKTLbI9pZor4oKTrJSkinrJ9v6QfSToh6caIeDVN6cDiYUoZaI8qppR7hm1EbOjSfPcCy98m6bYiRQF1ROAC7bHYgcsZpIABMKUMtMdiTikTtsCACFygPRYrcAlbYAgELtAeixG4hC0wJAIXaI/UgUvYAgUQuEB7pAxcLh6PJY2ji4H2qOuF4yVGtgAAJEfYAgCQGGELAEBihC0AAIkRtgAAJEbYAgCQGF/9wRvwdRigPdpw4fc2YGQLAEBihC0AAIkRtgAAJNYzbG1vsn3M9t5c2322d2e3g7Z3Z+2rbP8q99qXUxYPAEAT9HOA1GZJX5R0z2xDRPzF7GPbt0v6RW75AxExVlaBAAA0Xc+wjYhHbK/q9ppnDlv9sKT3lFsWAADtUfQz20slHY2IZ3Nt59r+vu3v2r604PsDANB4Rb9nu0HS1tzzI5JGI+JF2xdJ+qbtCyPil3NXtD0uabzg9gHUQL4/j46OVlwNUD9Dj2xtL5f055Lum22LiFci4sXs8S5JByS9pdv6ETEREZ2I6AxbA4B6yPfnkZGRqssBaqfINPKfSXomIqZmG2yP2F6WPT5P0mpJzxUrEQCAZuvnqz9bJT0m6QLbU7avy15ar9dPIUvSuyXtsf0DSd+QdENEHC+zYAAAmqafo5E3zNP+l13atknaVrwsAADagzNIAQCQGGELAEBihC0AAIkRtgAAJEbYAgCQGGELAEBihC0AAIkRtgAAJEbYAgCQGGELAEBihC0AAIkRtgAAJOaIqLoG2Z6W9F+SXqi6lhKcIfajTpqwH38YEa25CKztlyTtq7qOEjThb6cf7Mfi6tqfaxG2kmR7sg0Xkmc/6qUt+9EkbfmZsx/10vT9YBoZAIDECFsAABKrU9hOVF1ASdiPemnLfjRJW37m7Ee9NHo/avOZLQAAbVWnkS0AAK1E2AIAkBhhCwBAYoQtAACJEbYAACRG2AIAkBhhCwBAYoQtAACJEbYAACRG2AIAkBhhCwBAYoQtAACJEbYAACRG2AIAkBhhCwBAYoQtAACJEbYAACRG2AIAkBhhCwBAYoQtAACJEbYAACRG2AIAkBhhCwBAYoQtAACJEbYAACRG2AIAkBhhCwBAYoQtAACJEbYAACSWLGxtr7W9z/Z+2zen2g4AAHXniCj/Te1lkn4s6b2SpiQ9KWlDRPyo9I0BAFBzqUa2ayTtj4jnIuLXku6VtC7RtgAAqLXlid53paTnc8+nJP1pfgHb45LGs6cXJaoDaIIXImKk6iKKyPfnk08++aK3vvWtFVcEVGPXrl1d+3OqsHWXttfNV0fEhKQJSbJd/lw20Bw/rbqAovL9udPpxOTkZMUVAdWw3bU/p0tEwf0AABE0SURBVJpGnpJ0Tu752ZIOJ9oWAAC1lipsn5S02va5tk+StF7S9kTbAgCg1pJMI0fECds3Sfq2pGWSNkXEUym2BQBA3aX6zFYRsUPSjlTvDwBAU3AGKQAAEiNsAQBIjLAFACAxwhYAgMQIWwAAEiNsAQBIjLAFACAxwhYAgMQIWwAAEiNsAQBIjLAFACAxwhYAgMQIWwAAEiNsAQBIjLAFACAxwhYAgMSGDlvb59j+ju2nbT9l+6+z9k/bPmR7d3a7qrxyAQBonuUF1j0h6e8i4nu2T5G0y/bO7LU7IuJzxcvDICKi5zK2F6ESAEWNbzvUc5mJD65chEpQhqHDNiKOSDqSPX7J9tOS+M1XoJ+QnbssoQvUUz8hO3dZQrf+ioxsf8P2KknvkPSfki6RdJPtj0qa1Mzo9+dd1hmXNF7G9peqQUJ2vnUJXZQh359HR0crrqaZBgnZ+dYldOvLRf7BliTbvy/pu5Jui4h/sX2WpBckhaR/lLQiIj7e4z2KFbEE9fq95UN0kGVRiV0R0am6iLJ0Op2YnJysuoxG6RW0+RAdZFksPttd+3Ohka3t35W0TdJXI+JfJCkijuZev0vSg0W2gTeaLzznC81ewRsRBC5QkfnCc77Q7BW849sOEbg1VORoZEu6W9LTEfH5XPuK3GLXSto7fHnoV79hSagC9ddvWBKqzVHke7aXSPqIpPfM+ZrPZ23/0PYeSZdL+h9lFIoZ3UamgwZot+WLfpwAYHDdRqaDBmi35Yt8/os0ihyN/O+Suv0rv2P4cjCoYUeqtglYoGaGHalOfHAlAVtznEGqQeaGY9Ep4bnrE77A4pkbjkWnhOeuT/jWC2ELAEBihC0AAIkRtg1V1lHFHJ0MVK+so4o5Orm+CFsAABIjbAEASIywbaiyjhzmCGSgemUdOcwRyPVF2AIAkBhhCwBAYoRtg5R9EoqyT5IBoH9ln4Si7JNkoFyEbcMNG7h8VgvUz7CBy2e19UfYNkwZFxEo42IGAIor4yICZVzMAOkRti3Rb+AyogXqr9/AZUTbHIUuHo9qzHfFnnxbrwvGz30/ANWY74o9+bZeF4yf+36oH8K2oXpdIq/fESxBC1Sv1yXy+h3BErT1Rdg22GxQDjM1TMgC9TIblMNMDROy9Vc4bG0flPSSpFclnYiIju3TJd0naZWkg5I+HBE/L7otdDdI6BKyQL0NErqEbHOUNbK9PCJeyD2/WdLDEfEZ2zdnzz9R0rYwD4IUaA+CtF1SHY28TtKW7PEWSdck2g4AALVXRtiGpIds77I9nrWdFRFHJCm7P3PuSrbHbU/aniyhBgAVyvfn6enpqssBaqeMaeRLIuKw7TMl7bT9TD8rRcSEpAlJss2XP4EGy/fnTqdDfwbmKDyyjYjD2f0xSQ9IWiPpqO0VkpTdHyu6HQAAmqpQ2No+2fYps48lvU/SXknbJW3MFtso6VtFtgMAQJMVnUY+S9ID2VGwyyV9LSL+zfaTku63fZ2kn0n6UMHtAADQWIXCNiKek/QnXdpflHRFkfcGAKAtuBABAACJEbYAACRG2AIAkBgXIughIn5zhZ1B7gHUz8Xn3zbwOo8duDVBJVhqGNn2MBucg94DADCLsO1h9ko6g94DADCLsO2BkS0AoCjCtgdGtgCAogjbHhjZAgCKImx7YGQLACiKsO2BkS0AoCjCtgdGtgCAogjbHhjZAgCKImx7YGQLACiKsO2h6SPbiOA/AHPw80BTbXn0Ym159OKqy6gVf+XqqkvoC2HbAyPbduL3BLRHEwJ36AsR2L5A0n25pvMk/b2kUyX9laTprP2TEbFj6Aor1vSRLebHRSOWHi4q0F7+ytWK6x+suox5DT2yjYh9ETEWEWOSLpL0sqQHspfvmH2tyUGL9mOEC7RHnUe4ZU0jXyHpQET8tKT3AxYNgQu0R10Dt6ywXS9pa+75Tbb32N5k+7RuK9getz1pe7KkGoChEbjF5Pvz9PR07xWAhOoYuIXD1vZJkj4g6etZ052Szpc0JumIpNu7rRcRExHRiYhO0RqAMhC4w8v355GRkarLAWoXuEMfIJVzpaTvRcRRSZq9lyTbd0mq7yfWLdFPSCy0DAcJ/RYHTaFq/Xy1Z6FlNl76WJnlNFqdDpoqYxp5g3JTyLZX5F67VtLeErYBLBpGuEB71GWEW2hka/v3JL1X0vW55s/aHpMUkg7OeQ0JLDQSmw0ORmuDYYSLqiw0Mp0d0TJ6HUwdRriFwjYiXpb05jltHylUEVATBC7QHlUHLmeQAhbAlDLQHlVOKRO2QA8ELtAeVQUuYQv0gcAF2qOKwCVsgT4RuEB7LHbgErbAAAhcoD0WM3DLOKkFaoyjacvHUcqoCl/5Kd9iHaXMyBYYAiNcoD0WY4RL2AJDInCB9kgduIQtUACBC7RHysAlbIGCCFygPVIFLmELlIDABdojReByNDKWHI4kBtqj6gsM9IuRLQAAiRG2AAAkRtgCAJAYYQsAQGJ9ha3tTbaP2d6bazvd9k7bz2b3p2Xttv0F2/tt77H9zlTFAwDQBP2ObDdLWjun7WZJD0fEakkPZ88l6UpJq7PbuKQ7i5cJAEBz9RW2EfGIpONzmtdJ2pI93iLpmlz7PTHjcUmn2l5RRrEAADRRkc9sz4qII5KU3Z+Zta+U9Hxuuams7XVsj9uetD1ZoAYANZDvz9PT01WXA9ROigOkup0x4A2n14mIiYjoREQnQQ0AFlG+P4+MjFRdDlA7RcL26Oz0cHZ/LGufknRObrmzJR0usB0AABqtSNhul7Qxe7xR0rdy7R/Njkp+l6RfzE43AwCwFPV1bmTbWyVdJukM21OSPiXpM5Lut32dpJ9J+lC2+A5JV0naL+llSR8ruWYAABqlr7CNiA3zvHRFl2VD0o1FigIAoE04gxQAAIkRtgAAJEbYAgCQGGELAEBihC0AAIkRtgAAJEbYAgCQGGELAEBihC0AAIkRtgAAJEbYAgCQGGELAEBihC0AAIkRtgAAJEbYAgCQGGELAEBiPcPW9ibbx2zvzbX9k+1nbO+x/YDtU7P2VbZ/ZXt3dvtyyuIBAGiCfka2myWtndO2U9LbI+KPJf1Y0i251w5ExFh2u6GcMgEAaK6eYRsRj0g6PqftoYg4kT19XNLZCWoDAKAVyvjM9uOS/jX3/Fzb37f9XduXzreS7XHbk7YnS6gBQIXy/Xl6errqcoDaKRS2tm+VdELSV7OmI5JGI+Idkv5W0tds/0G3dSNiIiI6EdEpUgOA6uX788jISNXlALUzdNja3ijpakn/PSJCkiLilYh4MXu8S9IBSW8po1AAAJpqqLC1vVbSJyR9ICJezrWP2F6WPT5P0mpJz5VRKAAATbW81wK2t0q6TNIZtqckfUozRx+/SdJO25L0eHbk8bsl/YPtE5JelXRDRBzv+sYAACwRPcM2IjZ0ab57nmW3SdpWtCgAANqEM0gBAJAYYQsAQGKELQAAiRG2AAAkRtgCAJAYYQsAQGKELQAAiRG2AAAkRtgCAJAYYQsAQGKELQAAiRG2AAAk1vNCBEtNdmneoWRXQAJQE09ccvnQ6675j++UWAmWOka2AAAkRtgCAJAYYQsAQGI9w9b2JtvHbO/NtX3a9iHbu7PbVbnXbrG93/Y+2+9PVTgAAE3Rz8h2s6S1XdrviIix7LZDkmy/TdJ6SRdm63zJ9rKyigUAoIl6hm1EPCLpeJ/vt07SvRHxSkT8RNJ+SWsK1AcAQOMV+cz2Jtt7smnm07K2lZKezy0zlbW9ge1x25O2JwvUAKAG8v15enq66nKA2hk2bO+UdL6kMUlHJN2etXf7omnXL65GxEREdCKiM2QNAGoi359HRkaqLgeonaHCNiKORsSrEfGapLv026niKUnn5BY9W9LhYiUCANBsQ4Wt7RW5p9dKmj1Sebuk9bbfZPtcSaslPVGsRAAAmq3n6Rptb5V0maQzbE9J+pSky2yPaWaK+KCk6yUpIp6yfb+kH0k6IenGiHg1TekAADRDz7CNiA1dmu9eYPnbJN1WpCgAANqEM0gBAJAYYQsAQGJcYm8OLpMHtAeXyUNdMLIFACAxwhYAgMSYRsaSF/HGk5zxcQLQTC/tfuMY8pSx1yqo5PUY2WJJ6xa0C7UDqK9uQbtQ+2KqvgKgIr0ClcAFmqNXoFYduIQtAACJEbYAACRG2AIAkBhhCwBAYoQtlqxeX+/h6z9Ac/T6ek/VX/8hbLGkzReoBC3QPPMFatVBK3FSC4BgBVqkDsHaDSNbAAAS6xm2tjfZPmZ7b67tPtu7s9tB27uz9lW2f5V77cspiwcAoAn6mUbeLOmLku6ZbYiIv5h9bPt2Sb/ILX8gIsbKKhAAgKbrGbYR8YjtVd1e88yHXR+W9J5yywIAoD2KfmZ7qaSjEfFsru1c29+3/V3bl863ou1x25O2JwvWAKBi+f48PT1ddTlA7RQN2w2StuaeH5E0GhHvkPS3kr5m+w+6rRgRExHRiYhOwRoAVCzfn0dGRqouB6idocPW9nJJfy7pvtm2iHglIl7MHu+SdEDSW4oWCQBAkxUZ2f6ZpGciYmq2wfaI7WXZ4/MkrZb0XLESAQBotn6++rNV0mOSLrA9Zfu67KX1ev0UsiS9W9Ie2z+Q9A1JN0TE8TILBgCgafo5GnnDPO1/2aVtm6RtxcsCAKA9OIMUAACJEbYAACRG2AIAkBhhCwBAYoQtAACJEbYAACRG2AIAkBhhCwBAYoQtAACJEbYAACRG2AIAkJgjouoaZHta0n9JeqHqWkpwhtiPOmnCfvxhRLTmIrC2X5K0r+o6StCEv51+sB+Lq2t/rkXYSpLtyTZcSJ79qJe27EeTtOVnzn7US9P3g2lkAAASI2wBAEisTmE7UXUBJWE/6qUt+9EkbfmZsx/10uj9qM1ntgAAtFWdRrYAALRS5WFre63tfbb327656noGYfug7R/a3m17Mms73fZO289m96dVXedctjfZPmZ7b66ta92e8YXs97PH9jurq/z15tmPT9s+lP1Odtu+KvfaLdl+7LP9/mqqbjf68+KjPzejP1catraXSfpnSVdKepukDbbfVmVNQ7g8IsZyh6TfLOnhiFgt6eHsed1slrR2Ttt8dV8paXV2G5d05yLV2I/NeuN+SNId2e9kLCJ2SFL2d7Ve0oXZOl/K/v5QEvpzZTaL/lz7/lz1yHaNpP0R8VxE/FrSvZLWVVxTUeskbckeb5F0TYW1dBURj0g6Pqd5vrrXSbonZjwu6VTbKxan0oXNsx/zWSfp3oh4JSJ+Imm/Zv7+UB76cwXoz83oz1WH7UpJz+eeT2VtTRGSHrK9y/Z41nZWRByRpOz+zMqqG8x8dTfxd3RTNkW2KTft18T9aJqm/4zpz/XUiv5cddi6S1uTDo++JCLeqZmpmRttv7vqghJo2u/oTknnSxqTdETS7Vl70/ajiZr+M6Y/109r+nPVYTsl6Zzc87MlHa6oloFFxOHs/pikBzQzjXF0dlomuz9WXYUDma/uRv2OIuJoRLwaEa9Juku/nVpq1H40VKN/xvTn+mlTf646bJ+UtNr2ubZP0swH3tsrrqkvtk+2fcrsY0nvk7RXM/VvzBbbKOlb1VQ4sPnq3i7po9lRjO+S9IvZ6ak6mvP507Wa+Z1IM/ux3vabbJ+rmQNEnljs+lqO/lwf9Oe6iYhKb5KukvRjSQck3Vp1PQPUfZ6kH2S3p2Zrl/RmzRz992x2f3rVtXapfatmpmT+n2b+h3jdfHVrZrrmn7Pfzw8ldaquv8d+/J+szj2a6ZArcsvfmu3HPklXVl1/G2/050pqpz83oD9zBikAABKrehoZAIDWI2wBAEiMsAUAIDHCFgCAxAhbAAASI2wBAEiMsAUAIDHCFgCAxP4/ucVr3zMqOHAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 576x864 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate some random images\n",
    "input_images, target_masks = simulation.generate_random_data(192, 192, count=3)\n",
    "\n",
    "for x in [input_images, target_masks]:\n",
    "    print(x.shape)\n",
    "    print(x.min(), x.max())\n",
    "\n",
    "# Change channel-order and make 3 channels for matplot\n",
    "input_images_rgb = [x.astype(np.uint8) for x in input_images]\n",
    "\n",
    "# Map each channel (i.e. class) to each color\n",
    "target_masks_rgb = [helper.masks_to_colorimg(x) for x in target_masks]\n",
    "\n",
    "# Left: Input image (black and white), Right: Target mask (6ch)\n",
    "helper.plot_side_by_side([input_images_rgb, target_masks_rgb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimDataset(Dataset):\n",
    "    def __init__(self, count, transform=None):\n",
    "        self.input_images, self.target_masks = simulation.generate_random_data(192, 192, count=count)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.input_images[idx]\n",
    "        mask = self.target_masks[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return [image, mask]\n",
    "\n",
    "# use the same transformations for train/val in this example\n",
    "trans = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # imagenet\n",
    "])\n",
    "\n",
    "train_set = SimDataset(2000, transform = trans)\n",
    "val_set = SimDataset(200, transform = trans)\n",
    "\n",
    "image_datasets = {\n",
    "    'train': train_set, 'val': val_set\n",
    "}\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "dataloaders_orig = {\n",
    "    'train': DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0),\n",
    "    'val': DataLoader(val_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders_orig['train'].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders_orig['train'].dataset.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders_orig['train'].dataset.input_images.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders_orig['val'].dataset.input_images.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders_orig['val'].dataset.input_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders_orig['val'].dataset.target_masks.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### my dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_img, out_target_perm = group_to_image_constructors.make_an_image_from_group(*np.array(synthetic_data.make_a_group()),permute_group_ids = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimDataset(Dataset):\n",
    "    def __init__(self, count, transform=None):\n",
    "        self.sim_data = [group_to_image_constructors.make_an_image_from_group(*np.array(synthetic_data.make_a_group()),permute_group_ids = False) for _ in range(count)]\n",
    "        self.input_images = np.array([x[0] for  x in self.sim_data]).astype('uint8')\n",
    "        self.target_masks = np.array([x[1] for  x in self.sim_data]).astype('float32')\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.input_images[idx]\n",
    "        mask = self.target_masks[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return [image, mask]\n",
    "\n",
    "# use the same transformations for train/val in this example\n",
    "trans = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # imagenet\n",
    "])\n",
    "\n",
    "train_set = SimDataset(2000, transform = trans)\n",
    "val_set = SimDataset(200, transform = trans)\n",
    "\n",
    "image_datasets = {\n",
    "    'train': train_set, 'val': val_set\n",
    "}\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "#it is at this next step that the numpy arrays are converted to tensors\n",
    "dataloaders = {\n",
    "    'train': DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0),\n",
    "    'val': DataLoader(val_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders['val'].dataset.target_masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders['val'].dataset.input_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 224, 224]) torch.Size([4, 4, 224, 224])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f49ac12ed90>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def reverse_transform(inp):\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    inp = (inp * 255).astype(np.uint8)\n",
    "\n",
    "    return inp\n",
    "\n",
    "# Get a batch of training data\n",
    "inputs, masks = next(iter(dataloaders['train']))\n",
    "\n",
    "print(inputs.shape, masks.shape)\n",
    "\n",
    "plt.imshow(reverse_transform(inputs[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convrelu(in_channels, out_channels, kernel, padding):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel, padding=padding),\n",
    "        nn.ReLU(inplace=True),\n",
    "    )\n",
    "\n",
    "\n",
    "class ResNetUNet(nn.Module):\n",
    "    def __init__(self, n_class):\n",
    "        super().__init__()\n",
    "\n",
    "        self.base_model = models.resnet18(pretrained=True)\n",
    "        self.base_layers = list(self.base_model.children())\n",
    "\n",
    "        self.layer0 = nn.Sequential(*self.base_layers[:3]) # size=(N, 64, x.H/2, x.W/2)\n",
    "        self.layer0_1x1 = convrelu(64, 64, 1, 0)\n",
    "        self.layer1 = nn.Sequential(*self.base_layers[3:5]) # size=(N, 64, x.H/4, x.W/4)\n",
    "        self.layer1_1x1 = convrelu(64, 64, 1, 0)\n",
    "        self.layer2 = self.base_layers[5]  # size=(N, 128, x.H/8, x.W/8)\n",
    "        self.layer2_1x1 = convrelu(128, 128, 1, 0)\n",
    "        self.layer3 = self.base_layers[6]  # size=(N, 256, x.H/16, x.W/16)\n",
    "        self.layer3_1x1 = convrelu(256, 256, 1, 0)\n",
    "        self.layer4 = self.base_layers[7]  # size=(N, 512, x.H/32, x.W/32)\n",
    "        self.layer4_1x1 = convrelu(512, 512, 1, 0)\n",
    "\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "        self.conv_up3 = convrelu(256 + 512, 512, 3, 1)\n",
    "        self.conv_up2 = convrelu(128 + 512, 256, 3, 1)\n",
    "        self.conv_up1 = convrelu(64 + 256, 256, 3, 1)\n",
    "        self.conv_up0 = convrelu(64 + 256, 128, 3, 1)\n",
    "\n",
    "        self.conv_original_size0 = convrelu(3, 64, 3, 1)\n",
    "        self.conv_original_size1 = convrelu(64, 64, 3, 1)\n",
    "        self.conv_original_size2 = convrelu(64 + 128, 64, 3, 1)\n",
    "\n",
    "        self.conv_last = nn.Conv2d(64, n_class, 1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x_original = self.conv_original_size0(input)\n",
    "        x_original = self.conv_original_size1(x_original)\n",
    "\n",
    "        layer0 = self.layer0(input)\n",
    "        layer1 = self.layer1(layer0)\n",
    "        layer2 = self.layer2(layer1)\n",
    "        layer3 = self.layer3(layer2)\n",
    "        layer4 = self.layer4(layer3)\n",
    "\n",
    "        layer4 = self.layer4_1x1(layer4)\n",
    "        x = self.upsample(layer4)\n",
    "        layer3 = self.layer3_1x1(layer3)\n",
    "        x = torch.cat([x, layer3], dim=1)\n",
    "        x = self.conv_up3(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer2 = self.layer2_1x1(layer2)\n",
    "        x = torch.cat([x, layer2], dim=1)\n",
    "        x = self.conv_up2(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer1 = self.layer1_1x1(layer1)\n",
    "        x = torch.cat([x, layer1], dim=1)\n",
    "        x = self.conv_up1(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer0 = self.layer0_1x1(layer0)\n",
    "        x = torch.cat([x, layer0], dim=1)\n",
    "        x = self.conv_up0(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, x_original], dim=1)\n",
    "        x = self.conv_original_size2(x)\n",
    "\n",
    "        out = self.conv_last(x)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = ResNetUNet(n_class=2)\n",
    "# model = model.to(device)\n",
    "\n",
    "# check keras-like model summary using torchsummary\n",
    "# from torchsummary import summary\n",
    "# summary(model, input_size=(3, 224, 224))\n",
    "# summary(model, input_size=(3, 512, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calc_loss(pred, target, metrics, bce_weight=0.5):\n",
    "    \n",
    "#     #add a new dimension after batch and broadcast/copy the predictions over this dimension\n",
    "#     broadcast_preds = batch_pred.unsqueeze(1).repeat(1,24,1,1,1)\n",
    "    \n",
    "#     #create the list of permutations of 4 group labels\n",
    "#     perms = torch.tensor([x for x in itertools.permutations([0,1,2,3])]).type(torch.long)\n",
    "    \n",
    "#     #create the permuted targets\n",
    "#     perms_target = target[:,perms,:,:]\n",
    "    \n",
    "#     #get the BCE loss\n",
    "#     batch_mins, _ = torch.min(torch.mean(F.binary_cross_entropy_with_logits(broadcast_preds, perms_target, reduction = 'none'), dim = (2,3,4)), dim = 1)\n",
    "#     bce = torch.mean(batch_mins)\n",
    "    \n",
    "#     #get the dice loss\n",
    "#     broadcast_preds = F.sigmoid(broadcast_preds)    \n",
    "#     smooth = 1\n",
    "    \n",
    "#     intersection = (broadcast_preds*perms_target).sum(dim=3).sum(dim=3)\n",
    "#     loss = (1 - ((2. * intersection + smooth) / (broadcast_preds.sum(dim=3).sum(dim=3) + perms_target.sum(dim=3).sum(dim=3) + smooth)))\n",
    "#     batch_dice_mins, _ = torch.min(torch.mean(loss, dim = 2), dim = 1)\n",
    "#     dice = torch.mean(batch_dice_mins)\n",
    "    \n",
    "#     #combine losses\n",
    "#     loss = bce * bce_weight + dice * (1 - bce_weight)\n",
    "\n",
    "#     metrics['bce'] += bce.data.cpu().numpy() * target.size(0)\n",
    "#     metrics['dice'] += dice.data.cpu().numpy() * target.size(0)\n",
    "#     metrics['loss'] += loss.data.cpu().numpy() * target.size(0)\n",
    "\n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(pred, target, metrics, bce_weight=0.5):\n",
    "    bce = F.binary_cross_entropy_with_logits(pred, target)\n",
    "\n",
    "    pred = F.sigmoid(pred)\n",
    "    dice = Unet_loss.dice_loss(pred, target)\n",
    "\n",
    "    loss = bce * bce_weight + dice * (1 - bce_weight)\n",
    "\n",
    "    metrics['bce'] += bce.data.cpu().numpy() * target.size(0)\n",
    "    metrics['dice'] += dice.data.cpu().numpy() * target.size(0)\n",
    "    metrics['loss'] += loss.data.cpu().numpy() * target.size(0)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def print_metrics(metrics, epoch_samples, phase):\n",
    "    outputs = []\n",
    "    for k in metrics.keys():\n",
    "        outputs.append(\"{}: {:4f}\".format(k, metrics[k] / epoch_samples))\n",
    "\n",
    "    print(\"{}: {}\".format(phase, \", \".join(outputs)))\n",
    "\n",
    "def train_model(model, optimizer, scheduler, num_epochs=25):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 1e10\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        #get new data every epoch\n",
    "        dataloaders = {\n",
    "            'train': DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0),\n",
    "            'val': DataLoader(val_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "        }\n",
    "        \n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        since = time.time()\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    print(\"LR\", param_group['lr'])\n",
    "\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            metrics = defaultdict(float)\n",
    "            epoch_samples = 0\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = calc_loss(outputs, labels, metrics)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                epoch_samples += inputs.size(0)\n",
    "\n",
    "            print_metrics(metrics, epoch_samples, phase)\n",
    "            epoch_loss = metrics['loss'] / epoch_samples\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                print(\"saving best model\")\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print('{:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    print('Best val loss: {:4f}'.format(best_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'Unet.loss' from '/home/a/anaconda3/lib/python3.7/site-packages/Unet/loss.py'>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(Unet_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Epoch 0/599\n",
      "----------\n",
      "LR 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/a/anaconda3/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:100: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "/home/a/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: bce: 0.011758, dice: 0.405283, loss: 0.208520\n",
      "val: bce: 0.003668, dice: 0.377182, loss: 0.190425\n",
      "saving best model\n",
      "1m 17s\n",
      "Epoch 1/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.003100, dice: 0.322457, loss: 0.162779\n",
      "val: bce: 0.002343, dice: 0.274617, loss: 0.138480\n",
      "saving best model\n",
      "1m 17s\n",
      "Epoch 2/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.002797, dice: 0.261921, loss: 0.132359\n",
      "val: bce: 0.000768, dice: 0.124826, loss: 0.062797\n",
      "saving best model\n",
      "1m 17s\n",
      "Epoch 3/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000668, dice: 0.131041, loss: 0.065854\n",
      "val: bce: 0.000565, dice: 0.118983, loss: 0.059774\n",
      "saving best model\n",
      "1m 17s\n",
      "Epoch 4/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000600, dice: 0.126672, loss: 0.063636\n",
      "val: bce: 0.000860, dice: 0.123377, loss: 0.062118\n",
      "1m 17s\n",
      "Epoch 5/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000576, dice: 0.123719, loss: 0.062148\n",
      "val: bce: 0.000602, dice: 0.115199, loss: 0.057901\n",
      "saving best model\n",
      "1m 17s\n",
      "Epoch 6/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000672, dice: 0.119196, loss: 0.059934\n",
      "val: bce: 0.000796, dice: 0.107790, loss: 0.054293\n",
      "saving best model\n",
      "1m 17s\n",
      "Epoch 7/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000631, dice: 0.114516, loss: 0.057574\n",
      "val: bce: 0.000817, dice: 0.106027, loss: 0.053422\n",
      "saving best model\n",
      "1m 17s\n",
      "Epoch 8/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000757, dice: 0.115150, loss: 0.057954\n",
      "val: bce: 0.000943, dice: 0.106375, loss: 0.053659\n",
      "1m 17s\n",
      "Epoch 9/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000568, dice: 0.097003, loss: 0.048785\n",
      "val: bce: 0.000383, dice: 0.074030, loss: 0.037206\n",
      "saving best model\n",
      "1m 17s\n",
      "Epoch 10/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000412, dice: 0.067248, loss: 0.033830\n",
      "val: bce: 0.000471, dice: 0.070531, loss: 0.035501\n",
      "saving best model\n",
      "1m 17s\n",
      "Epoch 11/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000405, dice: 0.061856, loss: 0.031131\n",
      "val: bce: 0.000575, dice: 0.071241, loss: 0.035908\n",
      "1m 17s\n",
      "Epoch 12/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000378, dice: 0.060850, loss: 0.030614\n",
      "val: bce: 0.000487, dice: 0.060876, loss: 0.030682\n",
      "saving best model\n",
      "1m 17s\n",
      "Epoch 13/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000323, dice: 0.053801, loss: 0.027062\n",
      "val: bce: 0.000346, dice: 0.061488, loss: 0.030917\n",
      "1m 17s\n",
      "Epoch 14/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000372, dice: 0.054882, loss: 0.027627\n",
      "val: bce: 0.000445, dice: 0.052110, loss: 0.026277\n",
      "saving best model\n",
      "1m 17s\n",
      "Epoch 15/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000309, dice: 0.047240, loss: 0.023775\n",
      "val: bce: 0.000415, dice: 0.056019, loss: 0.028217\n",
      "1m 17s\n",
      "Epoch 16/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000326, dice: 0.048555, loss: 0.024440\n",
      "val: bce: 0.000530, dice: 0.065215, loss: 0.032872\n",
      "1m 17s\n",
      "Epoch 17/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000331, dice: 0.045283, loss: 0.022807\n",
      "val: bce: 0.000660, dice: 0.064348, loss: 0.032504\n",
      "1m 17s\n",
      "Epoch 18/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000339, dice: 0.044952, loss: 0.022646\n",
      "val: bce: 0.000518, dice: 0.055122, loss: 0.027820\n",
      "1m 17s\n",
      "Epoch 19/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000310, dice: 0.042255, loss: 0.021282\n",
      "val: bce: 0.000437, dice: 0.060598, loss: 0.030517\n",
      "1m 17s\n",
      "Epoch 20/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000300, dice: 0.041103, loss: 0.020701\n",
      "val: bce: 0.000515, dice: 0.058256, loss: 0.029386\n",
      "1m 17s\n",
      "Epoch 21/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000290, dice: 0.040153, loss: 0.020222\n",
      "val: bce: 0.000628, dice: 0.053224, loss: 0.026926\n",
      "1m 17s\n",
      "Epoch 22/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000355, dice: 0.047503, loss: 0.023929\n",
      "val: bce: 0.000558, dice: 0.053912, loss: 0.027235\n",
      "1m 17s\n",
      "Epoch 23/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000269, dice: 0.039036, loss: 0.019653\n",
      "val: bce: 0.000534, dice: 0.052946, loss: 0.026740\n",
      "1m 17s\n",
      "Epoch 24/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000293, dice: 0.039059, loss: 0.019676\n",
      "val: bce: 0.000538, dice: 0.058358, loss: 0.029448\n",
      "1m 17s\n",
      "Epoch 25/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000347, dice: 0.047037, loss: 0.023692\n",
      "val: bce: 0.000512, dice: 0.061302, loss: 0.030907\n",
      "1m 17s\n",
      "Epoch 26/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000308, dice: 0.039251, loss: 0.019780\n",
      "val: bce: 0.000664, dice: 0.058207, loss: 0.029436\n",
      "1m 18s\n",
      "Epoch 27/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000278, dice: 0.035246, loss: 0.017762\n",
      "val: bce: 0.000487, dice: 0.054601, loss: 0.027544\n",
      "1m 18s\n",
      "Epoch 28/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000262, dice: 0.034119, loss: 0.017190\n",
      "val: bce: 0.000484, dice: 0.052118, loss: 0.026301\n",
      "1m 18s\n",
      "Epoch 29/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000261, dice: 0.036607, loss: 0.018434\n",
      "val: bce: 0.000537, dice: 0.066884, loss: 0.033710\n",
      "1m 18s\n",
      "Epoch 30/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000312, dice: 0.039606, loss: 0.019959\n",
      "val: bce: 0.000612, dice: 0.057676, loss: 0.029144\n",
      "1m 18s\n",
      "Epoch 31/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000224, dice: 0.032219, loss: 0.016221\n",
      "val: bce: 0.000520, dice: 0.051481, loss: 0.026001\n",
      "saving best model\n",
      "1m 18s\n",
      "Epoch 32/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000213, dice: 0.030535, loss: 0.015374\n",
      "val: bce: 0.000422, dice: 0.052505, loss: 0.026463\n",
      "1m 18s\n",
      "Epoch 33/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000193, dice: 0.029451, loss: 0.014822\n",
      "val: bce: 0.000444, dice: 0.047782, loss: 0.024113\n",
      "saving best model\n",
      "1m 18s\n",
      "Epoch 34/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000196, dice: 0.030730, loss: 0.015463\n",
      "val: bce: 0.000364, dice: 0.051353, loss: 0.025859\n",
      "1m 18s\n",
      "Epoch 35/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000191, dice: 0.029499, loss: 0.014845\n",
      "val: bce: 0.000492, dice: 0.051255, loss: 0.025874\n",
      "1m 18s\n",
      "Epoch 36/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000231, dice: 0.031867, loss: 0.016049\n",
      "val: bce: 0.000705, dice: 0.066191, loss: 0.033448\n",
      "1m 18s\n",
      "Epoch 37/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000232, dice: 0.031675, loss: 0.015954\n",
      "val: bce: 0.000377, dice: 0.050626, loss: 0.025501\n",
      "1m 18s\n",
      "Epoch 38/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000213, dice: 0.028098, loss: 0.014155\n",
      "val: bce: 0.000453, dice: 0.049513, loss: 0.024983\n",
      "1m 18s\n",
      "Epoch 39/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000178, dice: 0.024995, loss: 0.012586\n",
      "val: bce: 0.000446, dice: 0.049248, loss: 0.024847\n",
      "1m 18s\n",
      "Epoch 40/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000200, dice: 0.029337, loss: 0.014769\n",
      "val: bce: 0.000448, dice: 0.053458, loss: 0.026953\n",
      "1m 18s\n",
      "Epoch 41/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000169, dice: 0.025268, loss: 0.012718\n",
      "val: bce: 0.000381, dice: 0.047990, loss: 0.024185\n",
      "1m 18s\n",
      "Epoch 42/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000165, dice: 0.024980, loss: 0.012572\n",
      "val: bce: 0.000411, dice: 0.052722, loss: 0.026566\n",
      "1m 18s\n",
      "Epoch 43/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000198, dice: 0.028145, loss: 0.014172\n",
      "val: bce: 0.000456, dice: 0.055199, loss: 0.027827\n",
      "1m 18s\n",
      "Epoch 44/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000168, dice: 0.024459, loss: 0.012314\n",
      "val: bce: 0.000484, dice: 0.046732, loss: 0.023608\n",
      "saving best model\n",
      "1m 18s\n",
      "Epoch 45/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000161, dice: 0.024947, loss: 0.012554\n",
      "val: bce: 0.000465, dice: 0.048551, loss: 0.024508\n",
      "1m 18s\n",
      "Epoch 46/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000142, dice: 0.021550, loss: 0.010846\n",
      "val: bce: 0.000411, dice: 0.048812, loss: 0.024611\n",
      "1m 18s\n",
      "Epoch 47/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000244, dice: 0.034864, loss: 0.017554\n",
      "val: bce: 0.000449, dice: 0.047264, loss: 0.023857\n",
      "1m 18s\n",
      "Epoch 48/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000176, dice: 0.025372, loss: 0.012774\n",
      "val: bce: 0.000444, dice: 0.057816, loss: 0.029130\n",
      "1m 18s\n",
      "Epoch 49/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000133, dice: 0.019844, loss: 0.009989\n",
      "val: bce: 0.000379, dice: 0.042146, loss: 0.021263\n",
      "saving best model\n",
      "1m 18s\n",
      "Epoch 50/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000131, dice: 0.020953, loss: 0.010542\n",
      "val: bce: 0.000310, dice: 0.039185, loss: 0.019747\n",
      "saving best model\n",
      "1m 18s\n",
      "Epoch 51/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000184, dice: 0.024672, loss: 0.012428\n",
      "val: bce: 0.000423, dice: 0.046178, loss: 0.023301\n",
      "1m 18s\n",
      "Epoch 52/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000166, dice: 0.026421, loss: 0.013293\n",
      "val: bce: 0.000390, dice: 0.042549, loss: 0.021470\n",
      "1m 18s\n",
      "Epoch 53/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000141, dice: 0.020262, loss: 0.010202\n",
      "val: bce: 0.000360, dice: 0.042098, loss: 0.021229\n",
      "1m 18s\n",
      "Epoch 54/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000136, dice: 0.019460, loss: 0.009798\n",
      "val: bce: 0.000367, dice: 0.042048, loss: 0.021207\n",
      "1m 18s\n",
      "Epoch 55/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000154, dice: 0.021273, loss: 0.010714\n",
      "val: bce: 0.000539, dice: 0.053103, loss: 0.026821\n",
      "1m 18s\n",
      "Epoch 56/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000150, dice: 0.023756, loss: 0.011953\n",
      "val: bce: 0.000428, dice: 0.044916, loss: 0.022672\n",
      "1m 18s\n",
      "Epoch 57/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000161, dice: 0.024396, loss: 0.012279\n",
      "val: bce: 0.000417, dice: 0.055665, loss: 0.028041\n",
      "1m 18s\n",
      "Epoch 58/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000151, dice: 0.022273, loss: 0.011212\n",
      "val: bce: 0.000502, dice: 0.042874, loss: 0.021688\n",
      "1m 18s\n",
      "Epoch 59/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000156, dice: 0.021722, loss: 0.010939\n",
      "val: bce: 0.000430, dice: 0.045343, loss: 0.022886\n",
      "1m 18s\n",
      "Epoch 60/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000124, dice: 0.017175, loss: 0.008650\n",
      "val: bce: 0.000388, dice: 0.042692, loss: 0.021540\n",
      "1m 18s\n",
      "Epoch 61/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000112, dice: 0.017305, loss: 0.008709\n",
      "val: bce: 0.000439, dice: 0.043358, loss: 0.021898\n",
      "1m 18s\n",
      "Epoch 62/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000111, dice: 0.016727, loss: 0.008419\n",
      "val: bce: 0.000436, dice: 0.043818, loss: 0.022127\n",
      "1m 18s\n",
      "Epoch 63/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000121, dice: 0.016966, loss: 0.008543\n",
      "val: bce: 0.000411, dice: 0.043414, loss: 0.021912\n",
      "1m 18s\n",
      "Epoch 64/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000109, dice: 0.015968, loss: 0.008038\n",
      "val: bce: 0.000384, dice: 0.045493, loss: 0.022938\n",
      "1m 18s\n",
      "Epoch 65/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000121, dice: 0.018704, loss: 0.009413\n",
      "val: bce: 0.000433, dice: 0.049143, loss: 0.024788\n",
      "1m 18s\n",
      "Epoch 66/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000116, dice: 0.016483, loss: 0.008300\n",
      "val: bce: 0.000359, dice: 0.043286, loss: 0.021822\n",
      "1m 18s\n",
      "Epoch 67/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000102, dice: 0.015662, loss: 0.007882\n",
      "val: bce: 0.000463, dice: 0.044317, loss: 0.022390\n",
      "1m 18s\n",
      "Epoch 68/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000101, dice: 0.015363, loss: 0.007732\n",
      "val: bce: 0.000334, dice: 0.044259, loss: 0.022297\n",
      "1m 18s\n",
      "Epoch 69/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000133, dice: 0.019781, loss: 0.009957\n",
      "val: bce: 0.000552, dice: 0.059558, loss: 0.030055\n",
      "1m 18s\n",
      "Epoch 70/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000108, dice: 0.017664, loss: 0.008886\n",
      "val: bce: 0.000304, dice: 0.042499, loss: 0.021401\n",
      "1m 18s\n",
      "Epoch 71/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000096, dice: 0.015630, loss: 0.007863\n",
      "val: bce: 0.000309, dice: 0.040474, loss: 0.020392\n",
      "1m 18s\n",
      "Epoch 72/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000107, dice: 0.016606, loss: 0.008357\n",
      "val: bce: 0.000393, dice: 0.044520, loss: 0.022456\n",
      "1m 18s\n",
      "Epoch 73/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000102, dice: 0.014943, loss: 0.007522\n",
      "val: bce: 0.000346, dice: 0.038975, loss: 0.019660\n",
      "saving best model\n",
      "1m 18s\n",
      "Epoch 74/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000104, dice: 0.015335, loss: 0.007719\n",
      "val: bce: 0.000356, dice: 0.041343, loss: 0.020850\n",
      "1m 18s\n",
      "Epoch 75/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000122, dice: 0.019547, loss: 0.009834\n",
      "val: bce: 0.000460, dice: 0.053765, loss: 0.027112\n",
      "1m 18s\n",
      "Epoch 76/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000148, dice: 0.020383, loss: 0.010265\n",
      "val: bce: 0.000422, dice: 0.039619, loss: 0.020021\n",
      "1m 18s\n",
      "Epoch 77/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000110, dice: 0.015583, loss: 0.007847\n",
      "val: bce: 0.000370, dice: 0.041795, loss: 0.021082\n",
      "1m 18s\n",
      "Epoch 78/599\n",
      "----------\n",
      "LR 0.0001\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-1ff1c58a3e7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mexp_lr_scheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStepLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_lr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m600\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-29-289d427beb03>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     67\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;31m# statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     94\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                     \u001b[0;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "num_class = 4\n",
    "model = ResNetUNet(num_class).to(device)\n",
    "\n",
    "# freeze backbone layers\n",
    "#for l in model.base_layers:\n",
    "#    for param in l.parameters():\n",
    "#        param.requires_grad = False\n",
    "\n",
    "optimizer_ft = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=300, gamma=0.1)\n",
    "\n",
    "model = train_model(model, optimizer_ft, exp_lr_scheduler, num_epochs=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://discuss.pytorch.org/t/save-and-load-model/6206/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, '/home/a/Documents/GNN-for-trans-grouping/GNN-for-trans-grouping/Unet_model_multi_seq_strict_encoding.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('/home/a/Documents/GNN-for-trans-grouping/GNN-for-trans-grouping/Unet_model_multi_seq_strict_encoding.pt')\n",
    "# model = torch.load(r'C:\\Users\\andy.knapper\\Documents\\OW\\Categorisation\\ML grouping\\GNN-for-trans-grouping\\Unet_model_1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sim_data = [group_to_image_constructors.make_an_image(*np.array(synthetic_data.make_a_group())) for _ in range(3)]\n",
    "# input_images = np.array([x[0] for  x in sim_data]).astype('uint8')\n",
    "# target_masks = np.array([x[1] for  x in sim_data]).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# output = model(torch.tensor(test_img))\n",
    "# prediction = torch.argmax(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model, input_size=(3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = SimDataset(1000, transform = trans)\n",
    "val_set = SimDataset(2, transform = trans)\n",
    "\n",
    "image_datasets = {\n",
    "    'train': train_set, 'val': val_set\n",
    "}\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "dataloaders = {\n",
    "    'train': DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0),\n",
    "    'val': DataLoader(val_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.eval()\n",
    "for i, (inputs, labels) in enumerate(dataloaders['train']):\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    a_arr, d_arr = np.nonzero(np.max(reverse_transform(inputs[0].cpu()), axis = 2))\n",
    "    \n",
    "    p = torch.nn.functional.softmax(outputs[0], dim=1)\n",
    "    p_arr = p.cpu().data.numpy()\n",
    "    \n",
    "    pred_labels = np.argmax(p_arr, axis = 0)[a_arr, d_arr]\n",
    "    target_labels = np.argmax(labels[0].cpu().data.numpy(), axis = 0)[a_arr, d_arr]\n",
    "    \n",
    "    ari = metrics.adjusted_rand_score(pred_labels, target_labels)\n",
    "      \n",
    "    plot_preds_and_targets(d_arr, a_arr, pred_labels, target_labels, ari, i)\n",
    "\n",
    "    \n",
    "    print(i, ari)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_preds_and_targets(inp_d_arr, inp_a_arr, inp_pred_arr, inp_targets_arr, inp_ari, i):\n",
    "    colour_list = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'tab:brown', 'tab:pink', 'tab:gray', 'tab:olive', 'tab:cyan']\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(10,16), sharex=True)\n",
    "    \n",
    "    for p in inp_pred_arr:\n",
    "        mask = (inp_pred_arr == p)\n",
    "        ax1.scatter(inp_d_arr[mask], inp_a_arr[mask], s=10, c=colour_list[p%10], marker='x')\n",
    "\n",
    "    for t in inp_targets_arr:\n",
    "        mask = (inp_targets_arr == t)\n",
    "        ax2.scatter(inp_d_arr[mask], inp_a_arr[mask], s=10, c=colour_list[t%10], marker='x')\n",
    "        \n",
    "    for ax in fig.axes:\n",
    "        matplotlib.pyplot.sca(ax)\n",
    "        plt.xticks(rotation=90)\n",
    "        #plt.legend(bbox_to_anchor=(1.02, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "    plt.savefig('/home/a/Documents/GNN-for-trans-grouping/GNN-for-trans-grouping/Charts/'+str(ari)+'_'+str(i)+'.png')\n",
    "        \n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders['train'].dataset.target_masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = dataloaders['train'].dataset.target_masks[999,:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 3\n",
    "a_arr, d_arr = np.nonzero(np.max(reverse_transform(inputs[idx].cpu()), axis = 2))\n",
    "# t_arr = (p_arr[idx, :, a_arr, d_arr][:,0]<0.5).astype(int)\n",
    "\n",
    "t_arr = np.argmax(target, axis = 0)[a_arr, d_arr]\n",
    "\n",
    "colour_list = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'tab:brown', 'tab:pink', 'tab:gray', 'tab:olive', 'tab:cyan']\n",
    "\n",
    "\n",
    "fig, (ax1) = plt.subplots(nrows=1, ncols=1, figsize=(10,8), sharex=True)\n",
    "for t in t_arr:\n",
    "    mask = (t_arr == t)\n",
    "    ax1.scatter(d_arr[mask], a_arr[mask], s=10, c=colour_list[t%10], marker='x')\n",
    "    \n",
    "for ax1 in fig.axes:\n",
    "    matplotlib.pyplot.sca(ax1)\n",
    "    plt.xticks(rotation=90)\n",
    "    #plt.legend(bbox_to_anchor=(1.02, 1), loc=2, borderaxespad=0.)\n",
    "    \n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = torch.nn.functional.softmax(outputs, dim=1)\n",
    "p_arr = p.cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 3\n",
    "a_arr, d_arr = np.nonzero(np.max(reverse_transform(inputs[idx].cpu()), axis = 2))\n",
    "# t_arr = (p_arr[idx, :, a_arr, d_arr][:,0]<0.5).astype(int)\n",
    "\n",
    "t_arr = np.argmax(p_arr[0,:,:,:], axis = 0)[a_arr, d_arr]\n",
    "\n",
    "colour_list = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'tab:brown', 'tab:pink', 'tab:gray', 'tab:olive', 'tab:cyan']\n",
    "\n",
    "\n",
    "fig, (ax1) = plt.subplots(nrows=1, ncols=1, figsize=(10,8), sharex=True)\n",
    "for t in t_arr:\n",
    "    mask = (t_arr == t)\n",
    "    ax1.scatter(d_arr[mask], a_arr[mask], s=10, c=colour_list[t%10], marker='x')\n",
    "    \n",
    "for ax1 in fig.axes:\n",
    "    matplotlib.pyplot.sca(ax1)\n",
    "    plt.xticks(rotation=90)\n",
    "    #plt.legend(bbox_to_anchor=(1.02, 1), loc=2, borderaxespad=0.)\n",
    "    \n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_arr[0,:,:,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(p_arr[0,2,a_arr, d_arr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(p_arr[0,:,:,:], axis = 0)[a_arr, d_arr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
