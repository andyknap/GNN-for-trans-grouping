{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Unet CNN Pixel classification model for grouping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version of the UNET model uses the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import timeit\n",
    "import statistics\n",
    "import time\n",
    "import torch\n",
    "import torch_geometric\n",
    "import importlib\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "\n",
    "from data_utils import synthetic_data\n",
    "from data_utils import graph_constructors\n",
    "from data_utils import group_to_image_constructors\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import importlib\n",
    "\n",
    "from Unet import helper\n",
    "from Unet import simulation\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from torchvision import transforms, datasets, models\n",
    "import torchvision.utils\n",
    "\n",
    "from Unet import loss as Unet_loss\n",
    "\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'data_utils.group_to_image_constructors' from '/home/a/Documents/GNN-for-trans-grouping/GNN-for-trans-grouping/data_utils/group_to_image_constructors.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(synthetic_data)\n",
    "importlib.reload(graph_constructors)\n",
    "importlib.reload(group_to_image_constructors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "colour_list = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'tab:brown', 'tab:pink', 'tab:gray', 'tab:olive', 'tab:cyan']\n",
    "\n",
    "for i in range(2):\n",
    "    d_lst, a_lst, g_lst, t_lst = synthetic_data.make_a_group()\n",
    "    \n",
    "    d_arr = np.array(d_lst)\n",
    "    a_arr = np.array(a_lst)\n",
    "    g_arr = np.array(g_lst)    \n",
    "    \n",
    "    fig, (ax1) = plt.subplots(nrows=1, ncols=1, figsize=(10,8), sharex=True)\n",
    "    for g in g_lst:\n",
    "        mask = (g_arr == g)\n",
    "        \n",
    "        ax1.scatter(d_arr[mask], a_arr[mask], s=10, c=colour_list[g%10], marker='x')\n",
    "        ax1.set_title(str(i))\n",
    "        #ax1.legend(loc=\"upper right\")\n",
    "    \n",
    "    for ax1 in fig.axes:\n",
    "        matplotlib.pyplot.sca(ax1)\n",
    "        plt.xticks(rotation=90)\n",
    "        #plt.legend(bbox_to_anchor=(1.02, 1), loc=2, borderaxespad=0.)\n",
    "    \n",
    "    plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_lst, a_lst, g_lst, t_lst = synthetic_data.make_a_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_lst = []\n",
    "\n",
    "for _ in range(1000):\n",
    "    d_lst, a_lst, g_lst, t_lst = synthetic_data.make_a_group()\n",
    "    max_lst = max_lst+ [max(d_lst)]\n",
    "    \n",
    "max(max_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_arr = np.array(d_lst)\n",
    "a_arr = np.array(a_lst)\n",
    "g_arr = np.array(g_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_a_arr = graph_constructors.normalise_amounts(a_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unet demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 192, 192, 3)\n",
      "0 255\n",
      "(3, 6, 192, 192)\n",
      "0.0 1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x864 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate some random images\n",
    "input_images, target_masks = simulation.generate_random_data(192, 192, count=3)\n",
    "\n",
    "for x in [input_images, target_masks]:\n",
    "    print(x.shape)\n",
    "    print(x.min(), x.max())\n",
    "\n",
    "# Change channel-order and make 3 channels for matplot\n",
    "input_images_rgb = [x.astype(np.uint8) for x in input_images]\n",
    "\n",
    "# Map each channel (i.e. class) to each color\n",
    "target_masks_rgb = [helper.masks_to_colorimg(x) for x in target_masks]\n",
    "\n",
    "# Left: Input image (black and white), Right: Target mask (6ch)\n",
    "helper.plot_side_by_side([input_images_rgb, target_masks_rgb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimDataset(Dataset):\n",
    "    def __init__(self, count, transform=None):\n",
    "        self.input_images, self.target_masks = simulation.generate_random_data(192, 192, count=count)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.input_images[idx]\n",
    "        mask = self.target_masks[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return [image, mask]\n",
    "\n",
    "# use the same transformations for train/val in this example\n",
    "trans = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # imagenet\n",
    "])\n",
    "\n",
    "train_set = SimDataset(2000, transform = trans)\n",
    "val_set = SimDataset(200, transform = trans)\n",
    "\n",
    "image_datasets = {\n",
    "    'train': train_set, 'val': val_set\n",
    "}\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "dataloaders_orig = {\n",
    "    'train': DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0),\n",
    "    'val': DataLoader(val_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders_orig['train'].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders_orig['train'].dataset.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders_orig['train'].dataset.input_images.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders_orig['val'].dataset.input_images.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders_orig['val'].dataset.input_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders_orig['val'].dataset.target_masks.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### my dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_img, out_target_perm = group_to_image_constructors.make_an_image_from_group(*np.array(synthetic_data.make_a_group()),permute_group_ids = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimDataset(Dataset):\n",
    "    def __init__(self, count, transform=None):\n",
    "        self.sim_data = [group_to_image_constructors.make_an_image_from_group(*np.array(synthetic_data.make_a_group()),permute_group_ids = False) for _ in range(count)]\n",
    "        self.input_images = np.array([x[0] for  x in self.sim_data]).astype('uint8')\n",
    "        self.target_masks = np.array([x[1] for  x in self.sim_data]).astype('float32')\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.input_images[idx]\n",
    "        mask = self.target_masks[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return [image, mask]\n",
    "\n",
    "# use the same transformations for train/val in this example\n",
    "trans = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # imagenet\n",
    "])\n",
    "\n",
    "train_set = SimDataset(2000, transform = trans)\n",
    "val_set = SimDataset(200, transform = trans)\n",
    "\n",
    "image_datasets = {\n",
    "    'train': train_set, 'val': val_set\n",
    "}\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "#it is at this next step that the numpy arrays are converted to tensors\n",
    "dataloaders = {\n",
    "    'train': DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0),\n",
    "    'val': DataLoader(val_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders['val'].dataset.target_masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders['val'].dataset.input_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 224, 224]) torch.Size([4, 4, 224, 224])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f8d941bab50>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def reverse_transform(inp):\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    inp = (inp * 255).astype(np.uint8)\n",
    "\n",
    "    return inp\n",
    "\n",
    "# Get a batch of training data\n",
    "inputs, masks = next(iter(dataloaders['train']))\n",
    "\n",
    "print(inputs.shape, masks.shape)\n",
    "\n",
    "plt.imshow(reverse_transform(inputs[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convrelu(in_channels, out_channels, kernel, padding):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel, padding=padding),\n",
    "        nn.ReLU(inplace=True),\n",
    "    )\n",
    "\n",
    "\n",
    "class ResNetUNet(nn.Module):\n",
    "    def __init__(self, n_class):\n",
    "        super().__init__()\n",
    "\n",
    "        self.base_model = models.resnet18(pretrained=True)\n",
    "        self.base_layers = list(self.base_model.children())\n",
    "\n",
    "        self.layer0 = nn.Sequential(*self.base_layers[:3]) # size=(N, 64, x.H/2, x.W/2)\n",
    "        self.layer0_1x1 = convrelu(64, 64, 1, 0)\n",
    "        self.layer1 = nn.Sequential(*self.base_layers[3:5]) # size=(N, 64, x.H/4, x.W/4)\n",
    "        self.layer1_1x1 = convrelu(64, 64, 1, 0)\n",
    "        self.layer2 = self.base_layers[5]  # size=(N, 128, x.H/8, x.W/8)\n",
    "        self.layer2_1x1 = convrelu(128, 128, 1, 0)\n",
    "        self.layer3 = self.base_layers[6]  # size=(N, 256, x.H/16, x.W/16)\n",
    "        self.layer3_1x1 = convrelu(256, 256, 1, 0)\n",
    "        self.layer4 = self.base_layers[7]  # size=(N, 512, x.H/32, x.W/32)\n",
    "        self.layer4_1x1 = convrelu(512, 512, 1, 0)\n",
    "\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "        self.conv_up3 = convrelu(256 + 512, 512, 3, 1)\n",
    "        self.conv_up2 = convrelu(128 + 512, 256, 3, 1)\n",
    "        self.conv_up1 = convrelu(64 + 256, 256, 3, 1)\n",
    "        self.conv_up0 = convrelu(64 + 256, 128, 3, 1)\n",
    "\n",
    "        self.conv_original_size0 = convrelu(3, 64, 3, 1)\n",
    "        self.conv_original_size1 = convrelu(64, 64, 3, 1)\n",
    "        self.conv_original_size2 = convrelu(64 + 128, 64, 3, 1)\n",
    "\n",
    "        self.conv_last = nn.Conv2d(64, n_class, 1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x_original = self.conv_original_size0(input)\n",
    "        x_original = self.conv_original_size1(x_original)\n",
    "\n",
    "        layer0 = self.layer0(input)\n",
    "        layer1 = self.layer1(layer0)\n",
    "        layer2 = self.layer2(layer1)\n",
    "        layer3 = self.layer3(layer2)\n",
    "        layer4 = self.layer4(layer3)\n",
    "\n",
    "        layer4 = self.layer4_1x1(layer4)\n",
    "        x = self.upsample(layer4)\n",
    "        layer3 = self.layer3_1x1(layer3)\n",
    "        x = torch.cat([x, layer3], dim=1)\n",
    "        x = self.conv_up3(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer2 = self.layer2_1x1(layer2)\n",
    "        x = torch.cat([x, layer2], dim=1)\n",
    "        x = self.conv_up2(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer1 = self.layer1_1x1(layer1)\n",
    "        x = torch.cat([x, layer1], dim=1)\n",
    "        x = self.conv_up1(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer0 = self.layer0_1x1(layer0)\n",
    "        x = torch.cat([x, layer0], dim=1)\n",
    "        x = self.conv_up0(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, x_original], dim=1)\n",
    "        x = self.conv_original_size2(x)\n",
    "\n",
    "        out = self.conv_last(x)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = ResNetUNet(n_class=2)\n",
    "# model = model.to(device)\n",
    "\n",
    "# check keras-like model summary using torchsummary\n",
    "# from torchsummary import summary\n",
    "# summary(model, input_size=(3, 224, 224))\n",
    "# summary(model, input_size=(3, 512, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calc_loss(pred, target, metrics, bce_weight=0.5):\n",
    "    \n",
    "#     #add a new dimension after batch and broadcast/copy the predictions over this dimension\n",
    "#     broadcast_preds = batch_pred.unsqueeze(1).repeat(1,24,1,1,1)\n",
    "    \n",
    "#     #create the list of permutations of 4 group labels\n",
    "#     perms = torch.tensor([x for x in itertools.permutations([0,1,2,3])]).type(torch.long)\n",
    "    \n",
    "#     #create the permuted targets\n",
    "#     perms_target = target[:,perms,:,:]\n",
    "    \n",
    "#     #get the BCE loss\n",
    "#     batch_mins, _ = torch.min(torch.mean(F.binary_cross_entropy_with_logits(broadcast_preds, perms_target, reduction = 'none'), dim = (2,3,4)), dim = 1)\n",
    "#     bce = torch.mean(batch_mins)\n",
    "    \n",
    "#     #get the dice loss\n",
    "#     broadcast_preds = F.sigmoid(broadcast_preds)    \n",
    "#     smooth = 1\n",
    "    \n",
    "#     intersection = (broadcast_preds*perms_target).sum(dim=3).sum(dim=3)\n",
    "#     loss = (1 - ((2. * intersection + smooth) / (broadcast_preds.sum(dim=3).sum(dim=3) + perms_target.sum(dim=3).sum(dim=3) + smooth)))\n",
    "#     batch_dice_mins, _ = torch.min(torch.mean(loss, dim = 2), dim = 1)\n",
    "#     dice = torch.mean(batch_dice_mins)\n",
    "    \n",
    "#     #combine losses\n",
    "#     loss = bce * bce_weight + dice * (1 - bce_weight)\n",
    "\n",
    "#     metrics['bce'] += bce.data.cpu().numpy() * target.size(0)\n",
    "#     metrics['dice'] += dice.data.cpu().numpy() * target.size(0)\n",
    "#     metrics['loss'] += loss.data.cpu().numpy() * target.size(0)\n",
    "\n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(pred, target, metrics, bce_weight=0.5):\n",
    "    bce = F.binary_cross_entropy_with_logits(pred, target)\n",
    "\n",
    "    pred = F.sigmoid(pred)\n",
    "    dice = Unet_loss.dice_loss(pred, target)\n",
    "\n",
    "    loss = bce * bce_weight + dice * (1 - bce_weight)\n",
    "\n",
    "    metrics['bce'] += bce.data.cpu().numpy() * target.size(0)\n",
    "    metrics['dice'] += dice.data.cpu().numpy() * target.size(0)\n",
    "    metrics['loss'] += loss.data.cpu().numpy() * target.size(0)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def print_metrics(metrics, epoch_samples, phase):\n",
    "    outputs = []\n",
    "    for k in metrics.keys():\n",
    "        outputs.append(\"{}: {:4f}\".format(k, metrics[k] / epoch_samples))\n",
    "\n",
    "    print(\"{}: {}\".format(phase, \", \".join(outputs)))\n",
    "\n",
    "def train_model(model, optimizer, scheduler, num_epochs=25):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 1e10\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        #get new data every epoch\n",
    "        dataloaders = {\n",
    "            'train': DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0),\n",
    "            'val': DataLoader(val_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "        }\n",
    "        \n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        since = time.time()\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    print(\"LR\", param_group['lr'])\n",
    "\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            metrics = defaultdict(float)\n",
    "            epoch_samples = 0\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = calc_loss(outputs, labels, metrics)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                epoch_samples += inputs.size(0)\n",
    "\n",
    "            print_metrics(metrics, epoch_samples, phase)\n",
    "            epoch_loss = metrics['loss'] / epoch_samples\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                print(\"saving best model\")\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print('{:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    print('Best val loss: {:4f}'.format(best_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'Unet.loss' from '/home/a/anaconda3/lib/python3.7/site-packages/Unet/loss.py'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(Unet_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Epoch 0/59\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.014710, dice: 0.399362, loss: 0.207036\n",
      "val: bce: 0.002163, dice: 0.388959, loss: 0.195561\n",
      "saving best model\n",
      "1m 17s\n",
      "Epoch 1/59\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.003718, dice: 0.337993, loss: 0.170856\n",
      "val: bce: 0.001987, dice: 0.288455, loss: 0.145221\n",
      "saving best model\n",
      "1m 17s\n",
      "Epoch 2/59\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.003251, dice: 0.245092, loss: 0.124171\n",
      "val: bce: 0.001056, dice: 0.142261, loss: 0.071659\n",
      "saving best model\n",
      "1m 17s\n",
      "Epoch 3/59\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000744, dice: 0.126795, loss: 0.063769\n",
      "val: bce: 0.001218, dice: 0.128444, loss: 0.064831\n",
      "saving best model\n",
      "1m 18s\n",
      "Epoch 4/59\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000673, dice: 0.119671, loss: 0.060172\n",
      "val: bce: 0.000740, dice: 0.135802, loss: 0.068271\n",
      "1m 18s\n",
      "Epoch 5/59\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000595, dice: 0.111041, loss: 0.055818\n",
      "val: bce: 0.000697, dice: 0.129651, loss: 0.065174\n",
      "1m 17s\n",
      "Epoch 6/59\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000698, dice: 0.113710, loss: 0.057204\n",
      "val: bce: 0.001015, dice: 0.125026, loss: 0.063021\n",
      "saving best model\n",
      "1m 17s\n",
      "Epoch 7/59\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000470, dice: 0.091681, loss: 0.046075\n",
      "val: bce: 0.000474, dice: 0.077052, loss: 0.038763\n",
      "saving best model\n",
      "1m 18s\n",
      "Epoch 8/59\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000344, dice: 0.066517, loss: 0.033430\n",
      "val: bce: 0.000385, dice: 0.064982, loss: 0.032683\n",
      "saving best model\n",
      "1m 18s\n",
      "Epoch 9/59\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000351, dice: 0.066839, loss: 0.033595\n",
      "val: bce: 0.000394, dice: 0.056851, loss: 0.028623\n",
      "saving best model\n",
      "1m 18s\n",
      "Epoch 10/59\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000325, dice: 0.057241, loss: 0.028783\n",
      "val: bce: 0.000726, dice: 0.095570, loss: 0.048148\n",
      "1m 17s\n",
      "Epoch 11/59\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000330, dice: 0.057938, loss: 0.029134\n",
      "val: bce: 0.000391, dice: 0.057804, loss: 0.029097\n",
      "1m 17s\n",
      "Epoch 12/59\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000309, dice: 0.054077, loss: 0.027193\n",
      "val: bce: 0.000487, dice: 0.062640, loss: 0.031564\n",
      "1m 17s\n",
      "Epoch 13/59\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000300, dice: 0.052026, loss: 0.026163\n",
      "val: bce: 0.000411, dice: 0.061466, loss: 0.030938\n",
      "1m 17s\n",
      "Epoch 14/59\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000260, dice: 0.045608, loss: 0.022934\n",
      "val: bce: 0.000430, dice: 0.057798, loss: 0.029114\n",
      "1m 17s\n",
      "Epoch 15/59\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000383, dice: 0.053224, loss: 0.026804\n",
      "val: bce: 0.000784, dice: 0.074341, loss: 0.037563\n",
      "1m 17s\n",
      "Epoch 16/59\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000345, dice: 0.051288, loss: 0.025817\n",
      "val: bce: 0.000412, dice: 0.059632, loss: 0.030022\n",
      "1m 17s\n",
      "Epoch 17/59\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000270, dice: 0.040697, loss: 0.020484\n",
      "val: bce: 0.000415, dice: 0.058553, loss: 0.029484\n",
      "1m 17s\n",
      "Epoch 18/59\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000286, dice: 0.041639, loss: 0.020962\n",
      "val: bce: 0.000446, dice: 0.053563, loss: 0.027004\n",
      "saving best model\n",
      "1m 17s\n",
      "Epoch 19/59\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000266, dice: 0.040984, loss: 0.020625\n",
      "val: bce: 0.000369, dice: 0.047801, loss: 0.024085\n",
      "saving best model\n",
      "1m 17s\n",
      "Epoch 20/59\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000248, dice: 0.038375, loss: 0.019311\n",
      "val: bce: 0.000354, dice: 0.046564, loss: 0.023459\n",
      "saving best model\n",
      "1m 17s\n",
      "Epoch 21/59\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000260, dice: 0.038433, loss: 0.019346\n",
      "val: bce: 0.000441, dice: 0.051477, loss: 0.025959\n",
      "1m 17s\n",
      "Epoch 22/59\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000312, dice: 0.042342, loss: 0.021327\n",
      "val: bce: 0.000454, dice: 0.053035, loss: 0.026745\n",
      "1m 17s\n",
      "Epoch 23/59\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000269, dice: 0.038392, loss: 0.019330\n",
      "val: bce: 0.000445, dice: 0.049318, loss: 0.024882\n",
      "1m 17s\n",
      "Epoch 24/59\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000289, dice: 0.038579, loss: 0.019434\n",
      "val: bce: 0.000468, dice: 0.051628, loss: 0.026048\n",
      "1m 17s\n",
      "Epoch 25/59\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000249, dice: 0.034371, loss: 0.017310\n",
      "val: bce: 0.000582, dice: 0.056171, loss: 0.028377\n",
      "1m 17s\n",
      "Epoch 26/59\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000322, dice: 0.040644, loss: 0.020483\n",
      "val: bce: 0.000558, dice: 0.051366, loss: 0.025962\n",
      "1m 17s\n",
      "Epoch 27/59\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000244, dice: 0.032812, loss: 0.016528\n",
      "val: bce: 0.000381, dice: 0.042054, loss: 0.021217\n",
      "saving best model\n",
      "1m 18s\n",
      "Epoch 28/59\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000199, dice: 0.028905, loss: 0.014552\n",
      "val: bce: 0.000377, dice: 0.042540, loss: 0.021458\n",
      "1m 17s\n",
      "Epoch 29/59\n",
      "----------\n",
      "LR 1e-05\n",
      "train: bce: 0.000175, dice: 0.026395, loss: 0.013285\n",
      "val: bce: 0.000339, dice: 0.040275, loss: 0.020307\n",
      "saving best model\n",
      "1m 18s\n",
      "Epoch 30/59\n",
      "----------\n",
      "LR 1e-05\n",
      "train: bce: 0.000155, dice: 0.023853, loss: 0.012004\n",
      "val: bce: 0.000320, dice: 0.039627, loss: 0.019973\n",
      "saving best model\n",
      "1m 17s\n",
      "Epoch 31/59\n",
      "----------\n",
      "LR 1e-05\n",
      "train: bce: 0.000146, dice: 0.023341, loss: 0.011744\n",
      "val: bce: 0.000311, dice: 0.039791, loss: 0.020051\n",
      "1m 17s\n",
      "Epoch 32/59\n",
      "----------\n",
      "LR 1e-05\n",
      "train: bce: 0.000137, dice: 0.021847, loss: 0.010992\n",
      "val: bce: 0.000301, dice: 0.036861, loss: 0.018581\n",
      "saving best model\n",
      "1m 17s\n",
      "Epoch 33/59\n",
      "----------\n",
      "LR 1e-05\n",
      "train: bce: 0.000131, dice: 0.021787, loss: 0.010959\n",
      "val: bce: 0.000307, dice: 0.038261, loss: 0.019284\n",
      "1m 17s\n",
      "Epoch 34/59\n",
      "----------\n",
      "LR 1e-05\n",
      "train: bce: 0.000126, dice: 0.020877, loss: 0.010501\n",
      "val: bce: 0.000303, dice: 0.037281, loss: 0.018792\n",
      "1m 17s\n",
      "Epoch 35/59\n",
      "----------\n",
      "LR 1e-05\n",
      "train: bce: 0.000121, dice: 0.020345, loss: 0.010233\n",
      "val: bce: 0.000295, dice: 0.038473, loss: 0.019384\n",
      "1m 17s\n",
      "Epoch 36/59\n",
      "----------\n",
      "LR 1e-05\n",
      "train: bce: 0.000115, dice: 0.019470, loss: 0.009792\n",
      "val: bce: 0.000301, dice: 0.039325, loss: 0.019813\n",
      "1m 17s\n",
      "Epoch 37/59\n",
      "----------\n",
      "LR 1e-05\n",
      "train: bce: 0.000111, dice: 0.019598, loss: 0.009855\n",
      "val: bce: 0.000295, dice: 0.039467, loss: 0.019881\n",
      "1m 17s\n",
      "Epoch 38/59\n",
      "----------\n",
      "LR 1e-05\n",
      "train: bce: 0.000108, dice: 0.019148, loss: 0.009628\n",
      "val: bce: 0.000295, dice: 0.038725, loss: 0.019510\n",
      "1m 17s\n",
      "Epoch 39/59\n",
      "----------\n",
      "LR 1e-05\n",
      "train: bce: 0.000109, dice: 0.019047, loss: 0.009578\n",
      "val: bce: 0.000303, dice: 0.040171, loss: 0.020237\n",
      "1m 17s\n",
      "Epoch 40/59\n",
      "----------\n",
      "LR 1e-05\n",
      "train: bce: 0.000103, dice: 0.018541, loss: 0.009322\n",
      "val: bce: 0.000286, dice: 0.037091, loss: 0.018688\n",
      "1m 17s\n",
      "Epoch 41/59\n",
      "----------\n",
      "LR 1e-05\n",
      "train: bce: 0.000100, dice: 0.017601, loss: 0.008851\n",
      "val: bce: 0.000283, dice: 0.037616, loss: 0.018950\n",
      "1m 17s\n",
      "Epoch 42/59\n",
      "----------\n",
      "LR 1e-05\n",
      "train: bce: 0.000100, dice: 0.017199, loss: 0.008650\n",
      "val: bce: 0.000278, dice: 0.036863, loss: 0.018570\n",
      "saving best model\n",
      "1m 17s\n",
      "Epoch 43/59\n",
      "----------\n",
      "LR 1e-05\n",
      "train: bce: 0.000097, dice: 0.017003, loss: 0.008550\n",
      "val: bce: 0.000297, dice: 0.037345, loss: 0.018821\n",
      "1m 17s\n",
      "Epoch 44/59\n",
      "----------\n",
      "LR 1e-05\n",
      "train: bce: 0.000098, dice: 0.017171, loss: 0.008634\n",
      "val: bce: 0.000294, dice: 0.036617, loss: 0.018456\n",
      "saving best model\n",
      "1m 17s\n",
      "Epoch 45/59\n",
      "----------\n",
      "LR 1e-05\n",
      "train: bce: 0.000095, dice: 0.016819, loss: 0.008457\n",
      "val: bce: 0.000301, dice: 0.039118, loss: 0.019709\n",
      "1m 17s\n",
      "Epoch 46/59\n",
      "----------\n",
      "LR 1e-05\n",
      "train: bce: 0.000099, dice: 0.017245, loss: 0.008672\n",
      "val: bce: 0.000284, dice: 0.036633, loss: 0.018459\n",
      "1m 17s\n",
      "Epoch 47/59\n",
      "----------\n",
      "LR 1e-05\n",
      "train: bce: 0.000094, dice: 0.016353, loss: 0.008223\n",
      "val: bce: 0.000278, dice: 0.036662, loss: 0.018470\n",
      "1m 17s\n",
      "Epoch 48/59\n",
      "----------\n",
      "LR 1e-05\n",
      "train: bce: 0.000092, dice: 0.016050, loss: 0.008071\n",
      "val: bce: 0.000283, dice: 0.037009, loss: 0.018646\n",
      "1m 17s\n",
      "Epoch 49/59\n",
      "----------\n",
      "LR 1e-05\n",
      "train: bce: 0.000090, dice: 0.015999, loss: 0.008045\n",
      "val: bce: 0.000288, dice: 0.036937, loss: 0.018612\n",
      "1m 17s\n",
      "Epoch 50/59\n",
      "----------\n",
      "LR 1e-05\n",
      "train: bce: 0.000089, dice: 0.016099, loss: 0.008094\n",
      "val: bce: 0.000301, dice: 0.037800, loss: 0.019051\n",
      "1m 17s\n",
      "Epoch 51/59\n",
      "----------\n",
      "LR 1e-05\n",
      "train: bce: 0.000089, dice: 0.015843, loss: 0.007966\n",
      "val: bce: 0.000272, dice: 0.036716, loss: 0.018494\n",
      "1m 18s\n",
      "Epoch 52/59\n",
      "----------\n",
      "LR 1e-05\n",
      "train: bce: 0.000087, dice: 0.015415, loss: 0.007751\n",
      "val: bce: 0.000283, dice: 0.037674, loss: 0.018978\n",
      "1m 18s\n",
      "Epoch 53/59\n",
      "----------\n",
      "LR 1e-05\n",
      "train: bce: 0.000086, dice: 0.015468, loss: 0.007777\n",
      "val: bce: 0.000294, dice: 0.037879, loss: 0.019086\n",
      "1m 17s\n",
      "Epoch 54/59\n",
      "----------\n",
      "LR 1e-05\n",
      "train: bce: 0.000086, dice: 0.015133, loss: 0.007609\n",
      "val: bce: 0.000285, dice: 0.037846, loss: 0.019065\n",
      "1m 17s\n",
      "Epoch 55/59\n",
      "----------\n",
      "LR 1e-05\n",
      "train: bce: 0.000085, dice: 0.014808, loss: 0.007447\n",
      "val: bce: 0.000283, dice: 0.037817, loss: 0.019050\n",
      "1m 17s\n",
      "Epoch 56/59\n",
      "----------\n",
      "LR 1e-05\n",
      "train: bce: 0.000085, dice: 0.014897, loss: 0.007491\n",
      "val: bce: 0.000280, dice: 0.035969, loss: 0.018125\n",
      "saving best model\n",
      "1m 17s\n",
      "Epoch 57/59\n",
      "----------\n",
      "LR 1e-05\n",
      "train: bce: 0.000085, dice: 0.014524, loss: 0.007305\n",
      "val: bce: 0.000284, dice: 0.037361, loss: 0.018822\n",
      "1m 17s\n",
      "Epoch 58/59\n",
      "----------\n",
      "LR 1e-05\n",
      "train: bce: 0.000083, dice: 0.014655, loss: 0.007369\n",
      "val: bce: 0.000284, dice: 0.038426, loss: 0.019355\n",
      "1m 17s\n",
      "Epoch 59/59\n",
      "----------\n",
      "LR 1.0000000000000002e-06\n",
      "train: bce: 0.000082, dice: 0.014224, loss: 0.007153\n",
      "val: bce: 0.000283, dice: 0.037900, loss: 0.019092\n",
      "1m 17s\n",
      "Best val loss: 0.018125\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "num_class = 4\n",
    "model = ResNetUNet(num_class).to(device)\n",
    "\n",
    "# freeze backbone layers\n",
    "#for l in model.base_layers:\n",
    "#    for param in l.parameters():\n",
    "#        param.requires_grad = False\n",
    "\n",
    "optimizer_ft = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=30, gamma=0.1)\n",
    "\n",
    "model = train_model(model, optimizer_ft, exp_lr_scheduler, num_epochs=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNetUNet(\n",
       "  (base_model): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       "  )\n",
       "  (layer0): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (layer0_1x1): Sequential(\n",
       "    (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (layer1): Sequential(\n",
       "    (0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer1_1x1): Sequential(\n",
       "    (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2_1x1): Sequential(\n",
       "    (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3_1x1): Sequential(\n",
       "    (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4_1x1): Sequential(\n",
       "    (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (upsample): Upsample(scale_factor=2.0, mode=bilinear)\n",
       "  (conv_up3): Sequential(\n",
       "    (0): Conv2d(768, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv_up2): Sequential(\n",
       "    (0): Conv2d(640, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv_up1): Sequential(\n",
       "    (0): Conv2d(320, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv_up0): Sequential(\n",
       "    (0): Conv2d(320, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv_original_size0): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv_original_size1): Sequential(\n",
       "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv_original_size2): Sequential(\n",
       "    (0): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv_last): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://discuss.pytorch.org/t/save-and-load-model/6206/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/a/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ResNetUNet. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/a/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ResNet. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/a/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Conv2d. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/a/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BatchNorm2d. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/a/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/a/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type MaxPool2d. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/a/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Sequential. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/a/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BasicBlock. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/a/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type AdaptiveAvgPool2d. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/a/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/a/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Upsample. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(model, '/home/a/Documents/GNN-for-trans-grouping/GNN-for-trans-grouping/Unet_model_multi_seq_strict_encoding.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('/home/a/Documents/GNN-for-trans-grouping/GNN-for-trans-grouping/Unet_model_multi_seq_strict_encoding.pt')\n",
    "# model = torch.load(r'C:\\Users\\andy.knapper\\Documents\\OW\\Categorisation\\ML grouping\\GNN-for-trans-grouping\\Unet_model_1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sim_data = [group_to_image_constructors.make_an_image(*np.array(synthetic_data.make_a_group())) for _ in range(3)]\n",
    "# input_images = np.array([x[0] for  x in sim_data]).astype('uint8')\n",
    "# target_masks = np.array([x[1] for  x in sim_data]).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# output = model(torch.tensor(test_img))\n",
    "# prediction = torch.argmax(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model, input_size=(3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = SimDataset(1000, transform = trans)\n",
    "val_set = SimDataset(2, transform = trans)\n",
    "\n",
    "image_datasets = {\n",
    "    'train': train_set, 'val': val_set\n",
    "}\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "dataloaders = {\n",
    "    'train': DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0),\n",
    "    'val': DataLoader(val_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_preds_and_targets(inp_d_arr, inp_a_arr, inp_pred_arr, inp_targets_arr, inp_ari, i):\n",
    "    colour_list = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'tab:brown', 'tab:pink', 'tab:gray', 'tab:olive', 'tab:cyan']\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(10,16), sharex=True)\n",
    "    \n",
    "    for p in inp_pred_arr:\n",
    "        mask = (inp_pred_arr == p)\n",
    "        ax1.scatter(inp_d_arr[mask], inp_a_arr[mask], s=10, c=colour_list[p%10], marker='x')\n",
    "\n",
    "    for t in inp_targets_arr:\n",
    "        mask = (inp_targets_arr == t)\n",
    "        ax2.scatter(inp_d_arr[mask], inp_a_arr[mask], s=10, c=colour_list[t%10], marker='x')\n",
    "        \n",
    "    for ax in fig.axes:\n",
    "        matplotlib.pyplot.sca(ax)\n",
    "        plt.xticks(rotation=90)\n",
    "        #plt.legend(bbox_to_anchor=(1.02, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "    plt.savefig('/home/a/Documents/GNN-for-trans-grouping/GNN-for-trans-grouping/Charts/'+str(ari)+'_'+str(i)+'.png')\n",
    "        \n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.0\n",
      "1 0.8260516282267641\n",
      "2 1.0\n",
      "3 1.0\n",
      "4 0.4920320002736528\n",
      "5 1.0\n",
      "6 1.0\n",
      "7 1.0\n",
      "8 1.0\n",
      "9 1.0\n",
      "10 1.0\n",
      "11 1.0\n",
      "12 1.0\n",
      "13 1.0\n",
      "14 0.7980316760332357\n",
      "15 0.437037037037037\n",
      "16 0.491038424502221\n",
      "17 0.7755615248328285\n",
      "18 1.0\n",
      "19 0.8083751868711471\n",
      "20 1.0\n",
      "21 0.8181161663055814\n",
      "22 1.0\n",
      "23 0.6487889273356401\n",
      "24 0.8629156747188814\n",
      "25 0.46852362622374444\n",
      "26 1.0\n",
      "27 0.707956210671494\n",
      "28 1.0\n",
      "29 1.0\n",
      "30 1.0\n",
      "31 1.0\n",
      "32 1.0\n",
      "33 0.6976166832174777\n",
      "34 1.0\n",
      "35 1.0\n",
      "36 0.49608884912885526\n",
      "37 1.0\n",
      "38 0.52\n",
      "39 0.8918128654970761\n",
      "40 1.0\n",
      "41 1.0\n",
      "42 0.8866298342541437\n",
      "43 1.0\n",
      "44 1.0\n",
      "45 1.0\n",
      "46 1.0\n",
      "47 0.010694637486201586\n",
      "48 1.0\n",
      "49 1.0\n",
      "50 1.0\n",
      "51 1.0\n",
      "52 1.0\n",
      "53 0.554336449272747\n",
      "54 0.797182881773399\n",
      "55 1.0\n",
      "56 0.6180146679062081\n",
      "57 1.0\n",
      "58 0.4101694915254237\n",
      "59 0.8478077231648251\n",
      "60 1.0\n",
      "61 1.0\n",
      "62 1.0\n",
      "63 1.0\n",
      "64 1.0\n",
      "65 -0.052599009900990173\n",
      "66 0.8256667955160417\n",
      "67 1.0\n",
      "68 1.0\n",
      "69 0.7490911294231701\n",
      "70 1.0\n",
      "71 1.0\n",
      "72 1.0\n",
      "73 1.0\n",
      "74 1.0\n",
      "75 0.7895816242821986\n",
      "76 1.0\n",
      "77 1.0\n",
      "78 1.0\n",
      "79 0.3241917213215293\n",
      "80 1.0\n",
      "81 0.09188082783716175\n",
      "82 1.0\n",
      "83 0.8820681307279958\n",
      "84 1.0\n",
      "85 1.0\n",
      "86 1.0\n",
      "87 1.0\n",
      "88 0.3478260869565217\n",
      "89 1.0\n",
      "90 1.0\n",
      "91 1.0\n",
      "92 1.0\n",
      "93 1.0\n",
      "94 1.0\n",
      "95 1.0\n",
      "96 1.0\n",
      "97 1.0\n",
      "98 1.0\n",
      "99 0.9075636416990683\n",
      "100 0.8803225260306442\n",
      "101 1.0\n",
      "102 1.0\n",
      "103 0.8709480461811723\n",
      "104 1.0\n",
      "105 0.007747318235995299\n",
      "106 1.0\n",
      "107 1.0\n",
      "108 1.0\n",
      "109 1.0\n",
      "110 1.0\n",
      "111 1.0\n",
      "112 0.7155842733545067\n",
      "113 1.0\n",
      "114 0.6356795329224781\n",
      "115 0.9752463900985561\n",
      "116 0.7718120805369127\n",
      "117 1.0\n",
      "118 0.0\n",
      "119 0.7339836924868958\n",
      "120 0.9002394253790902\n",
      "121 1.0\n",
      "122 0.845124956001408\n",
      "123 1.0\n",
      "124 0.6578947368421053\n",
      "125 0.7447399582599756\n",
      "126 1.0\n",
      "127 1.0\n",
      "128 1.0\n",
      "129 1.0\n",
      "130 1.0\n",
      "131 1.0\n",
      "132 1.0\n",
      "133 1.0\n",
      "134 0.5817519148294809\n",
      "135 0.7209439586301931\n",
      "136 1.0\n",
      "137 0.0\n",
      "138 1.0\n",
      "139 0.8773263861604579\n",
      "140 0.6229450856942987\n",
      "141 1.0\n",
      "142 1.0\n",
      "143 1.0\n",
      "144 0.8741305067903279\n",
      "145 1.0\n",
      "146 1.0\n",
      "147 1.0\n",
      "148 0.42621310655327665\n",
      "149 1.0\n",
      "150 0.7963201325538617\n",
      "151 1.0\n",
      "152 1.0\n",
      "153 1.0\n",
      "154 0.787422687111298\n",
      "155 1.0\n",
      "156 1.0\n",
      "157 0.12077294685990342\n",
      "158 1.0\n",
      "159 1.0\n",
      "160 0.6868769074262462\n",
      "161 1.0\n",
      "162 1.0\n",
      "163 1.0\n",
      "164 1.0\n",
      "165 0.8476283846687527\n",
      "166 0.0\n",
      "167 0.8401360544217688\n",
      "168 0.2805536542702867\n",
      "169 1.0\n",
      "170 0.24393020477584643\n",
      "171 1.0\n",
      "172 0.8965869609646434\n",
      "173 1.0\n",
      "174 1.0\n",
      "175 1.0\n",
      "176 1.0\n",
      "177 1.0\n",
      "178 1.0\n",
      "179 1.0\n",
      "180 1.0\n",
      "181 1.0\n",
      "182 1.0\n",
      "183 1.0\n",
      "184 0.15384615384615394\n",
      "185 1.0\n",
      "186 1.0\n",
      "187 1.0\n",
      "188 0.8976377952755905\n",
      "189 0.10773791270571398\n",
      "190 0.9513243655526279\n",
      "191 1.0\n",
      "192 1.0\n",
      "193 0.7776739841195702\n",
      "194 1.0\n",
      "195 1.0\n",
      "196 0.7426905526449215\n",
      "197 1.0\n",
      "198 1.0\n",
      "199 1.0\n",
      "200 1.0\n",
      "201 0.2636924192963365\n",
      "202 1.0\n",
      "203 1.0\n",
      "204 1.0\n",
      "205 -0.16666666666666657\n",
      "206 1.0\n",
      "207 1.0\n",
      "208 0.9204668581010098\n",
      "209 1.0\n",
      "210 1.0\n",
      "211 1.0\n",
      "212 1.0\n",
      "213 1.0\n",
      "214 0.9114696903619712\n",
      "215 0.9184329937256149\n",
      "216 1.0\n",
      "217 1.0\n",
      "218 0.32863849765258224\n",
      "219 1.0\n",
      "220 0.0\n",
      "221 1.0\n",
      "222 1.0\n",
      "223 1.0\n",
      "224 1.0\n",
      "225 1.0\n",
      "226 0.8112339930151339\n",
      "227 0.7473971667520055\n",
      "228 1.0\n",
      "229 1.0\n",
      "230 1.0\n",
      "231 1.0\n",
      "232 1.0\n",
      "233 1.0\n",
      "234 1.0\n",
      "235 1.0\n",
      "236 0.8366468418389422\n",
      "237 0.6679566563467492\n",
      "238 1.0\n",
      "239 0.017463498425422285\n",
      "240 1.0\n",
      "241 1.0\n",
      "242 0.6829020285728812\n",
      "243 0.7058823529411765\n",
      "244 1.0\n",
      "245 1.0\n",
      "246 0.0\n",
      "247 1.0\n",
      "248 1.0\n",
      "249 1.0\n",
      "250 1.0\n",
      "251 1.0\n",
      "252 1.0\n",
      "253 1.0\n",
      "254 1.0\n",
      "255 1.0\n",
      "256 1.0\n",
      "257 0.7668601899336057\n",
      "258 1.0\n",
      "259 1.0\n",
      "260 1.0\n",
      "261 1.0\n",
      "262 0.9233549049392735\n",
      "263 1.0\n",
      "264 0.8308777138615698\n",
      "265 0.4564234901227498\n",
      "266 0.826269007566297\n",
      "267 0.7907984401465317\n",
      "268 1.0\n",
      "269 1.0\n",
      "270 0.905114222549742\n",
      "271 1.0\n",
      "272 1.0\n",
      "273 1.0\n",
      "274 1.0\n",
      "275 1.0\n",
      "276 0.7611438760115211\n",
      "277 1.0\n",
      "278 1.0\n",
      "279 1.0\n",
      "280 1.0\n",
      "281 0.81078217513244\n",
      "282 0.5257214554579673\n",
      "283 0.7623496912577187\n",
      "284 1.0\n",
      "285 1.0\n",
      "286 1.0\n",
      "287 0.0\n",
      "288 1.0\n",
      "289 1.0\n",
      "290 1.0\n",
      "291 -0.016640665626625004\n",
      "292 0.8789285595003506\n",
      "293 0.1549789652405588\n",
      "294 1.0\n",
      "295 1.0\n",
      "296 1.0\n",
      "297 1.0\n",
      "298 1.0\n",
      "299 1.0\n",
      "300 1.0\n",
      "301 0.8332457707260691\n",
      "302 1.0\n",
      "303 1.0\n",
      "304 0.5477386934673367\n",
      "305 0.7553103327953941\n",
      "306 0.40413793103448276\n",
      "307 0.7363945578231292\n",
      "308 1.0\n",
      "309 0.7585034013605442\n",
      "310 -0.13195342820181097\n",
      "311 1.0\n",
      "312 1.0\n",
      "313 0.7828359075600976\n",
      "314 0.008152553256833287\n",
      "315 1.0\n",
      "316 0.5284507042253521\n",
      "317 0.4261340159432413\n",
      "318 0.9594932749452612\n",
      "319 1.0\n",
      "320 1.0\n",
      "321 1.0\n",
      "322 1.0\n",
      "323 1.0\n",
      "324 1.0\n",
      "325 1.0\n",
      "326 1.0\n",
      "327 1.0\n",
      "328 1.0\n",
      "329 1.0\n",
      "330 1.0\n",
      "331 1.0\n",
      "332 1.0\n",
      "333 1.0\n",
      "334 0.8598564136560923\n",
      "335 1.0\n",
      "336 0.8207727257504549\n",
      "337 1.0\n",
      "338 1.0\n",
      "339 0.8158521788705835\n",
      "340 1.0\n",
      "341 1.0\n",
      "342 1.0\n",
      "343 0.35476718403547675\n",
      "344 0.0\n",
      "345 0.9413688513780826\n",
      "346 1.0\n",
      "347 1.0\n",
      "348 1.0\n",
      "349 1.0\n",
      "350 1.0\n",
      "351 1.0\n",
      "352 0.7940052709795312\n",
      "353 1.0\n",
      "354 1.0\n",
      "355 1.0\n",
      "356 1.0\n",
      "357 1.0\n",
      "358 1.0\n",
      "359 1.0\n",
      "360 1.0\n",
      "361 0.9622233389412047\n",
      "362 1.0\n",
      "363 0.07102193558660772\n",
      "364 1.0\n",
      "365 1.0\n",
      "366 1.0\n",
      "367 0.977760736196319\n",
      "368 0.25\n",
      "369 1.0\n",
      "370 1.0\n",
      "371 0.37536400698893424\n",
      "372 1.0\n",
      "373 0.8728507380066004\n",
      "374 1.0\n",
      "375 0.7559539980077877\n",
      "376 0.5131462249306212\n",
      "377 0.3033204954720516\n",
      "378 1.0\n",
      "379 1.0\n",
      "380 1.0\n",
      "381 0.813890645877232\n",
      "382 0.7886785236182826\n",
      "383 1.0\n",
      "384 1.0\n",
      "385 0.3383233532934132\n",
      "386 1.0\n",
      "387 1.0\n",
      "388 1.0\n",
      "389 0.9348914858096828\n",
      "390 1.0\n",
      "391 0.5166013261700789\n",
      "392 1.0\n",
      "393 1.0\n",
      "394 0.8405690458670592\n",
      "395 0.9316845634322919\n",
      "396 1.0\n",
      "397 1.0\n",
      "398 1.0\n",
      "399 1.0\n",
      "400 1.0\n",
      "401 0.8816705336426914\n",
      "402 0.9023809523809524\n",
      "403 1.0\n",
      "404 1.0\n",
      "405 1.0\n",
      "406 0.25905806655767105\n",
      "407 1.0\n",
      "408 1.0\n",
      "409 1.0\n",
      "410 1.0\n",
      "411 1.0\n",
      "412 1.0\n",
      "413 1.0\n",
      "414 1.0\n",
      "415 0.6918123275068997\n",
      "416 1.0\n",
      "417 1.0\n",
      "418 1.0\n",
      "419 1.0\n",
      "420 0.7607267764224175\n",
      "421 0.0\n",
      "422 1.0\n",
      "423 1.0\n",
      "424 1.0\n",
      "425 1.0\n",
      "426 1.0\n",
      "427 0.6027060270602707\n",
      "428 0.7986779651926739\n",
      "429 1.0\n",
      "430 1.0\n",
      "431 0.7472217049468012\n",
      "432 0.33974659784138905\n",
      "433 1.0\n",
      "434 1.0\n",
      "435 1.0\n",
      "436 0.6712938118667848\n",
      "437 1.0\n",
      "438 1.0\n",
      "439 0.1009783314271584\n",
      "440 0.7053206002728513\n",
      "441 1.0\n",
      "442 1.0\n",
      "443 1.0\n",
      "444 1.0\n",
      "445 1.0\n",
      "446 1.0\n",
      "447 1.0\n",
      "448 1.0\n",
      "449 1.0\n",
      "450 1.0\n",
      "451 1.0\n",
      "452 1.0\n",
      "453 0.9486823855755895\n",
      "454 1.0\n",
      "455 0.48712349910003955\n",
      "456 1.0\n",
      "457 0.7867237961245763\n",
      "458 0.5761348696929857\n",
      "459 0.2049081737069816\n",
      "460 1.0\n",
      "461 1.0\n",
      "462 0.0\n",
      "463 0.28739002932551316\n",
      "464 1.0\n",
      "465 0.4166666666666667\n",
      "466 1.0\n",
      "467 1.0\n",
      "468 0.7247830802603037\n",
      "469 1.0\n",
      "470 1.0\n",
      "471 1.0\n",
      "472 0.0\n",
      "473 1.0\n",
      "474 0.7035040431266847\n",
      "475 1.0\n",
      "476 1.0\n",
      "477 0.0\n",
      "478 1.0\n",
      "479 1.0\n",
      "480 0.8878804832861289\n",
      "481 1.0\n",
      "482 1.0\n",
      "483 0.7170539745660909\n",
      "484 0.32238869977753354\n",
      "485 0.20737027942937147\n",
      "486 1.0\n",
      "487 0.6661528085557376\n",
      "488 1.0\n",
      "489 0.6487889273356401\n",
      "490 1.0\n",
      "491 0.12068965517241378\n",
      "492 1.0\n",
      "493 1.0\n",
      "494 1.0\n",
      "495 1.0\n",
      "496 1.0\n",
      "497 1.0\n",
      "498 1.0\n",
      "499 1.0\n",
      "500 -0.020099513676102517\n",
      "501 1.0\n",
      "502 0.45614035087719296\n",
      "503 0.8542673833188201\n",
      "504 1.0\n",
      "505 0.7780015101938083\n",
      "506 1.0\n",
      "507 1.0\n",
      "508 0.04481470841712155\n",
      "509 1.0\n",
      "510 1.0\n",
      "511 1.0\n",
      "512 1.0\n",
      "513 0.9571291550810428\n",
      "514 1.0\n",
      "515 1.0\n",
      "516 1.0\n",
      "517 0.9365309962132939\n",
      "518 0.8289110337375435\n",
      "519 1.0\n",
      "520 0.33880765015941966\n",
      "521 0.5919208573784006\n",
      "522 1.0\n",
      "523 0.7082713218830028\n",
      "524 1.0\n",
      "525 1.0\n",
      "526 1.0\n",
      "527 1.0\n",
      "528 1.0\n",
      "529 1.0\n",
      "530 1.0\n",
      "531 -0.0429360514777063\n",
      "532 1.0\n",
      "533 1.0\n",
      "534 1.0\n",
      "535 1.0\n",
      "536 0.6936190940087676\n",
      "537 1.0\n",
      "538 1.0\n",
      "539 1.0\n",
      "540 0.4166666666666667\n",
      "541 0.9166320715552704\n",
      "542 0.8480354372579056\n",
      "543 1.0\n",
      "544 1.0\n",
      "545 0.712013165112452\n",
      "546 0.8576438743085795\n",
      "547 1.0\n",
      "548 1.0\n",
      "549 1.0\n",
      "550 1.0\n",
      "551 1.0\n",
      "552 1.0\n",
      "553 1.0\n",
      "554 1.0\n",
      "555 1.0\n",
      "556 0.0\n",
      "557 1.0\n",
      "558 0.9289617486338798\n",
      "559 1.0\n",
      "560 1.0\n",
      "561 1.0\n",
      "562 1.0\n",
      "563 1.0\n",
      "564 1.0\n",
      "565 1.0\n",
      "566 1.0\n",
      "567 -0.05076872027430601\n",
      "568 1.0\n",
      "569 1.0\n",
      "570 1.0\n",
      "571 0.5199168687218566\n",
      "572 1.0\n",
      "573 1.0\n",
      "574 1.0\n",
      "575 0.7520634182364149\n",
      "576 0.8999407933688574\n",
      "577 1.0\n",
      "578 1.0\n",
      "579 0.525328330206379\n",
      "580 0.6266724180314562\n",
      "581 1.0\n",
      "582 1.0\n",
      "583 0.21576493334823985\n",
      "584 0.4847972972972973\n",
      "585 1.0\n",
      "586 1.0\n",
      "587 1.0\n",
      "588 1.0\n",
      "589 0.08866206700334309\n",
      "590 1.0\n",
      "591 1.0\n",
      "592 1.0\n",
      "593 1.0\n",
      "594 1.0\n",
      "595 1.0\n",
      "596 1.0\n",
      "597 1.0\n",
      "598 1.0\n",
      "599 1.0\n",
      "600 0.9651581574155257\n",
      "601 1.0\n",
      "602 1.0\n",
      "603 0.7025983252673318\n",
      "604 1.0\n",
      "605 0.04721030042918461\n",
      "606 1.0\n",
      "607 0.9265328171423427\n",
      "608 1.0\n",
      "609 1.0\n",
      "610 1.0\n",
      "611 1.0\n",
      "612 1.0\n",
      "613 1.0\n",
      "614 0.7322939569591579\n",
      "615 1.0\n",
      "616 -0.030870697777531455\n",
      "617 1.0\n",
      "618 1.0\n",
      "619 1.0\n",
      "620 1.0\n",
      "621 0.7058823529411765\n",
      "622 0.4874141876430206\n",
      "623 1.0\n",
      "624 1.0\n",
      "625 0.8908782405696241\n",
      "626 0.7141592920353982\n",
      "627 0.34179435712203105\n",
      "628 1.0\n",
      "629 1.0\n",
      "630 0.7950769230769231\n",
      "631 1.0\n",
      "632 1.0\n",
      "633 1.0\n",
      "634 1.0\n",
      "635 1.0\n",
      "636 1.0\n",
      "637 0.7771462896220312\n",
      "638 1.0\n",
      "639 0.9626868413853924\n",
      "640 1.0\n",
      "641 1.0\n",
      "642 0.9173824044569185\n",
      "643 1.0\n",
      "644 1.0\n",
      "645 1.0\n",
      "646 0.8260869565217391\n",
      "647 0.8737342930340368\n",
      "648 1.0\n",
      "649 1.0\n",
      "650 0.37759081655179183\n",
      "651 0.7141190198366395\n",
      "652 0.33261165904899365\n",
      "653 1.0\n",
      "654 1.0\n",
      "655 0.9148832684824902\n",
      "656 1.0\n",
      "657 1.0\n",
      "658 0.34416826003824086\n",
      "659 1.0\n",
      "660 1.0\n",
      "661 1.0\n",
      "662 1.0\n",
      "663 1.0\n",
      "664 0.9259633134972644\n",
      "665 1.0\n",
      "666 1.0\n",
      "667 1.0\n",
      "668 1.0\n",
      "669 0.9573672400897532\n",
      "670 0.9352791878172589\n",
      "671 1.0\n",
      "672 -0.0059772863120142695\n",
      "673 0.7822376738305942\n",
      "674 1.0\n",
      "675 0.9322865554465162\n",
      "676 1.0\n",
      "677 1.0\n",
      "678 1.0\n",
      "679 0.8642269667521645\n",
      "680 1.0\n",
      "681 1.0\n",
      "682 1.0\n",
      "683 1.0\n",
      "684 1.0\n",
      "685 1.0\n",
      "686 1.0\n",
      "687 1.0\n",
      "688 1.0\n",
      "689 1.0\n",
      "690 1.0\n",
      "691 1.0\n",
      "692 1.0\n",
      "693 1.0\n",
      "694 0.1986301369863013\n",
      "695 1.0\n",
      "696 1.0\n",
      "697 1.0\n",
      "698 0.9198606271777005\n",
      "699 0.8255326580509954\n",
      "700 1.0\n",
      "701 0.8809950422509547\n",
      "702 1.0\n",
      "703 0.29722619302588515\n",
      "704 1.0\n",
      "705 1.0\n",
      "706 1.0\n",
      "707 1.0\n",
      "708 0.9183673469387755\n",
      "709 1.0\n",
      "710 0.6625447241227794\n",
      "711 1.0\n",
      "712 1.0\n",
      "713 0.7139056831922612\n",
      "714 0.952455524931095\n",
      "715 1.0\n",
      "716 0.8260645730675884\n",
      "717 0.961453744493392\n",
      "718 1.0\n",
      "719 1.0\n",
      "720 1.0\n",
      "721 1.0\n",
      "722 1.0\n",
      "723 1.0\n",
      "724 1.0\n",
      "725 1.0\n",
      "726 0.0\n",
      "727 0.8843724109362054\n",
      "728 1.0\n",
      "729 0.05409153952843278\n",
      "730 0.7071438616228378\n",
      "731 1.0\n",
      "732 0.8257575757575758\n",
      "733 1.0\n",
      "734 1.0\n",
      "735 1.0\n",
      "736 1.0\n",
      "737 0.8750081290238668\n",
      "738 1.0\n",
      "739 1.0\n",
      "740 0.8803274897081271\n",
      "741 1.0\n",
      "742 1.0\n",
      "743 1.0\n",
      "744 1.0\n",
      "745 1.0\n",
      "746 1.0\n",
      "747 1.0\n",
      "748 0.9346276537679361\n",
      "749 1.0\n",
      "750 0.8361396303901437\n",
      "751 1.0\n",
      "752 0.8747229826619737\n",
      "753 0.7620091896407686\n",
      "754 1.0\n",
      "755 0.0\n",
      "756 1.0\n",
      "757 1.0\n",
      "758 0.4988950101116401\n",
      "759 1.0\n",
      "760 1.0\n",
      "761 1.0\n",
      "762 0.5228447134421701\n",
      "763 0.13223721191680718\n",
      "764 1.0\n",
      "765 1.0\n",
      "766 1.0\n",
      "767 0.5782648128582248\n",
      "768 0.0\n",
      "769 1.0\n",
      "770 1.0\n",
      "771 0.8583174451858914\n",
      "772 1.0\n",
      "773 1.0\n",
      "774 1.0\n",
      "775 0.4953998584571833\n",
      "776 1.0\n",
      "777 1.0\n",
      "778 1.0\n",
      "779 1.0\n",
      "780 0.8876662081613939\n",
      "781 1.0\n",
      "782 1.0\n",
      "783 1.0\n",
      "784 1.0\n",
      "785 1.0\n",
      "786 0.5274357929066449\n",
      "787 1.0\n",
      "788 1.0\n",
      "789 0.8581547750533834\n",
      "790 1.0\n",
      "791 0.6675843083275981\n",
      "792 0.6809965548435196\n",
      "793 -0.05588444233956898\n",
      "794 0.8420917521472655\n",
      "795 0.8038967611336033\n",
      "796 1.0\n",
      "797 1.0\n",
      "798 1.0\n",
      "799 1.0\n",
      "800 1.0\n",
      "801 0.7422752197754373\n",
      "802 0.705912930474334\n",
      "803 1.0\n",
      "804 0.0\n",
      "805 1.0\n",
      "806 0.0707964601769912\n",
      "807 1.0\n",
      "808 1.0\n",
      "809 1.0\n",
      "810 0.9333990511983241\n",
      "811 0.7761741234555831\n",
      "812 0.9167412177198093\n",
      "813 1.0\n",
      "814 1.0\n",
      "815 1.0\n",
      "816 1.0\n",
      "817 1.0\n",
      "818 0.5479452054794519\n",
      "819 1.0\n",
      "820 1.0\n",
      "821 0.722083481983589\n",
      "822 1.0\n",
      "823 1.0\n",
      "824 1.0\n",
      "825 0.7714772387428973\n",
      "826 1.0\n",
      "827 0.1461768913119754\n",
      "828 0.7415452295848226\n",
      "829 1.0\n",
      "830 1.0\n",
      "831 1.0\n",
      "832 1.0\n",
      "833 1.0\n",
      "834 1.0\n",
      "835 1.0\n",
      "836 1.0\n",
      "837 0.5372812579254375\n",
      "838 0.5750000000000001\n",
      "839 0.0\n",
      "840 0.9035449299258038\n",
      "841 0.8735307245945545\n",
      "842 1.0\n",
      "843 1.0\n",
      "844 1.0\n",
      "845 1.0\n",
      "846 0.9148783928131248\n",
      "847 1.0\n",
      "848 1.0\n",
      "849 1.0\n",
      "850 1.0\n",
      "851 0.5070422535211268\n",
      "852 1.0\n",
      "853 1.0\n",
      "854 1.0\n",
      "855 0.0\n",
      "856 0.9305293331871648\n",
      "857 1.0\n",
      "858 1.0\n",
      "859 1.0\n",
      "860 0.7138873396542108\n",
      "861 0.0\n",
      "862 1.0\n",
      "863 1.0\n",
      "864 1.0\n",
      "865 1.0\n",
      "866 1.0\n",
      "867 1.0\n",
      "868 0.9619046910014071\n",
      "869 1.0\n",
      "870 1.0\n",
      "871 1.0\n",
      "872 1.0\n",
      "873 1.0\n",
      "874 1.0\n",
      "875 0.0\n",
      "876 0.5873857108180273\n",
      "877 1.0\n",
      "878 1.0\n",
      "879 1.0\n",
      "880 1.0\n",
      "881 1.0\n",
      "882 1.0\n",
      "883 0.4888795067728493\n",
      "884 0.8517110266159695\n",
      "885 1.0\n",
      "886 0.9180978880587487\n",
      "887 1.0\n",
      "888 0.6620689655172413\n",
      "889 1.0\n",
      "890 0.8709052457222296\n",
      "891 1.0\n",
      "892 1.0\n",
      "893 0.09302325581395351\n",
      "894 1.0\n",
      "895 1.0\n",
      "896 1.0\n",
      "897 1.0\n",
      "898 0.6730961129829423\n",
      "899 1.0\n",
      "900 1.0\n",
      "901 1.0\n",
      "902 0.34814453705575304\n",
      "903 0.21725304083230426\n",
      "904 0.6496380275918591\n",
      "905 1.0\n",
      "906 1.0\n",
      "907 1.0\n",
      "908 1.0\n",
      "909 1.0\n",
      "910 1.0\n",
      "911 1.0\n",
      "912 0.6782693957592842\n",
      "913 1.0\n",
      "914 1.0\n",
      "915 1.0\n",
      "916 1.0\n",
      "917 1.0\n",
      "918 1.0\n",
      "919 0.8470394736842105\n",
      "920 1.0\n",
      "921 1.0\n",
      "922 0.44692737430167595\n",
      "923 1.0\n",
      "924 1.0\n",
      "925 1.0\n",
      "926 0.9267238190131848\n",
      "927 0.9214705480388137\n",
      "928 0.9518984915438143\n",
      "929 0.5700907502415867\n",
      "930 0.04467960023515581\n",
      "931 1.0\n",
      "932 0.7615384615384615\n",
      "933 0.7368421052631579\n",
      "934 1.0\n",
      "935 0.7456807187284036\n",
      "936 0.0\n",
      "937 1.0\n",
      "938 0.0\n",
      "939 1.0\n",
      "940 1.0\n",
      "941 1.0\n",
      "942 0.41228511168689\n",
      "943 1.0\n",
      "944 0.9356154664774743\n",
      "945 1.0\n",
      "946 0.0\n",
      "947 0.6458946505081227\n",
      "948 1.0\n",
      "949 1.0\n",
      "950 1.0\n",
      "951 1.0\n",
      "952 1.0\n",
      "953 1.0\n",
      "954 1.0\n",
      "955 1.0\n",
      "956 1.0\n",
      "957 0.706130237735549\n",
      "958 1.0\n",
      "959 1.0\n",
      "960 0.8701526820699905\n",
      "961 0.11764705882352938\n",
      "962 0.9578697416983231\n",
      "963 1.0\n",
      "964 0.43301955104996415\n",
      "965 1.0\n",
      "966 0.7245516977233729\n",
      "967 1.0\n",
      "968 1.0\n",
      "969 0.6986477784932389\n",
      "970 1.0\n",
      "971 1.0\n",
      "972 0.9020661650543413\n",
      "973 0.0\n",
      "974 1.0\n",
      "975 1.0\n",
      "976 1.0\n",
      "977 1.0\n",
      "978 0.9092575618698442\n",
      "979 1.0\n",
      "980 1.0\n",
      "981 1.0\n",
      "982 1.0\n",
      "983 1.0\n",
      "984 0.1818181818181818\n",
      "985 0.9069738938595416\n",
      "986 1.0\n",
      "987 1.0\n",
      "988 0.9410938871014917\n",
      "989 1.0\n",
      "990 0.8749922220148093\n",
      "991 0.2322834645669292\n",
      "992 0.8749106504646175\n",
      "993 1.0\n",
      "994 0.0\n",
      "995 0.9221522003918868\n",
      "996 0.6688780950124393\n",
      "997 0.0\n",
      "998 1.0\n",
      "999 0.8498646184450461\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.eval()\n",
    "for i, (inputs, labels) in enumerate(dataloaders['train']):\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    a_arr, d_arr = np.nonzero(np.max(reverse_transform(inputs[0].cpu()), axis = 2))\n",
    "    \n",
    "    p = torch.nn.functional.softmax(outputs[0], dim=1)\n",
    "    p_arr = p.cpu().data.numpy()\n",
    "    \n",
    "    pred_labels = np.argmax(p_arr, axis = 0)[a_arr, d_arr]\n",
    "    target_labels = np.argmax(labels[0].cpu().data.numpy(), axis = 0)[a_arr, d_arr]\n",
    "    \n",
    "    ari = metrics.adjusted_rand_score(pred_labels, target_labels)\n",
    "      \n",
    "    plot_preds_and_targets(d_arr, a_arr, pred_labels, target_labels, ari, i)\n",
    "\n",
    "    \n",
    "    print(i, ari)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders['train'].dataset.target_masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = dataloaders['train'].dataset.target_masks[999,:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 3\n",
    "a_arr, d_arr = np.nonzero(np.max(reverse_transform(inputs[idx].cpu()), axis = 2))\n",
    "# t_arr = (p_arr[idx, :, a_arr, d_arr][:,0]<0.5).astype(int)\n",
    "\n",
    "t_arr = np.argmax(target, axis = 0)[a_arr, d_arr]\n",
    "\n",
    "colour_list = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'tab:brown', 'tab:pink', 'tab:gray', 'tab:olive', 'tab:cyan']\n",
    "\n",
    "\n",
    "fig, (ax1) = plt.subplots(nrows=1, ncols=1, figsize=(10,8), sharex=True)\n",
    "for t in t_arr:\n",
    "    mask = (t_arr == t)\n",
    "    ax1.scatter(d_arr[mask], a_arr[mask], s=10, c=colour_list[t%10], marker='x')\n",
    "    \n",
    "for ax1 in fig.axes:\n",
    "    matplotlib.pyplot.sca(ax1)\n",
    "    plt.xticks(rotation=90)\n",
    "    #plt.legend(bbox_to_anchor=(1.02, 1), loc=2, borderaxespad=0.)\n",
    "    \n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = torch.nn.functional.softmax(outputs, dim=1)\n",
    "p_arr = p.cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 3\n",
    "a_arr, d_arr = np.nonzero(np.max(reverse_transform(inputs[idx].cpu()), axis = 2))\n",
    "# t_arr = (p_arr[idx, :, a_arr, d_arr][:,0]<0.5).astype(int)\n",
    "\n",
    "t_arr = np.argmax(p_arr[0,:,:,:], axis = 0)[a_arr, d_arr]\n",
    "\n",
    "colour_list = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'tab:brown', 'tab:pink', 'tab:gray', 'tab:olive', 'tab:cyan']\n",
    "\n",
    "\n",
    "fig, (ax1) = plt.subplots(nrows=1, ncols=1, figsize=(10,8), sharex=True)\n",
    "for t in t_arr:\n",
    "    mask = (t_arr == t)\n",
    "    ax1.scatter(d_arr[mask], a_arr[mask], s=10, c=colour_list[t%10], marker='x')\n",
    "    \n",
    "for ax1 in fig.axes:\n",
    "    matplotlib.pyplot.sca(ax1)\n",
    "    plt.xticks(rotation=90)\n",
    "    #plt.legend(bbox_to_anchor=(1.02, 1), loc=2, borderaxespad=0.)\n",
    "    \n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_arr[0,:,:,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(p_arr[0,2,a_arr, d_arr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(p_arr[0,:,:,:], axis = 0)[a_arr, d_arr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
