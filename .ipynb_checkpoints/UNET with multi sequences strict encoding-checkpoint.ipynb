{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Unet CNN Pixel classification model for grouping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version of the UNET model uses the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import timeit\n",
    "import statistics\n",
    "import time\n",
    "import torch\n",
    "import torch_geometric\n",
    "import importlib\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "\n",
    "from data_utils import synthetic_data\n",
    "from data_utils import graph_constructors\n",
    "from data_utils import group_to_image_constructors\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import importlib\n",
    "\n",
    "from Unet import helper\n",
    "from Unet import simulation\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from torchvision import transforms, datasets, models\n",
    "import torchvision.utils\n",
    "\n",
    "from Unet import loss as Unet_loss\n",
    "\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'data_utils.group_to_image_constructors' from '/home/a/Documents/GNN-for-trans-grouping/GNN-for-trans-grouping/data_utils/group_to_image_constructors.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(synthetic_data)\n",
    "importlib.reload(graph_constructors)\n",
    "importlib.reload(group_to_image_constructors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "colour_list = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'tab:brown', 'tab:pink', 'tab:gray', 'tab:olive', 'tab:cyan']\n",
    "\n",
    "for i in range(2):\n",
    "    d_lst, a_lst, g_lst, t_lst = synthetic_data.make_a_group()\n",
    "    \n",
    "    d_arr = np.array(d_lst)\n",
    "    a_arr = np.array(a_lst)\n",
    "    g_arr = np.array(g_lst)    \n",
    "    \n",
    "    fig, (ax1) = plt.subplots(nrows=1, ncols=1, figsize=(10,8), sharex=True)\n",
    "    for g in g_lst:\n",
    "        mask = (g_arr == g)\n",
    "        \n",
    "        ax1.scatter(d_arr[mask], a_arr[mask], s=10, c=colour_list[g%10], marker='x')\n",
    "        ax1.set_title(str(i))\n",
    "        #ax1.legend(loc=\"upper right\")\n",
    "    \n",
    "    for ax1 in fig.axes:\n",
    "        matplotlib.pyplot.sca(ax1)\n",
    "        plt.xticks(rotation=90)\n",
    "        #plt.legend(bbox_to_anchor=(1.02, 1), loc=2, borderaxespad=0.)\n",
    "    \n",
    "    plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_lst, a_lst, g_lst, t_lst = synthetic_data.make_a_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182\n",
      "196\n",
      "168\n",
      "177\n",
      "128\n",
      "127\n",
      "52\n",
      "117\n",
      "167\n",
      "56\n",
      "81\n",
      "22\n",
      "196\n",
      "195\n",
      "97\n",
      "188\n",
      "108\n",
      "209\n",
      "126\n",
      "119\n",
      "112\n",
      "56\n",
      "161\n",
      "182\n",
      "147\n",
      "147\n",
      "189\n",
      "119\n",
      "189\n",
      "105\n",
      "147\n",
      "98\n",
      "189\n",
      "28\n",
      "80\n",
      "119\n",
      "84\n",
      "172\n",
      "168\n",
      "45\n",
      "182\n",
      "105\n",
      "116\n",
      "49\n",
      "78\n",
      "199\n",
      "189\n",
      "42\n",
      "79\n",
      "143\n",
      "78\n",
      "196\n",
      "126\n",
      "199\n",
      "91\n",
      "76\n",
      "81\n",
      "49\n",
      "72\n",
      "161\n",
      "85\n",
      "108\n",
      "159\n",
      "102\n",
      "94\n",
      "160\n",
      "70\n",
      "112\n",
      "98\n",
      "182\n",
      "107\n",
      "119\n",
      "182\n",
      "77\n",
      "72\n",
      "77\n",
      "107\n",
      "135\n",
      "178\n",
      "182\n",
      "166\n",
      "147\n",
      "41\n",
      "35\n",
      "182\n",
      "133\n",
      "140\n",
      "165\n",
      "173\n",
      "43\n",
      "154\n",
      "73\n",
      "34\n",
      "133\n",
      "93\n",
      "67\n",
      "133\n",
      "133\n",
      "176\n",
      "95\n"
     ]
    }
   ],
   "source": [
    "for _ in range(100):\n",
    "    d_lst, a_lst, g_lst, t_lst = synthetic_data.make_a_group()\n",
    "    print(max(d_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_arr = np.array(d_lst)\n",
    "a_arr = np.array(a_lst)\n",
    "g_arr = np.array(g_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_a_arr = graph_constructors.normalise_amounts(a_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unet demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 192, 192, 3)\n",
      "0 255\n",
      "(3, 6, 192, 192)\n",
      "0.0 1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdsAAAKvCAYAAAAiIWV+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3df4zddZ3v8dfrtqvJsmwAOXB7W2YLpOKK2R3lpCshEBB/FEKsLFHbbLCrxIEEknXXm4iSLGRvzDWuSK5xFx1iQ9logd0uSrxdlct1RQ0sTLXWIhRaLDJt0w7gFa4YvC3v+8d8Z/fLcGbmzPl+P/P9cZ6P5OSc8znf7/m+vzP9zqufz/mez9cRIQAAkM5/qroAAADajrAFACAxwhYAgMQIWwAAEiNsAQBIjLAFACCxZGFre53tPbb32r4+1XYAAKg7p/iere1lkp6Q9C5Jk5IekbQxIn5W+sYAAKi5VD3btZL2RsRTEfFbSXdKWp9oWwAA1NryRO+7UtIzueeTkv5kroVtM40VhtmzEdGpuoiynHzyybF69eqqywAqsWPHjp7Hc6qwdY+2VwWq7TFJY4m2DzTJ01UXUFT+eB4ZGdHExETFFQHVsN3zeE41jDwp6bTc81WSDuYXiIjxiOhGRDdRDQCWSP547nRa00kHSpMqbB+RtMb26bZfJ2mDpHsTbQsAgFpLMowcEUdtXyfp25KWSdocEY+m2BYAAHWX6jNbRcR2SdtTvT8AAE3BDFIAACRG2AIAkBhhCwBAYoQtAACJEbYAACRG2AIAkBhhCwBAYoQtAACJEbYAACRG2AIAkBhhCwBAYoQtAACJEbYAACRG2AIAkBhhCwBAYoQt+hIRVZcAoCT+8mVVlzB0Bg5b26fZ/q7tx2w/avsvsvabbB+wvTO7XVpeuagSgQu0B4G7tIr0bI9K+nhE/KGkt0u61vabs9duiYjR7La9cJWoDQIXaA8Cd+kMHLYRcSgifpQ9flHSY5JWllUY6ovABdqDwF0apXxma3u1pLdK+res6Trbu2xvtn1iGdtAvRC4QHsQuOkVDlvbvydpm6SPRcQLkm6VdKakUUmHJN08x3pjtidsTxStAdUgcDEjfzxPTU1VXQ4GQOCmVShsbf+OpoP2qxHxz5IUEYcj4lhEvCLpNklre60bEeMR0Y2IbpEaUC0CF9Krj+dOp1N1ORgQgZtOkbORLekrkh6LiM/n2lfkFrtc0u7By0MTELhAexC4aRTp2Z4n6UpJ75j1NZ/P2v6p7V2SLpL0l2UUinojcIH2IHDLt3zQFSPiB5Lc4yW+6jOkIkLTAx4Ams5fvkxx9TerLqM1mEEKpaKHC7QHPdzyELYoHYELtAeBWw7CFkkQuEB7ELjFEbZIhsAF2oPALYawRVIELtAeBO7gCFskR+AC7UHgDoawxZIgcIH2IHAXj7DFkiFwgfYgcBdn4EktUK2ZCST6va+LutUDVO3cMz+96HUe3HdDgkoWj4kv+kfYNtRMYPV7X9b2ADQfAbn0GEZuqJkh2X7vAQDVIWwbaql7tgCAwRG2DUXPFgCag7BtKHq2ANAchG1D0bMFgOYgbBuKni0ANEfhr/7Y3i/pRUnHJB2NiK7tkyTdJWm1pP2SPhARvyy6LfyHpn7PFgCGUVk924siYjQiutnz6yXdHxFrJN2fPUeJ6NkCQHOkGkZeL2lL9niLpPcl2s7Q4jNbAGiOMsI2JH3H9g7bY1nbqRFxSJKy+1NK2A5y6NkCQHOUMV3jeRFx0PYpku6z/Xg/K2XBPLbgguiJz2xRJ/njeWRkpOJqgPpxmcOMtm+S9H8lfVTShRFxyPYKSf8aEWfNsx5jnRhmO3LnOzRet9uNiYmJqssAKmG75/FcaBjZ9nG2j595LOndknZLulfSpmyxTZK+UWQ7AAA0WdFh5FMl3ZMNUy6X9LWI+JbtRyTdbfsqSb+Q9P6C2wEAoLEKhW1EPCXpj3u0Pyfp4iLvDQBAWzCDFAAAiRG2AAAkRtgCAJBYGd+zxSLN93UrvhMLNMuLO+fusxw/+soSVoI6o2e7xBb6XjPTKwLNMV/Q9vM6hgf/EpZQv0FK4AL112+QEriQCFsAAJIjbAEASIywBQAgMcIWAIDECFsAABIjbAEASIywXUL9TljBxBZA/fU7YQUTW0AibJfcQkFK0ALNsVCQErSYwXSNFSBQgfYgUNEPerYAACQ2cM/W9lmS7so1nSHpryWdIOmjkqay9k9FxPaBKwQAoOEGDtuI2CNpVJJsL5N0QNI9kj4s6ZaI+FwpFQIA0HBlDSNfLGlfRDxd0vsBANAaZYXtBklbc8+vs73L9mbbJ5a0DQAAGqlw2Np+naT3SvrHrOlWSWdqeoj5kKSb51hvzPaE7YmiNQCoVv54npqaWngFYMiU0bO9RNKPIuKwJEXE4Yg4FhGvSLpN0tpeK0XEeER0I6JbQg19iwiuFwuULH88dzqdJdvulu+fqy3fP3fJtgcMqoyw3ajcELLtFbnXLpe0u4RtAADQWIUmtbD9u5LeJenqXPNnbY9KCkn7Z70GAMDQKRS2EfGSpDfMaruyUEUAALQMM0gBAJAYYQsAQGKELQAAibXyqj/9fLVnvmW4Kg9QH/18tWe+ZTad/2CZ5QADoWcLAEBirezZztcznenR0nsFmmG+nulMj5beK+qOni0AAIkRtgAAJEbYAgCQGGELAEBihC0AAIkRtgAAJNbKr/7Mh6/8AO3BV37QFPRsAQBIjLAFACAxwhYAgMT6Clvbm20fsb0713aS7ftsP5ndn5i12/YXbO+1vcv221IVDwBAE/Tbs71d0rpZbddLuj8i1ki6P3suSZdIWpPdxiTdWrxMAACaq6+wjYgHJD0/q3m9pC3Z4y2S3pdrvyOmPSTpBNsryigWAIAmKvKZ7akRcUiSsvtTsvaVkp7JLTeZtQEAMJRSnCDV64usr7lSu+0x2xO2JxLUAGAJ5Y/nqampqssBaqdI2B6eGR7O7o9k7ZOSTsstt0rSwdkrR8R4RHQjolugBgA1kD+eO51O1eUAtVMkbO+VtCl7vEnSN3LtH8rOSn67pF/NDDcDADCM+pqu0fZWSRdKOtn2pKQbJX1G0t22r5L0C0nvzxbfLulSSXslvSTpwyXXDABAo/QVthGxcY6XLu6xbEi6tkhRAAC0CTNIAQCQGGELAEBihC0AAIkRtgAAJEbYAgCQGGELAEBihC0AAIkRtgAAJEbYAgCQGGELAEBihC0AAIkRtgAAJEbYAgCQGGELAEBihC0AAIkRtgAAJLZg2NrebPuI7d25tr+1/bjtXbbvsX1C1r7a9m9s78xuX0pZPAAATdBPz/Z2Setmtd0n6S0R8UeSnpD0ydxr+yJiNLtdU06ZAAA014JhGxEPSHp+Vtt3IuJo9vQhSasS1AYAQCuU8ZntRyT9S+756bZ/bPt7ts8v4f0BAGi05UVWtn2DpKOSvpo1HZI0EhHP2T5H0tdtnx0RL/RYd0zSWJHtA6iH/PE8MjJScTVA/Qzcs7W9SdJlkv4sIkKSIuLliHgue7xD0j5Jb+y1fkSMR0Q3IrqD1gCgHvLHc6fTqbocoHYGClvb6yR9QtJ7I+KlXHvH9rLs8RmS1kh6qoxCAQBoqgWHkW1vlXShpJNtT0q6UdNnH79e0n22Jemh7MzjCyT9je2jko5JuiYinu/5xgAADIkFwzYiNvZo/socy26TtK1oUQAAtAkzSAEAkBhhCwBAYoQtAACJEbYAACRG2AIAkBhhCwBAYoQtAACJEbYAACRG2AIAkBhhCwBAYoQtAACJEbYAACRW6OLxbZJdknde2RWOANTcw+ddtOAya3/43SWoBJhGzxYAgMQIWwAAEiNsAQBIbMGwtb3Z9hHbu3NtN9k+YHtndrs099onbe+1vcf2e1IVDgBAU/RzgtTtkr4o6Y5Z7bdExOfyDbbfLGmDpLMl/RdJ/8v2GyPiWAm1ok/9nOw1g5O+gHob23ag72XHr1iZsBIUsWDYRsQDtlf3+X7rJd0ZES9L+rntvZLWSnpw4ArRt8WE7Ox1CF2gXhYTsrPXIXTrp8hnttfZ3pUNM5+Yta2U9ExumcmsDYkNErRlrg+gPIMEbZnro3yDhu2tks6UNCrpkKSbs/Ze3aOef8Vtj9mesD0xYA3QdEiWFZRlvheGS/54npqaqrqcxhrbdqC0oCzzvVDcQJNaRMThmce2b5P0zezppKTTcouuknRwjvcYlzSevUflf+GbOIy6UDDOt0/zrRsRjfx5oDr547nb7VZ+PDdxwoqFgnG+oeH51h3bdoBh5RoYKGxtr4iIQ9nTyyXNnKl8r6Sv2f68pk+QWiPp4cJVYlH6CcqZZejJAvXWT1DOLENPtr76+erPVk2f4HSW7UnbV0n6rO2f2t4l6SJJfylJEfGopLsl/UzStyRdy5nIacwVkovtkc61PCEMLJ25QnKxPdK5lieEq+c6/FGtwzByk5QVtKnfE33bERHdqosoS7fbjYkJTsXoV1lBm/o90R/bPY9nZpBqiaKhSKgC9VE0FAnV+iFsG6ZXD7SsoOz1PnUY+QDaqlcPtKyg7PU+DCdXh7AFACAxwrbhyh7+ZTgZqE7Zw78MJ9cHYQsAQGKELQAAiRG2AAAkRtgCAJAYYQsAQGKELQAAiRG2DVf2pBNMYgFUp+xJJ5jEoj4IWwAAEiNsGybllIopp4IE8Fopp1RMORUkFo+wbYmigcvwMVAfRQOX4eP6IWwbqOxr0HJ5PaA6ZV+Dlsvr1RNh21BlBS5BC1SvrMAlaOtr+UIL2N4s6TJJRyLiLVnbXZLOyhY5QdL/iYhR26slPSZpT/baQxFxTdlFY34zATpfYDJsDDTDTIDOF5gMG9ffgmEr6XZJX5R0x0xDRHxw5rHtmyX9Krf8vogYLatAzM32vKE5aKDSqwWW3vgVK+cNzUEDlV5tPSwYthHxQNZjfQ1P/1X+gKR3lFsW+jUTjGX0VAlZoFozwVhGT5WQrZein9meL+lwRDyZazvd9o9tf8/2+QXfH30qGpQELVAfRYOSoK2ffoaR57NR0tbc80OSRiLiOdvnSPq67bMj4oXZK9oekzRWcPvIGaSXS8iiDPnjeWRkpOJq2mGQXi4hW1/u5w9zNoz8zZkTpLK25ZIOSDonIibnWO9fJf3XiJhY4P05WwfDbEdEdKsuoizdbjcmJuY95IHWst3zeC4yjPxOSY/ng9Z2x/ay7PEZktZIeqrANgAAaLwFw9b2VkkPSjrL9qTtq7KXNujVQ8iSdIGkXbZ/IumfJF0TEc+XWTAAAE3Tz9nIG+do//MebdskbSteFgAA7cEMUgAAJEbYAgCQGGELAEBihC0AAIkRtgAAJEbYAgCQGGELAEBihC0AAIkRtgAAJEbYAgCQGGELAEBifV1iL3kR9pSkX0t6tupaSnCy2I86acJ+/EFEdKouoiy2X5S0p+o6StCEfzv9YD+WVs/juRZhK0m2J9pwTU/2o17ash9N0pafOftRL03fD4aRAQBIjLAFACCxOoXteNUFlIT9qJe27EeTtOVnzn7US6P3ozaf2QIA0FZ16tkCANBKhC0AAIkRtgAAJEbYAgCQGGELAEBihC0AAIkRtgAAJEbYAgCQGGELAEBihC0AAIkRtgAAJEbYAgCQGGELAEBihC0AAIkRtgAAJEbYAgCQGGELAEBihC0AAIkRtgAAJEbYAgCQGGELAEBihC0AAIkRtgAAJEbYAgCQGGELAEBihC0AAIkRtgAAJEbYAgCQGGELAEBiycLW9jrbe2zvtX19qu0AAFB3jojy39ReJukJSe+SNCnpEUkbI+JnpW8MAICaS9WzXStpb0Q8FRG/lXSnpPWJtgUAQK2lCtuVkp7JPZ/M2gAAGDrLE72ve7S9arza9piksezpOYnqAJrg2YjoVF1EEfnj+bjjjjvnTW96U8UVAdXYsWNHz+M5VdhOSjot93yVpIP5BSJiXNK4JNku/4NjoDmerrqAovLHc7fbjYmJiYorAqphu+fxnGoY+RFJa2yfbvt1kjZIujfRtgAAqLUkPduIOGr7OknflrRM0uaIeDTFtgAAqLtUw8iKiO2Stqd6fwAAmoIZpAAASIywBQAgMcIWAIDECFsAABIjbAEASIywLSAilOJCDgCW3pbvn6st3z+36jLQUoQtAACJEbYAACRG2AIAkBhhCwBAYoQtAACJEbYAACRG2AIAkBhhCwBAYskusdd0i5msop9lbRcpB0ABi5msop9lN53/YJFyMITo2QIAkNjAPVvbp0m6Q9J/lvSKpPGI+B+2b5L0UUlT2aKfyi4k3yj99ERnerT0WoF666cnOtOjpdeKFIoMIx+V9PGI+JHt4yXtsH1f9totEfG54uUBANB8A4dtRBySdCh7/KLtxyStLKswAADaopTPbG2vlvRWSf+WNV1ne5ftzbZPLGMbAAA0VeGwtf17krZJ+lhEvCDpVklnShrVdM/35jnWG7M9YXuiaA0AqpU/nqemphZeARgyhcLW9u9oOmi/GhH/LEkRcTgijkXEK5Juk7S217oRMR4R3YjoFqkBQPXyx3On06m6HKB2Bg5bT5+C+xVJj0XE53PtK3KLXS5p9+DlAQDQfEXORj5P0pWSfmp7Z9b2KUkbbY9KCkn7JV1dqEIAABquyNnIP5DU6wumjftOLQAAKTFdYwFMZgG0B5NZICWmawQAIDHCFgCAxAhbAAASI2wBAEiME6RQS1wjGGiPF3cu3K87fvSVJaikOvRsUTv9BO1ilgNQnX6CdjHLNRU9W9TGIOHJNYWBehokPGfWaWMvt93/lQAAoAYIW9RC0SFhhpSB+ig6JNzGIeX27REap6ygJHCB6pUVlG0L3HbtDQAANUTYAgCQGGELAEBihC0AAIkRtgAAJFZ4Ugvb+yW9KOmYpKMR0bV9kqS7JK2WtF/SByLil0W3BQBAE5XVs70oIkYjops9v17S/RGxRtL92XMAAIZSqmHk9ZK2ZI+3SHpfou0AAFB7ZYRtSPqO7R22x7K2UyPikCRl96eUsB20VFnzGjM/MlC9suY1btv8yGVciOC8iDho+xRJ99l+vJ+VsmAeW3BBDAXbhWaAImirlT+eR0ZGKq4GVTt+9JVCM0C1LWilEnq2EXEwuz8i6R5JayUdtr1CkrL7Iz3WG4+Ibu5zXgANlT+eO51O1eUAtVMobG0fZ/v4mceS3i1pt6R7JW3KFtsk6RtFtoPhYHvRPdRB1gGQ3vGjryy6hzrIOk1RdBj5VEn3ZH/slkv6WkR8y/Yjku62fZWkX0h6f8HtYIj0O6RMyAL11++QcltDdkahsI2IpyT9cY/25yRdXOS9MdwIUqA92h6k/WAGKQAAEiNsAQBIjLAFACCxMr5nO5Q4gQdoj4fPu2jBZdb+8LtLUMlrnXvmpwu/x4P7biihEhRBz7alZv4zMOg9AKA8hG1LzfSqB70HAJSHsG0perYAUB+EbUvRswWA+iBsW4qeLQDUB2HbUvRsAaA+CNuWomcLAPVB2LYUPVsAqA8mtRhQ3UMpIv796jmD3APDpKoJKzA86Nm2FD1bAKgPwral+MwWAOqDsG0perYAUB8Df2Zr+yxJd+WazpD015JOkPRRSVNZ+6ciYvvAFWIgfGYLAPUxcNhGxB5Jo5Jke5mkA5LukfRhSbdExOdKqRADoWcLAPVR1jDyxZL2RcTTJb0fCuIzWwCoj7LCdoOkrbnn19neZXuz7RNL2gYWgZ4tANSHi/ZkbL9O0kFJZ0fEYdunSnpWUkj6b5JWRMRHeqw3Jmkse3pOoSKAZtsREd2qiygifzyPjIyc8/TTDHJhONnueTyX0bO9RNKPIuKwJEXE4Yg4FhGvSLpN0tpeK0XEeER0m/5HBsCrj+dOp1N1OUDtlBG2G5UbQra9Ivfa5ZJ2l7ANAAAaq9B0jbZ/V9K7JF2da/6s7VFNDyPvn/UaAABDp1DYRsRLkt4wq+3KQhUBANAyzCAFAEBihC0AAIkRtgAAJEbYAgCQGGELAEBihC0AAIkRtgAAJEbYAgCQGGELAEBihC0AAIkRtgAAJEbYAgCQGGELAEBihC0AAIkRtgAAJEbYAgCQWF9ha3uz7SO2d+faTrJ9n+0ns/sTs3bb/oLtvbZ32X5bquIBAGiCfnu2t0taN6vtekn3R8QaSfdnzyXpEklrstuYpFuLlwkAQHP1FbYR8YCk52c1r5e0JXu8RdL7cu13xLSHJJ1ge0UZxQIA0ERFPrM9NSIOSVJ2f0rWvlLSM7nlJrM2AACGUooTpNyjLV6zkD1me8L2RIIaACyh/PE8NTVVdTlA7RQJ28Mzw8PZ/ZGsfVLSabnlVkk6OHvliBiPiG5EdAvUAKAG8sdzp9OpuhygdoqE7b2SNmWPN0n6Rq79Q9lZyW+X9KuZ4WYAAIbR8n4Wsr1V0oWSTrY9KelGSZ+RdLftqyT9QtL7s8W3S7pU0l5JL0n6cMk1AwDQKH2FbURsnOOli3ssG5KuLVIUAABtwgxSAAAkRtgCAJAYYQsAQGKELQAAiRG2AAAkRtgCAJAYYQsAQGKELQAAiRG2AAAkRtgCAJAYYQsAQGKELQAAiRG2AAAkRtgCAJAYYQsAQGKELQAAiS0YtrY32z5ie3eu7W9tP257l+17bJ+Qta+2/RvbO7Pbl1IWDwBAE/TTs71d0rpZbfdJektE/JGkJyR9MvfavogYzW7XlFMmAADNtWDYRsQDkp6f1fadiDiaPX1I0qoEtQEA0AplfGb7EUn/knt+uu0f2/6e7fNLeH8AABpteZGVbd8g6aikr2ZNhySNRMRzts+R9HXbZ0fECz3WHZM0VmT7AOohfzyPjIxUXA1QPwP3bG1vknSZpD+LiJCkiHg5Ip7LHu+QtE/SG3utHxHjEdGNiO6gNQCoh/zx3Ol0qi4HqJ2Bwtb2OkmfkPTeiHgp196xvSx7fIakNZKeKqNQAACaasFhZNtbJV0o6WTbk5Ju1PTZx6+XdJ9tSXooO/P4Akl/Y/uopGOSromI53u+MQAAQ2LBsI2IjT2avzLHstskbStaFAAAbcIMUgAAJEbYAgCQGGELAEBihC0AAIkVmtRiWGRfI55XdlY2aiwi+D1BY9sOLLjM+BUrl6ASFOEvX6a4+ptVl9E3wnYe/YTs7GX5Y15vBO7w6idkZy9L6NZbkwKXYeQ5LCZoy1gPS4ff0fBZTNCWsR6Wjr98WdUl9IWebQ9z/THu1SPqtSy9p/rjdzQ85grMXr3WXsuObTtAD7fmmtDDpWc7S6/wtD3nH+a5XqP3VH/8jtqvV3iOX7FyzvCc6zV6uPVX9x4uYZszV9D2g8BtJn5H7TVX0PaDwG2mOgcuYTuPxQ4zMizZTATucFjsUDBDx81U18AlbDOz/+AOGpyz1+MPeTPwe2qX2b3QQYNz9nr0bpuhjoFL2AIZAhdoj7oFLmHbQ9HhYIaTm4vAbZ+iw8EMJzdXnQKXsAVmIXCB9qhL4BK2QA8ELtAedQjcBcPW9mbbR2zvzrXdZPuA7Z3Z7dLca5+0vdf2HtvvSVU4kBqBC7RH1YHbT8/2dknrerTfEhGj2W27JNl+s6QNks7O1vl728vKKhZYagQu0B5VBu6C0zVGxAO2V/f5fusl3RkRL0v6ue29ktZKenDgCitQdCo//kDXEyeuDaei0y3ydZ96qvv0jLMV+cz2Otu7smHmE7O2lZKeyS0zmbUBADC0Bg3bWyWdKWlU0iFJN2ftvboOPbt5tsdsT9ieGLCGUpU1GUVZk2MATZI/nqempqoup7TJKMqaHAMYKGwj4nBEHIuIVyTdpumhYmm6J3tabtFVkg7O8R7jEdGNiO4gNSyFxQYuw8cYVvnjudPpVF1OT4sNXIaPUaaBwtb2itzTyyXNnKl8r6QNtl9v+3RJayQ9XKzEpVPkYgJFLmIAoHxFLiZQ5CIGQC8LniBle6ukCyWdbHtS0o2SLrQ9qukh4v2SrpakiHjU9t2SfibpqKRrI+JYmtLTsP2a4Jx5vpgwJmiB6o1fsfI1wTnzfDFhTNCiKNdh6NN29UXMUuTnQtBikXbU+eOUxep2uzExUYtTMf5dkSFhghaLYbvn8cwMUnMo66o/AKpX1lV/gEEtOIw8zGaCs59eLiEL1NtMcPbTyyVkUTbCtg8EKdAeBCmqwDAyAACJEbYAACRG2AIAkBhhCwBAYoQtAACJEbYAACRG2AIAkBhhCwBAYoQtAACJEbYAACRG2AIAkBhhCwBAYoQtAACJLRi2tjfbPmJ7d67tLts7s9t+2zuz9tW2f5N77UspiwcAoAn6ucTe7ZK+KOmOmYaI+ODMY9s3S/pVbvl9ETFaVoEAADTdgmEbEQ/YXt3rNU9f6PUDkt5RblkAALRH0c9sz5d0OCKezLWdbvvHtr9n+/yC7w8AQOP1M4w8n42StuaeH5I0EhHP2T5H0tdtnx0RL8xe0faYpLGC2wdQA/njeWRkpOJqgPoZuGdre7mkP5V010xbRLwcEc9lj3dI2ifpjb3Wj4jxiOhGRHfQGgDUQ/547nQ6VZcD1E6RYeR3Sno8IiZnGmx3bC/LHp8haY2kp4qVCABAs/Xz1Z+tkh6UdJbtSdtXZS9t0KuHkCXpAkm7bP9E0j9JuiYini+zYAAAmqafs5E3ztH+5z3atknaVrwsAADagxmkAABIjLAFACAxwhYAgMQIWwAAEiNsAQBIjLAFACAxwhYAgMQIWwAAEiNsAQBIjLAFACAxwhYAgMQIWwAAEnNEVF2DbE9J+rWkZ6uupQQni/2okybsxx9ERGsuAmv7RUl7qq6jBE34t9MP9mNp9TyeaxG2kmR7og0Xkmc/6qUt+9EkbfmZsx/10vT9YBgZAIDECFsAABKrU9iOV11ASdiPemnLfjRJW37m7Ee9NHo/avOZLQAAbVWnni0AAK1E2AIAkBhhCwBAYoQtAACJEbYAACRG2AIAkBhhCwBAYoQtAACJEbYAACRG2AIAkBhhCwBAYoQtAACJEbYAACRG2AIAkBhhCwBAYoQtAACJEbYAACRG2AIAkBhhCwBAYoQtAACJEbYAACRG2AIAkBhhCwBAYoQtAACJEbYAACRG2AIAkBhhCwBAYoQtAACJEbYAACSWLGxtry+yNQ8AABHVSURBVLO9x/Ze29en2g4AAHXniCj/Te1lkp6Q9C5Jk5IekbQxIn5W+sYAAKi5VD3btZL2RsRTEfFbSXdKWp9oWwAA1NryRO+7UtIzueeTkv4kv4DtMUlj2dNzEtUBNMGzEdGpuogi8sfzcccdd86b3vSmiisCqrFjx46ex3OqsHWPtleNV0fEuKRxSbJd/lg20BxPV11AUfnjudvtxsTERMUVAdWw3fN4TjWMPCnptNzzVZIOJtoWAAC1lipsH5G0xvbptl8naYOkexNtCwCAWksyjBwRR21fJ+nbkpZJ2hwRj6bYFgAAdZfqM1tFxHZJ21O9PwAATcEMUgAAJEbYAgCQGGELAEBihC0AAIkRtgAAJEbYAgCQGGELAEBihC0AAIkRtgAAJEbYAgCQGGELAEBihC0AAIkRtgAAJEbYAgCQGGELAEBihC0AAIkNHLa2T7P9XduP2X7U9l9k7TfZPmB7Z3a7tLxy0TYRUXUJAEriL19WdQm1VaRne1TSxyPiDyW9XdK1tt+cvXZLRIxmt+2Fq0SrEbhAexC4vQ0cthFxKCJ+lD1+UdJjklaWVRiGC4ELtAeB+1rLy3gT26slvVXSv0k6T9J1tj8kaULTvd9f9lhnTNJYGdtHO0SEbC+4zEIWeg+UL388j4yMVFwN6sBfvkxx9TfnXebFnQv3944ffaWskipV+AQp278naZukj0XEC5JulXSmpFFJhyTd3Gu9iBiPiG5EdIvWgPaYL0z77f3SS156+eO50+lUXQ5qYr4ebj9Bu5jl6q7QXtj+HU0H7Vcj4p8lKSIOR8SxiHhF0m2S1hYvE8OkV1guNkAJXKAeegXuYgO0DYFb5GxkS/qKpMci4vO59hW5xS6XtHvw8jCs8mE5aHASuEA95AN30OBseuAW+cz2PElXSvqp7Z1Z26ckbbQ9Kikk7Zd0daEKMbTKCMt+PgcGkJ6/fJle+JNiX055ced/auxnuAOHbUT8QFKvv2J81QcAhsi5Z35akrTqvFXzLveXU+/U2359TJJ05Qe/m7yuOml2vxyNxRAvMJx+dNyyqkuoBGGLSjC0CwyvYQxcwhaVoGcLDLf/Prmm6hKWFGGLStCzBTBMgUvYohL0bAFIwxO4hC0qQc8WwIxhCFzCFpWgZwsgr+2BS9iiEkvVs6UHDdTHt584Y97XFwrcpk5oIZV01R9gsfIzO9n+9+fz3S8WQQssrVU/PG3BZR794Wm68gP/W1euenJR793koJXo2aIi+aBdzP1i3x9A/fzD3e9Y1PJND1qJsEVFZnqqi7nvN0AJWqD++g3QNgStxDAyKjJoz5YgBdqjLUHaD8IWAFDIg/tuqLqE2mMYGQCAxAhbAAASI2wBAEis8Ge2tvdLelHSMUlHI6Jr+yRJd0laLWm/pA9ExC+LbgsAgCYq6wSpiyLi2dzz6yXdHxGfsX199vwTJW1rKCxmEgfO0AXqbWzbgb6XHb9iZcJKUJVUZyOvl3Rh9niLpH8VYduXQWZKmlmH0AXqZTEhO3sdQrddyvjMNiR9x/YO22NZ26kRcUiSsvtTZq9ke8z2hO2JEmpohaKT8zO5P6qSP56npqaqLqcWBgnaMtdHvZQRtudFxNskXSLpWtsX9LNSRIxHRDciuiXU0HhlBSWBiyrkj+dOp1N1OZUrKygJ3PYoPIwcEQez+yO275G0VtJh2ysi4pDtFZKOFN1Om80XkPMNDc+13mKmNgRQrvkCcr6h4bnWG9t2gCHlFijUs7V9nO3jZx5Lerek3ZLulbQpW2yTpG8U2U6bzRWYthcMzPmWWYoebkTQkwZy5grM8StWLhiY8y2zFD3cLd8/V1u+f27y7QyrosPIp0r6ge2fSHpY0v+MiG9J+oykd9l+UtK7sueYZb6gXYwqAxfAtPmCdjGqDFykU2gYOSKekvTHPdqfk3RxkfceVoMO/w56zVcA6Qw6/Dt+xUrCtWWYQaoivYKx6OesvdYngIH0egVj0c9Ze61PADcXYQsAQGKEbU2UdfYwZyED1Svr7GHOQm4PwhYAgMQIWwAAEiNsAQBIjLAFACAxwhYAgMRSXWIPDVXkEn/94GxpYOkMMv3iYtbZdP6Di37/YUXPtia46g/QHlz1B7PRs8WrLKbnyUXrgXpbTM9zpkdLbzUNerYVSTG1YoopIAEsLMXUiimmgER1CNuaGTRwGT4G6mfQwGX4uH0I2wqVdWm8si7VB2BwZV0ar6xL9aFeCNuKzRe4C4XufMsQtMDSmy9wFwrd+ZYhaJuPE6RqYL5r0Q4yPEzQAtWZ71q0gwwPE7TtMHDP1vZZtnfmbi/Y/pjtm2wfyLVfWmbBbcVVf4D24Ko/mG3gsI2IPRExGhGjks6R9JKke7KXb5l5LSK2l1HoMEhx8XgA1Uhx8Xg0V1nDyBdL2hcRT/MHv5iZnx+zMgHNNxOYixk+JmTbqayw3SBpa+75dbY/JGlC0scj4pezV7A9JmmspO23DgGKJskfzyMjIxVXUz8EKAqfjWz7dZLeK+kfs6ZbJZ0paVTSIUk391ovIsYjohsR3aI1AKhW/njudDpVlwPUThk920sk/SgiDkvSzL0k2b5N0jdL2AZqiN430B5M05hWGd+z3ajcELLtFbnXLpe0u4RtAADQWIV6trZ/V9K7JF2da/6s7VFJIWn/rNcAABg6hcI2Il6S9IZZbVcWqggAgJZhukYAABIjbAEASIywBQAgMcIWAIDECFsAABIjbAEASIywBQAgMcIWAIDECFsAABIjbAEASIywBQAgMcIWAIDECFsAABIjbAEASIywBQAgMcIWAIDE+gpb25ttH7G9O9d2ku37bD+Z3Z+Ytdv2F2zvtb3L9ttSFQ8AQBP027O9XdK6WW3XS7o/ItZIuj97LkmXSFqT3cYk3Vq8TAAAmquvsI2IByQ9P6t5vaQt2eMtkt6Xa78jpj0k6QTbK8ooFgCAJiryme2pEXFIkrL7U7L2lZKeyS03mbW9iu0x2xO2JwrUAKAG8sfz1NRU1eUAtZPiBCn3aIvXNESMR0Q3IroJagCwhPLHc6fTqbocoHaKhO3hmeHh7P5I1j4p6bTccqskHSywHQAAGq1I2N4raVP2eJOkb+TaP5Sdlfx2Sb+aGW4GAGAYLe9nIdtbJV0o6WTbk5JulPQZSXfbvkrSLyS9P1t8u6RLJe2V9JKkD5dcMwAAjdJX2EbExjleurjHsiHp2iJFAQDQJswgBQBAYoQtAACJEbYAACRG2AIAkBhhCwBAYoQtAACJEbYAACRG2AIAkBhhCwBAYoQtAACJEbYAACRG2AIAkBhhCwBAYoQtAACJEbYAACRG2AIAkNiCYWt7s+0jtnfn2v7W9uO2d9m+x/YJWftq27+xvTO7fSll8QAANEE/PdvbJa2b1XafpLdExB9JekLSJ3Ov7YuI0ex2TTllAgDQXAuGbUQ8IOn5WW3fiYij2dOHJK1KUBsAAK1Qxme2H5H0L7nnp9v+se3v2T5/rpVsj9mesD1RQg0AKpQ/nqempqouB6idQmFr+wZJRyV9NWs6JGkkIt4q6a8kfc327/daNyLGI6IbEd0iNQCoXv547nQ6VZcD1M7AYWt7k6TLJP1ZRIQkRcTLEfFc9niHpH2S3lhGoQAANNVAYWt7naRPSHpvRLyUa+/YXpY9PkPSGklPlVEoAABNtXyhBWxvlXShpJNtT0q6UdNnH79e0n22Jemh7MzjCyT9je2jko5JuiYinu/5xgAADIkFwzYiNvZo/socy26TtK1oUQAAtAkzSAEAkBhhCwBAYgsOIwPDLDvRfl7ZeQsAau7h8y5acJm1P/xukm3TswUAIDHCFgCAxAhbAAASI2wBAEiMsAUAIDHCFgCAxAhbAAASI2wBAEiMSS2AeTBhBdAeqSas6Ac9WwAAEiNsAQBIjLAFACAxwhYAgMQWDFvbm20fsb0713aT7QO2d2a3S3OvfdL2Xtt7bL8nVeEAADRFPz3b2yWt69F+S0SMZrftkmT7zZI2SDo7W+fvbS8rq1gAAJpowbCNiAckPd/n+62XdGdEvBwRP5e0V9LaAvUBANB4RT6zvc72rmyY+cSsbaWkZ3LLTGZtr2F7zPaE7YkCNQCogfzxPDU1VXU5QO0MGra3SjpT0qikQ5Juztp7zQAQvd4gIsYjohsR3QFrAFAT+eO50+lUXQ5QOwOFbUQcjohjEfGKpNv0H0PFk5JOyy26StLBYiUCANBsA4Wt7RW5p5dLmjlT+V5JG2y/3vbpktZIerhYiQAANNuCcyPb3irpQkkn256UdKOkC22PanqIeL+kqyUpIh61fbekn0k6KunaiDiWpnQAAJphwbCNiI09mr8yz/KflvTpIkUBANAmzCAFAEBihC0AAIkRtgAAJEbYAgCQGGELAEBihC0AAIkRtgAAJEbYAgCQGGELAEBihC0AAIkRtgAAJEbYAgCQGGELAEBihC0AAIkRtgAAJEbYAgCQ2IJha3uz7SO2d+fa7rK9M7vtt70za19t+ze5176UsngAAJpgeR/L3C7pi5LumGmIiA/OPLZ9s6Rf5ZbfFxGjZRUIAEDTLRi2EfGA7dW9XrNtSR+Q9I5yywIAoD2KfmZ7vqTDEfFkru102z+2/T3b58+1ou0x2xO2JwrWAKBi+eN5amqq6nKA2ikathslbc09PyRpJCLeKumvJH3N9u/3WjEixiOiGxHdgjUAqFj+eO50OlWXA9TOwGFre7mkP5V010xbRLwcEc9lj3dI2ifpjUWLBACgyYr0bN8p6fGImJxpsN2xvSx7fIakNZKeKlYiAADN1s9Xf7ZKelDSWbYnbV+VvbRBrx5ClqQLJO2y/RNJ/yTpmoh4vsyCAQBomn7ORt44R/uf92jbJmlb8bIAAGgPZpACACAxwhYAgMQIWwAAEiNsAQBIjLAFACAxwhYAgMQIWwAAEiNsAQBIjLAFACAxwhYAgMQIWwAAEnNEVF2DbE9J+rWkZ6uupQQni/2okybsxx9ERGsuAmv7RUl7qq6jBE34t9MP9mNp9TyeaxG2kmR7og0Xkmc/6qUt+9EkbfmZsx/10vT9YBgZAIDECFsAABKrU9iOV11ASdiPemnLfjRJW37m7Ee9NHo/avOZLQAAbVWnni0AAK1UedjaXmd7j+29tq+vup7FsL3f9k9t77Q9kbWdZPs+209m9ydWXedstjfbPmJ7d66tZ92e9oXs97PL9tuqq/zV5tiPm2wfyH4nO21fmnvtk9l+7LH9nmqqbjeO56XH8dyM47nSsLW9TNLfSbpE0pslbbT95iprGsBFETGaOyX9ekn3R8QaSfdnz+vmdknrZrXNVfclktZktzFJty5Rjf24Xa/dD0m6JfudjEbEdknK/l1tkHR2ts7fZ//+UBKO58rcLo7n2h/PVfds10raGxFPRcRvJd0paX3FNRW1XtKW7PEWSe+rsJaeIuIBSc/Pap6r7vWS7ohpD0k6wfaKpal0fnPsx1zWS7ozIl6OiJ9L2qvpf38oD8dzBTiem3E8Vx22KyU9k3s+mbU1RUj6ju0dtseytlMj4pAkZfenVFbd4sxVdxN/R9dlQ2Sbc8N+TdyPpmn6z5jjuZ5acTxXHbbu0dak06PPi4i3aXpo5lrbF1RdUAJN+x3dKulMSaOSDkm6OWtv2n40UdN/xhzP9dOa47nqsJ2UdFru+SpJByuqZdEi4mB2f0TSPZoexjg8MyyT3R+prsJFmavuRv2OIuJwRByLiFck3ab/GFpq1H40VKN/xhzP9dOm47nqsH1E0hrbp9t+naY/8L634pr6Yvs428fPPJb0bkm7NV3/pmyxTZK+UU2FizZX3fdK+lB2FuPbJf1qZniqjmZ9/nS5pn8n0vR+bLD9etuna/oEkYeXur6W43iuD47nuomISm+SLpX0hKR9km6oup5F1H2GpJ9kt0dnapf0Bk2f/fdkdn9S1bX2qH2rpodk/p+m/4d41Vx1a3q45u+y389PJXWrrn+B/fiHrM5dmj4gV+SWvyHbjz2SLqm6/jbeOJ4rqZ3juQHHMzNIAQCQWNXDyAAAtB5hCwBAYoQtAACJEbYAACRG2AIAkBhhCwBAYoQtAACJEbYAACT2/wE4Afdnj6mRcgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 576x864 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate some random images\n",
    "input_images, target_masks = simulation.generate_random_data(192, 192, count=3)\n",
    "\n",
    "for x in [input_images, target_masks]:\n",
    "    print(x.shape)\n",
    "    print(x.min(), x.max())\n",
    "\n",
    "# Change channel-order and make 3 channels for matplot\n",
    "input_images_rgb = [x.astype(np.uint8) for x in input_images]\n",
    "\n",
    "# Map each channel (i.e. class) to each color\n",
    "target_masks_rgb = [helper.masks_to_colorimg(x) for x in target_masks]\n",
    "\n",
    "# Left: Input image (black and white), Right: Target mask (6ch)\n",
    "helper.plot_side_by_side([input_images_rgb, target_masks_rgb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimDataset(Dataset):\n",
    "    def __init__(self, count, transform=None):\n",
    "        self.input_images, self.target_masks = simulation.generate_random_data(192, 192, count=count)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.input_images[idx]\n",
    "        mask = self.target_masks[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return [image, mask]\n",
    "\n",
    "# use the same transformations for train/val in this example\n",
    "trans = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # imagenet\n",
    "])\n",
    "\n",
    "train_set = SimDataset(2000, transform = trans)\n",
    "val_set = SimDataset(200, transform = trans)\n",
    "\n",
    "image_datasets = {\n",
    "    'train': train_set, 'val': val_set\n",
    "}\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "dataloaders_orig = {\n",
    "    'train': DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0),\n",
    "    'val': DataLoader(val_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders_orig['train'].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders_orig['train'].dataset.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders_orig['train'].dataset.input_images.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders_orig['val'].dataset.input_images.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders_orig['val'].dataset.input_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders_orig['val'].dataset.target_masks.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### my dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_img, out_target_perm = group_to_image_constructors.make_an_image_from_group(*np.array(synthetic_data.make_a_group()),permute_group_ids = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimDataset(Dataset):\n",
    "    def __init__(self, count, transform=None):\n",
    "        self.sim_data = [group_to_image_constructors.make_an_image_from_group(*np.array(synthetic_data.make_a_group()),permute_group_ids = False) for _ in range(count)]\n",
    "        self.input_images = np.array([x[0] for  x in self.sim_data]).astype('uint8')\n",
    "        self.target_masks = np.array([x[1] for  x in self.sim_data]).astype('float32')\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.input_images[idx]\n",
    "        mask = self.target_masks[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return [image, mask]\n",
    "\n",
    "# use the same transformations for train/val in this example\n",
    "trans = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # imagenet\n",
    "])\n",
    "\n",
    "train_set = SimDataset(2000, transform = trans)\n",
    "val_set = SimDataset(200, transform = trans)\n",
    "\n",
    "image_datasets = {\n",
    "    'train': train_set, 'val': val_set\n",
    "}\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "#it is at this next step that the numpy arrays are converted to tensors\n",
    "dataloaders = {\n",
    "    'train': DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0),\n",
    "    'val': DataLoader(val_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 4, 224, 224)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloaders['val'].dataset.target_masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 224, 224, 3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloaders['val'].dataset.input_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 224, 224]) torch.Size([4, 4, 224, 224])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f4340117bd0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD8CAYAAAB3lxGOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAOx0lEQVR4nO3df6zddX3H8edLFJKpCaBCSKlrIdUMzVKRIIlK3A8VyGJhia5kmY0jqyaQaOKSVU02sv/mRBOjw9RILIsD3RRpjE6bxuj+GErRWsAKFKxyadNOXIBNoxbe++N87zzeH97be86Xc04/z0dycr7nc77nfN83p/fVz/d72s87VYWkdj1n0gVImixDQGqcISA1zhCQGmcISI0zBKTG9RYCSa5I8kCSQ0l29HUcSaNJH/9OIMlpwIPAG4E54G7g2qr6/tgPJmkkfc0ELgUOVdUjVfVL4HZgS0/HkjSC5/b0vuuAR4cezwGvWW7nJP6zRal/P6mqlywc7CsEssTYb/yiJ9kObO/p+JIW+9FSg32FwBywfujx+cCR4R2qaiewE5wJSJPU1zWBu4FNSTYmOR3YCuzu6ViSRtDLTKCqTiS5AfgqcBpwS1Xd38exJI2ml68IT7oITwekZ8M9VXXJwkH/xaDUOENAapwhIDXOEJAaZwhIjTMEpMYZAlLjDAGpcYaA1DhDQGqcISA1zhCQGmcISI0zBKTGGQJS49YcAknWJ/l6koNJ7k/y7m78xiSPJdnf3a4aX7mSxm2UlYVOAO+tqu8keSFwT5I93XMfqaoPjV6epL6tOQSq6ihwtNt+KslBBkuNS5ohY7kmkGQD8CrgW93QDUkOJLklyVnjOIakfowcAkleAHweeE9VPQncDFwIbGYwU7hpmddtT7Ivyb5Ra5C0diMtNJrkecCXgK9W1YeXeH4D8KWqeuUK7+NCo1L/xrvQaJIAnwIODgdAkvOGdrsGuG+tx5DUv1G+HXgt8BfAvUn2d2PvB65NsplB27HDwDtHqlBSr+w7ILXDvgOSFjMEpMYZAlLjDAGpcYaA1DhDQGqcISA1zhCQGmcISI0zBKTGGQJS4wwBqXGGgNQ4Q0BqnCEgNc4QkBo3yspCACQ5DDwFPA2cqKpLkpwNfBbYwGB1obdV1X+PeixJ4zeumcAfVNXmoVVLdgB7q2oTsLd7LGkK9XU6sAXY1W3vAq7u6TiSRjSOECjga0nuSbK9Gzu361A036nonIUvsu+ANB1GviYAvLaqjiQ5B9iT5AereVFV7QR2gguNSpM08kygqo5098eBO4BLgWPz/Qe6++OjHkdSP0YKgSTP7zoSk+T5wJsYNBvZDWzrdtsG3DnKcST1Z9TTgXOBOwbNiHgu8C9V9e9J7gY+l+Q64MfAW0c8jqSe2HxEaofNRyQtZghIjTMEpMYZAlLjDAGpcYaA1DhDQGqcISA1zhCQGmcISI0zBKTGGQJS4wwBqXGGgNQ4Q0Bq3JoXFUnycga9BeZdAPwtcCbwV8B/dePvr6ovr7lCSb0ay6IiSU4DHgNeA7wD+J+q+tBJvN5FRaT+9bqoyB8BD1fVj8b0fpKeJeMKga3AbUOPb0hyIMktSc4a0zEk9WDkEEhyOvAW4F+7oZuBC4HNwFHgpmVeZ/MRaQqMfE0gyRbg+qp60xLPbQC+VFWvXOE9vCYg9a+3awLXMnQqMN90pHMNgz4EkqbUSH0HkvwO8EbgnUPDH0yymUGPwsMLnpM0Zew7ILXDvgOSFjMEpMYZAlLjDAGpcYaA1DhDQGqcISA1zhCQGmcISI0zBKTGGQJS4wwBqXGGgNQ4Q0BqnCEgNW5VIdAtGHo8yX1DY2cn2ZPkoe7+rG48ST6a5FC32OjFfRUvaXSrnQl8GrhiwdgOYG9VbQL2do8BrgQ2dbftDBYelTSlVhUCVfVN4KcLhrcAu7rtXcDVQ+O31sBdwJkL1h2UNEVGuSZwblUdBejuz+nG1wGPDu03141JmkIjLTS6jCwxtmgNwSTbGZwuSJqgUWYCx+an+d398W58Dlg/tN/5wJGFL66qnVV1yVILH0p69owSAruBbd32NuDOofG3d98SXAY8MX/aIGkKVdWKNwbNRY4Cv2LwN/11wIsYfCvwUHd/drdvgI8DDwP3Apes4v3Lmzdvvd/2LfX7Z98BqR32HZC0mCEgNc4QkBpnCEiNMwSkxhkCUuMMAalxhoDUOENAapwhIDXOEJAaZwhIjTMEpMYZAlLjDAGpcYaA1LgVQ2CZxiP/mOQHXXORO5Kc2Y1vSPLzJPu72yf6LF7S6FYzE/g0ixuP7AFeWVW/DzwIvG/ouYeranN3e9d4ypTUlxVDYKnGI1X1tao60T28i8GKwpJm0DiuCfwl8JWhxxuTfDfJN5K8frkXJdmeZF+SfWOoQdIajdR8JMkHgBPAZ7qho8BLq+rxJK8GvpjkFVX15MLXVtVOYGf3Pi40Kk3ImmcCSbYBfwL8ec2vG171i6p6vNu+h8Gy4y8bR6GS+rGmEEhyBfA3wFuq6mdD4y9Jclq3fQGDzsSPjKNQSf1Y8XQgyW3AG4AXJ5kD/o7BtwFnAHuSANzVfRNwOfD3SU4ATwPvqqqF3YwlTRGbj0jtsPmIpMUMAalxhoDUOENAatzUhsAzU3DBUmrB1IbAcwZfPUrq2dSGgKRnxykXAp5GSCfnlAsBTyOkk3PKhYCkkzOTIeCUXxqfmQyBeYaBNLqZDIH5837P/6XRzWQISBqfmQgBp/1Sf9bad+DGJI8N9Re4aui59yU5lOSBJG8eS5FO+6XerLXvAMBHhvoLfBkgyUXAVuAV3Wv+aX65MUnTaU19B36LLcDt3YKjPwQOAZeOUJ+kno1yTeCGrg3ZLUnO6sbWAY8O7TPXjS1i3wFpOqw1BG4GLgQ2M+g1cFM3vtTJ+5JX9apqZ1VdstSaZ5KePWsKgao6VlVPV9UzwCf59ZR/Dlg/tOv5wJHRSpTUp7X2HThv6OE1wPw3B7uBrUnOSLKRQd+Bb49WoqQ+rbXvwBuSbGYw1T8MvBOgqu5P8jng+wzak11fVU/3U7qkcbDvgNQO+w5IWswQkBpnCEiNMwSkxhkCUuMMgVXyvzPrVGUIrJL/nVmnKkNAapwhMAaeKmiWGQJj4KmCZpkhIDXOEJAaZwhIjTMExsyLhJo1p3QITOIX0ouEmjVr7Tvw2aGeA4eT7O/GNyT5+dBzn+iz+JX4CymtbMWVhRj0HfgYcOv8QFX92fx2kpuAJ4b2f7iqNo+rwGn2TJVBo5m3YghU1TeTbFjquSQB3gb84XjLGq++flkNAJ0KRr0m8HrgWFU9NDS2Mcl3k3wjyetHfP+x8JdVWt5qTgd+m2uB24YeHwVeWlWPJ3k18MUkr6iqJxe+MMl2YPuIx5c0ojXPBJI8F/hT4LPzY137sce77XuAh4GXLfV6m49I02GU04E/Bn5QVXPzA0leMt+ANMkFDPoOPDJaiZL6tJqvCG8D/hN4eZK5JNd1T23lN08FAC4HDiT5HvBvwLuqarXNTCVNgH0HpHbYd0DSYoaA1DhDQGqcISA1zhCQGmcISI0zBKTGGQJS4wwBqXGGgNQ4Q0BqnCEgNc4QkBpnCEiNMwSkxq1mUZH1Sb6e5GCS+5O8uxs/O8meJA9192d140ny0SSHkhxIcnHfP4SktVvNTOAE8N6q+j3gMuD6JBcBO4C9VbUJ2Ns9BriSwbJimxgsJHrz2KuWNDYrhkBVHa2q73TbTwEHgXXAFmBXt9su4Opuewtwaw3cBZyZ5LyxVy5pLE7qmkDXhORVwLeAc6vqKAyCAjin220d8OjQy+a6MUlTaNV9B5K8APg88J6qejLLN/RY6olFawjad0CaDquaCSR5HoMA+ExVfaEbPjY/ze/uj3fjc8D6oZefDxxZ+J72HZCmw2q+HQjwKeBgVX146KndwLZuextw59D427tvCS4Dnpg/bZA0fVZccjzJ64D/AO4FnumG38/gusDngJcCPwbeWlU/7ULjY8AVwM+Ad1TVvhWO4ZLjUv+WXHLcvgNSO+w7IGkxQ0BqnCEgNc4QkBpnCEiNMwSkxhkCUuMMAalxhoDUOENAapwhIDXOEJAaZwhIjTMEpMYZAlLjDAGpcYaA1DhDQGrcqpcc79lPgP/t7mfVi5nt+mH2f4ZZrx/6/Rl+d6nBqVhjECDJvllefnzW64fZ/xlmvX6YzM/g6YDUOENAatw0hcDOSRcwolmvH2b/Z5j1+mECP8PUXBOQNBnTNBOQNAETD4EkVyR5IMmhJDsmXc9qJTmc5N4k+5Ps68bOTrInyUPd/VmTrnNYkluSHE9y39DYkjV3vSQ/2n0uB5JcPLnK/7/Wpeq/Mclj3eewP8lVQ8+9r6v/gSRvnkzVv5ZkfZKvJzmY5P4k7+7GJ/sZVNXEbsBpwMPABcDpwPeAiyZZ00nUfhh48YKxDwI7uu0dwD9Mus4F9V0OXAzct1LNwFXAVxi0mr8M+NaU1n8j8NdL7HtR9+fpDGBj9+fstAnXfx5wcbf9QuDBrs6JfgaTnglcChyqqkeq6pfA7cCWCdc0ii3Arm57F3D1BGtZpKq+Cfx0wfByNW8Bbq2Bu4Az51vRT8oy9S9nC3B7Vf2iqn4IHGLw521iqupoVX2n234KOAisY8KfwaRDYB3w6NDjuW5sFhTwtST3JNnejZ1bXRv27v6ciVW3esvVPEufzQ3ddPmWoVOwqa4/yQbgVQy6e0/0M5h0CGSJsVn5uuK1VXUxcCVwfZLLJ13QmM3KZ3MzcCGwGTgK3NSNT239SV4AfB54T1U9+dt2XWJs7D/DpENgDlg/9Ph84MiEajkpVXWkuz8O3MFgqnlsfrrW3R+fXIWrtlzNM/HZVNWxqnq6qp4BPsmvp/xTWX+S5zEIgM9U1Re64Yl+BpMOgbuBTUk2Jjkd2ArsnnBNK0ry/CQvnN8G3gTcx6D2bd1u24A7J1PhSVmu5t3A27sr1JcBT8xPWafJgnPkaxh8DjCof2uSM5JsBDYB33626xuWJMCngINV9eGhpyb7GUzyaunQFdAHGVy9/cCk61llzRcwuPL8PeD++bqBFwF7gYe6+7MnXeuCum9jMGX+FYO/Za5brmYGU9GPd5/LvcAlU1r/P3f1Heh+ac4b2v8DXf0PAFdOQf2vYzCdPwDs725XTfoz8F8MSo2b9OmApAkzBKTGGQJS4wwBqXGGgNQ4Q0BqnCEgNc4QkBr3fzKcUMIKgBHGAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def reverse_transform(inp):\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    inp = (inp * 255).astype(np.uint8)\n",
    "\n",
    "    return inp\n",
    "\n",
    "# Get a batch of training data\n",
    "inputs, masks = next(iter(dataloaders['train']))\n",
    "\n",
    "print(inputs.shape, masks.shape)\n",
    "\n",
    "plt.imshow(reverse_transform(inputs[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convrelu(in_channels, out_channels, kernel, padding):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel, padding=padding),\n",
    "        nn.ReLU(inplace=True),\n",
    "    )\n",
    "\n",
    "\n",
    "class ResNetUNet(nn.Module):\n",
    "    def __init__(self, n_class):\n",
    "        super().__init__()\n",
    "\n",
    "        self.base_model = models.resnet18(pretrained=True)\n",
    "        self.base_layers = list(self.base_model.children())\n",
    "\n",
    "        self.layer0 = nn.Sequential(*self.base_layers[:3]) # size=(N, 64, x.H/2, x.W/2)\n",
    "        self.layer0_1x1 = convrelu(64, 64, 1, 0)\n",
    "        self.layer1 = nn.Sequential(*self.base_layers[3:5]) # size=(N, 64, x.H/4, x.W/4)\n",
    "        self.layer1_1x1 = convrelu(64, 64, 1, 0)\n",
    "        self.layer2 = self.base_layers[5]  # size=(N, 128, x.H/8, x.W/8)\n",
    "        self.layer2_1x1 = convrelu(128, 128, 1, 0)\n",
    "        self.layer3 = self.base_layers[6]  # size=(N, 256, x.H/16, x.W/16)\n",
    "        self.layer3_1x1 = convrelu(256, 256, 1, 0)\n",
    "        self.layer4 = self.base_layers[7]  # size=(N, 512, x.H/32, x.W/32)\n",
    "        self.layer4_1x1 = convrelu(512, 512, 1, 0)\n",
    "\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "        self.conv_up3 = convrelu(256 + 512, 512, 3, 1)\n",
    "        self.conv_up2 = convrelu(128 + 512, 256, 3, 1)\n",
    "        self.conv_up1 = convrelu(64 + 256, 256, 3, 1)\n",
    "        self.conv_up0 = convrelu(64 + 256, 128, 3, 1)\n",
    "\n",
    "        self.conv_original_size0 = convrelu(3, 64, 3, 1)\n",
    "        self.conv_original_size1 = convrelu(64, 64, 3, 1)\n",
    "        self.conv_original_size2 = convrelu(64 + 128, 64, 3, 1)\n",
    "\n",
    "        self.conv_last = nn.Conv2d(64, n_class, 1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x_original = self.conv_original_size0(input)\n",
    "        x_original = self.conv_original_size1(x_original)\n",
    "\n",
    "        layer0 = self.layer0(input)\n",
    "        layer1 = self.layer1(layer0)\n",
    "        layer2 = self.layer2(layer1)\n",
    "        layer3 = self.layer3(layer2)\n",
    "        layer4 = self.layer4(layer3)\n",
    "\n",
    "        layer4 = self.layer4_1x1(layer4)\n",
    "        x = self.upsample(layer4)\n",
    "        layer3 = self.layer3_1x1(layer3)\n",
    "        x = torch.cat([x, layer3], dim=1)\n",
    "        x = self.conv_up3(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer2 = self.layer2_1x1(layer2)\n",
    "        x = torch.cat([x, layer2], dim=1)\n",
    "        x = self.conv_up2(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer1 = self.layer1_1x1(layer1)\n",
    "        x = torch.cat([x, layer1], dim=1)\n",
    "        x = self.conv_up1(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer0 = self.layer0_1x1(layer0)\n",
    "        x = torch.cat([x, layer0], dim=1)\n",
    "        x = self.conv_up0(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, x_original], dim=1)\n",
    "        x = self.conv_original_size2(x)\n",
    "\n",
    "        out = self.conv_last(x)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = ResNetUNet(n_class=2)\n",
    "# model = model.to(device)\n",
    "\n",
    "# check keras-like model summary using torchsummary\n",
    "# from torchsummary import summary\n",
    "# summary(model, input_size=(3, 224, 224))\n",
    "# summary(model, input_size=(3, 512, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calc_loss(pred, target, metrics, bce_weight=0.5):\n",
    "    \n",
    "#     #add a new dimension after batch and broadcast/copy the predictions over this dimension\n",
    "#     broadcast_preds = batch_pred.unsqueeze(1).repeat(1,24,1,1,1)\n",
    "    \n",
    "#     #create the list of permutations of 4 group labels\n",
    "#     perms = torch.tensor([x for x in itertools.permutations([0,1,2,3])]).type(torch.long)\n",
    "    \n",
    "#     #create the permuted targets\n",
    "#     perms_target = target[:,perms,:,:]\n",
    "    \n",
    "#     #get the BCE loss\n",
    "#     batch_mins, _ = torch.min(torch.mean(F.binary_cross_entropy_with_logits(broadcast_preds, perms_target, reduction = 'none'), dim = (2,3,4)), dim = 1)\n",
    "#     bce = torch.mean(batch_mins)\n",
    "    \n",
    "#     #get the dice loss\n",
    "#     broadcast_preds = F.sigmoid(broadcast_preds)    \n",
    "#     smooth = 1\n",
    "    \n",
    "#     intersection = (broadcast_preds*perms_target).sum(dim=3).sum(dim=3)\n",
    "#     loss = (1 - ((2. * intersection + smooth) / (broadcast_preds.sum(dim=3).sum(dim=3) + perms_target.sum(dim=3).sum(dim=3) + smooth)))\n",
    "#     batch_dice_mins, _ = torch.min(torch.mean(loss, dim = 2), dim = 1)\n",
    "#     dice = torch.mean(batch_dice_mins)\n",
    "    \n",
    "#     #combine losses\n",
    "#     loss = bce * bce_weight + dice * (1 - bce_weight)\n",
    "\n",
    "#     metrics['bce'] += bce.data.cpu().numpy() * target.size(0)\n",
    "#     metrics['dice'] += dice.data.cpu().numpy() * target.size(0)\n",
    "#     metrics['loss'] += loss.data.cpu().numpy() * target.size(0)\n",
    "\n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(pred, target, metrics, bce_weight=0.5):\n",
    "    bce = F.binary_cross_entropy_with_logits(pred, target)\n",
    "\n",
    "    pred = F.sigmoid(pred)\n",
    "    dice = Unet_loss.dice_loss(pred, target)\n",
    "\n",
    "    loss = bce * bce_weight + dice * (1 - bce_weight)\n",
    "\n",
    "    metrics['bce'] += bce.data.cpu().numpy() * target.size(0)\n",
    "    metrics['dice'] += dice.data.cpu().numpy() * target.size(0)\n",
    "    metrics['loss'] += loss.data.cpu().numpy() * target.size(0)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def print_metrics(metrics, epoch_samples, phase):\n",
    "    outputs = []\n",
    "    for k in metrics.keys():\n",
    "        outputs.append(\"{}: {:4f}\".format(k, metrics[k] / epoch_samples))\n",
    "\n",
    "    print(\"{}: {}\".format(phase, \", \".join(outputs)))\n",
    "\n",
    "def train_model(model, optimizer, scheduler, num_epochs=25):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 1e10\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        since = time.time()\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    print(\"LR\", param_group['lr'])\n",
    "\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            metrics = defaultdict(float)\n",
    "            epoch_samples = 0\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = calc_loss(outputs, labels, metrics)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                epoch_samples += inputs.size(0)\n",
    "\n",
    "            print_metrics(metrics, epoch_samples, phase)\n",
    "            epoch_loss = metrics['loss'] / epoch_samples\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                print(\"saving best model\")\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print('{:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    print('Best val loss: {:4f}'.format(best_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(Unet_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Epoch 0/599\n",
      "----------\n",
      "LR 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/a/anaconda3/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:100: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "/home/a/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: bce: 0.012486, dice: 0.400259, loss: 0.206373\n",
      "val: bce: 0.001687, dice: 0.368063, loss: 0.184875\n",
      "saving best model\n",
      "1m 16s\n",
      "Epoch 1/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.002756, dice: 0.299435, loss: 0.151096\n",
      "val: bce: 0.001602, dice: 0.251017, loss: 0.126310\n",
      "saving best model\n",
      "1m 16s\n",
      "Epoch 2/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.003362, dice: 0.259507, loss: 0.131435\n",
      "val: bce: 0.001527, dice: 0.247124, loss: 0.124325\n",
      "saving best model\n",
      "1m 16s\n",
      "Epoch 3/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.001370, dice: 0.190308, loss: 0.095839\n",
      "val: bce: 0.000664, dice: 0.108943, loss: 0.054803\n",
      "saving best model\n",
      "1m 17s\n",
      "Epoch 4/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000760, dice: 0.122056, loss: 0.061408\n",
      "val: bce: 0.000838, dice: 0.106621, loss: 0.053729\n",
      "saving best model\n",
      "1m 17s\n",
      "Epoch 5/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000932, dice: 0.114795, loss: 0.057864\n",
      "val: bce: 0.000723, dice: 0.102130, loss: 0.051426\n",
      "saving best model\n",
      "1m 17s\n",
      "Epoch 6/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000465, dice: 0.088004, loss: 0.044234\n",
      "val: bce: 0.000414, dice: 0.067208, loss: 0.033811\n",
      "saving best model\n",
      "1m 17s\n",
      "Epoch 7/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000464, dice: 0.073612, loss: 0.037038\n",
      "val: bce: 0.000392, dice: 0.065788, loss: 0.033090\n",
      "saving best model\n",
      "1m 17s\n",
      "Epoch 8/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000391, dice: 0.065536, loss: 0.032964\n",
      "val: bce: 0.000204, dice: 0.051288, loss: 0.025746\n",
      "saving best model\n",
      "1m 17s\n",
      "Epoch 9/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000401, dice: 0.065979, loss: 0.033190\n",
      "val: bce: 0.000327, dice: 0.058722, loss: 0.029524\n",
      "1m 17s\n",
      "Epoch 10/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000343, dice: 0.052441, loss: 0.026392\n",
      "val: bce: 0.000285, dice: 0.048172, loss: 0.024229\n",
      "saving best model\n",
      "1m 17s\n",
      "Epoch 11/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000322, dice: 0.052008, loss: 0.026165\n",
      "val: bce: 0.000284, dice: 0.045700, loss: 0.022992\n",
      "saving best model\n",
      "1m 17s\n",
      "Epoch 12/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000389, dice: 0.054502, loss: 0.027445\n",
      "val: bce: 0.000332, dice: 0.052434, loss: 0.026383\n",
      "1m 17s\n",
      "Epoch 13/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000359, dice: 0.052505, loss: 0.026432\n",
      "val: bce: 0.000223, dice: 0.043039, loss: 0.021631\n",
      "saving best model\n",
      "1m 17s\n",
      "Epoch 14/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000356, dice: 0.049026, loss: 0.024691\n",
      "val: bce: 0.000519, dice: 0.064919, loss: 0.032719\n",
      "1m 17s\n",
      "Epoch 15/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000364, dice: 0.048051, loss: 0.024208\n",
      "val: bce: 0.000457, dice: 0.054248, loss: 0.027352\n",
      "1m 17s\n",
      "Epoch 16/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000382, dice: 0.047899, loss: 0.024141\n",
      "val: bce: 0.000353, dice: 0.047221, loss: 0.023787\n",
      "1m 17s\n",
      "Epoch 17/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000329, dice: 0.044770, loss: 0.022550\n",
      "val: bce: 0.000484, dice: 0.051030, loss: 0.025757\n",
      "1m 17s\n",
      "Epoch 18/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000315, dice: 0.041003, loss: 0.020659\n",
      "val: bce: 0.000256, dice: 0.042979, loss: 0.021618\n",
      "saving best model\n",
      "1m 17s\n",
      "Epoch 19/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000350, dice: 0.043985, loss: 0.022168\n",
      "val: bce: 0.000329, dice: 0.038815, loss: 0.019572\n",
      "saving best model\n",
      "1m 17s\n",
      "Epoch 20/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000317, dice: 0.037639, loss: 0.018978\n",
      "val: bce: 0.000274, dice: 0.044919, loss: 0.022597\n",
      "1m 17s\n",
      "Epoch 21/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000317, dice: 0.039992, loss: 0.020154\n",
      "val: bce: 0.000351, dice: 0.051889, loss: 0.026120\n",
      "1m 17s\n",
      "Epoch 22/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000255, dice: 0.034945, loss: 0.017600\n",
      "val: bce: 0.000359, dice: 0.041517, loss: 0.020938\n",
      "1m 17s\n",
      "Epoch 23/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000344, dice: 0.041435, loss: 0.020889\n",
      "val: bce: 0.000336, dice: 0.045636, loss: 0.022986\n",
      "1m 17s\n",
      "Epoch 24/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000303, dice: 0.038071, loss: 0.019187\n",
      "val: bce: 0.000341, dice: 0.041453, loss: 0.020897\n",
      "1m 17s\n",
      "Epoch 25/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000272, dice: 0.035099, loss: 0.017685\n",
      "val: bce: 0.000453, dice: 0.043260, loss: 0.021857\n",
      "1m 17s\n",
      "Epoch 26/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000315, dice: 0.037261, loss: 0.018788\n",
      "val: bce: 0.000598, dice: 0.057657, loss: 0.029128\n",
      "1m 17s\n",
      "Epoch 27/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000305, dice: 0.036427, loss: 0.018366\n",
      "val: bce: 0.000287, dice: 0.034512, loss: 0.017399\n",
      "saving best model\n",
      "1m 17s\n",
      "Epoch 28/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000272, dice: 0.037561, loss: 0.018917\n",
      "val: bce: 0.000324, dice: 0.041185, loss: 0.020755\n",
      "1m 17s\n",
      "Epoch 29/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000235, dice: 0.030834, loss: 0.015534\n",
      "val: bce: 0.000275, dice: 0.035096, loss: 0.017686\n",
      "1m 17s\n",
      "Epoch 30/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000245, dice: 0.029697, loss: 0.014971\n",
      "val: bce: 0.000361, dice: 0.039698, loss: 0.020029\n",
      "1m 17s\n",
      "Epoch 31/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000221, dice: 0.028585, loss: 0.014403\n",
      "val: bce: 0.000428, dice: 0.036873, loss: 0.018650\n",
      "1m 17s\n",
      "Epoch 32/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000204, dice: 0.025233, loss: 0.012719\n",
      "val: bce: 0.000307, dice: 0.031703, loss: 0.016005\n",
      "saving best model\n",
      "1m 17s\n",
      "Epoch 33/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000188, dice: 0.023893, loss: 0.012041\n",
      "val: bce: 0.000289, dice: 0.035173, loss: 0.017731\n",
      "1m 17s\n",
      "Epoch 34/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000164, dice: 0.022533, loss: 0.011348\n",
      "val: bce: 0.000281, dice: 0.032101, loss: 0.016191\n",
      "1m 17s\n",
      "Epoch 35/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000190, dice: 0.027354, loss: 0.013772\n",
      "val: bce: 0.000353, dice: 0.046304, loss: 0.023328\n",
      "1m 17s\n",
      "Epoch 36/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000211, dice: 0.025481, loss: 0.012846\n",
      "val: bce: 0.000462, dice: 0.042826, loss: 0.021644\n",
      "1m 17s\n",
      "Epoch 37/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000239, dice: 0.028161, loss: 0.014200\n",
      "val: bce: 0.000587, dice: 0.052468, loss: 0.026527\n",
      "1m 17s\n",
      "Epoch 38/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000207, dice: 0.024602, loss: 0.012405\n",
      "val: bce: 0.000451, dice: 0.048364, loss: 0.024407\n",
      "1m 17s\n",
      "Epoch 39/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000183, dice: 0.022534, loss: 0.011358\n",
      "val: bce: 0.000391, dice: 0.042913, loss: 0.021652\n",
      "1m 17s\n",
      "Epoch 40/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000213, dice: 0.026589, loss: 0.013401\n",
      "val: bce: 0.000340, dice: 0.051306, loss: 0.025823\n",
      "1m 17s\n",
      "Epoch 41/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000183, dice: 0.023032, loss: 0.011607\n",
      "val: bce: 0.000247, dice: 0.028946, loss: 0.014597\n",
      "saving best model\n",
      "1m 17s\n",
      "Epoch 42/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000183, dice: 0.024024, loss: 0.012103\n",
      "val: bce: 0.000293, dice: 0.034041, loss: 0.017167\n",
      "1m 17s\n",
      "Epoch 43/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000276, dice: 0.028498, loss: 0.014387\n",
      "val: bce: 0.000440, dice: 0.044080, loss: 0.022260\n",
      "1m 17s\n",
      "Epoch 44/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000224, dice: 0.026275, loss: 0.013249\n",
      "val: bce: 0.000331, dice: 0.033727, loss: 0.017029\n",
      "1m 17s\n",
      "Epoch 45/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000223, dice: 0.023796, loss: 0.012009\n",
      "val: bce: 0.000333, dice: 0.037783, loss: 0.019058\n",
      "1m 17s\n",
      "Epoch 46/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000206, dice: 0.022937, loss: 0.011571\n",
      "val: bce: 0.000430, dice: 0.039668, loss: 0.020049\n",
      "1m 17s\n",
      "Epoch 47/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000194, dice: 0.021753, loss: 0.010974\n",
      "val: bce: 0.000249, dice: 0.028025, loss: 0.014137\n",
      "saving best model\n",
      "1m 17s\n",
      "Epoch 48/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000163, dice: 0.020448, loss: 0.010305\n",
      "val: bce: 0.000376, dice: 0.033634, loss: 0.017005\n",
      "1m 17s\n",
      "Epoch 49/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000162, dice: 0.019502, loss: 0.009832\n",
      "val: bce: 0.000312, dice: 0.031099, loss: 0.015705\n",
      "1m 17s\n",
      "Epoch 50/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000138, dice: 0.018797, loss: 0.009468\n",
      "val: bce: 0.000333, dice: 0.035781, loss: 0.018057\n",
      "1m 17s\n",
      "Epoch 51/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000164, dice: 0.019378, loss: 0.009771\n",
      "val: bce: 0.000252, dice: 0.025891, loss: 0.013071\n",
      "saving best model\n",
      "1m 17s\n",
      "Epoch 52/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000173, dice: 0.020122, loss: 0.010148\n",
      "val: bce: 0.000431, dice: 0.052127, loss: 0.026279\n",
      "1m 17s\n",
      "Epoch 53/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000158, dice: 0.018829, loss: 0.009493\n",
      "val: bce: 0.000499, dice: 0.040990, loss: 0.020744\n",
      "1m 17s\n",
      "Epoch 54/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000176, dice: 0.020023, loss: 0.010100\n",
      "val: bce: 0.000314, dice: 0.033884, loss: 0.017099\n",
      "1m 17s\n",
      "Epoch 55/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000124, dice: 0.015827, loss: 0.007975\n",
      "val: bce: 0.000276, dice: 0.029488, loss: 0.014882\n",
      "1m 17s\n",
      "Epoch 56/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000160, dice: 0.019994, loss: 0.010077\n",
      "val: bce: 0.000362, dice: 0.038483, loss: 0.019423\n",
      "1m 17s\n",
      "Epoch 57/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000164, dice: 0.020993, loss: 0.010579\n",
      "val: bce: 0.000364, dice: 0.031007, loss: 0.015686\n",
      "1m 17s\n",
      "Epoch 58/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000148, dice: 0.018762, loss: 0.009455\n",
      "val: bce: 0.000296, dice: 0.036283, loss: 0.018290\n",
      "1m 17s\n",
      "Epoch 59/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000127, dice: 0.016628, loss: 0.008378\n",
      "val: bce: 0.000287, dice: 0.035835, loss: 0.018061\n",
      "1m 17s\n",
      "Epoch 60/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000121, dice: 0.015249, loss: 0.007685\n",
      "val: bce: 0.000275, dice: 0.031495, loss: 0.015885\n",
      "1m 17s\n",
      "Epoch 61/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000123, dice: 0.016383, loss: 0.008253\n",
      "val: bce: 0.000245, dice: 0.032812, loss: 0.016528\n",
      "1m 17s\n",
      "Epoch 62/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000107, dice: 0.014657, loss: 0.007382\n",
      "val: bce: 0.000249, dice: 0.038687, loss: 0.019468\n",
      "1m 17s\n",
      "Epoch 63/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000122, dice: 0.014105, loss: 0.007113\n",
      "val: bce: 0.000283, dice: 0.038415, loss: 0.019349\n",
      "1m 17s\n",
      "Epoch 64/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000156, dice: 0.016990, loss: 0.008573\n",
      "val: bce: 0.000393, dice: 0.038041, loss: 0.019217\n",
      "1m 17s\n",
      "Epoch 65/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000136, dice: 0.015844, loss: 0.007990\n",
      "val: bce: 0.000254, dice: 0.028350, loss: 0.014302\n",
      "1m 17s\n",
      "Epoch 66/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000137, dice: 0.016485, loss: 0.008311\n",
      "val: bce: 0.000259, dice: 0.032202, loss: 0.016230\n",
      "1m 17s\n",
      "Epoch 67/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000183, dice: 0.023865, loss: 0.012024\n",
      "val: bce: 0.000295, dice: 0.036379, loss: 0.018337\n",
      "1m 17s\n",
      "Epoch 68/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000131, dice: 0.018831, loss: 0.009481\n",
      "val: bce: 0.000347, dice: 0.036458, loss: 0.018402\n",
      "1m 17s\n",
      "Epoch 69/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000113, dice: 0.013880, loss: 0.006997\n",
      "val: bce: 0.000305, dice: 0.029749, loss: 0.015027\n",
      "1m 17s\n",
      "Epoch 70/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000133, dice: 0.015233, loss: 0.007683\n",
      "val: bce: 0.000304, dice: 0.036510, loss: 0.018407\n",
      "1m 18s\n",
      "Epoch 71/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000097, dice: 0.013189, loss: 0.006643\n",
      "val: bce: 0.000353, dice: 0.038425, loss: 0.019389\n",
      "1m 18s\n",
      "Epoch 72/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000123, dice: 0.014715, loss: 0.007419\n",
      "val: bce: 0.000326, dice: 0.038946, loss: 0.019636\n",
      "1m 18s\n",
      "Epoch 73/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000106, dice: 0.013178, loss: 0.006642\n",
      "val: bce: 0.000319, dice: 0.038694, loss: 0.019507\n",
      "1m 18s\n",
      "Epoch 74/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000166, dice: 0.020726, loss: 0.010446\n",
      "val: bce: 0.000435, dice: 0.042094, loss: 0.021264\n",
      "1m 18s\n",
      "Epoch 75/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000193, dice: 0.021156, loss: 0.010675\n",
      "val: bce: 0.000333, dice: 0.029435, loss: 0.014884\n",
      "1m 18s\n",
      "Epoch 76/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000117, dice: 0.014106, loss: 0.007111\n",
      "val: bce: 0.000282, dice: 0.028516, loss: 0.014399\n",
      "1m 18s\n",
      "Epoch 77/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000100, dice: 0.011160, loss: 0.005630\n",
      "val: bce: 0.000220, dice: 0.027358, loss: 0.013789\n",
      "1m 18s\n",
      "Epoch 78/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000122, dice: 0.012845, loss: 0.006484\n",
      "val: bce: 0.000428, dice: 0.033221, loss: 0.016824\n",
      "1m 18s\n",
      "Epoch 79/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000115, dice: 0.013020, loss: 0.006568\n",
      "val: bce: 0.000290, dice: 0.026484, loss: 0.013387\n",
      "1m 18s\n",
      "Epoch 80/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000131, dice: 0.013655, loss: 0.006893\n",
      "val: bce: 0.000365, dice: 0.035159, loss: 0.017762\n",
      "1m 18s\n",
      "Epoch 81/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000138, dice: 0.017187, loss: 0.008663\n",
      "val: bce: 0.000311, dice: 0.035188, loss: 0.017750\n",
      "1m 18s\n",
      "Epoch 82/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000111, dice: 0.014088, loss: 0.007099\n",
      "val: bce: 0.000281, dice: 0.031576, loss: 0.015929\n",
      "1m 18s\n",
      "Epoch 83/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000112, dice: 0.013180, loss: 0.006646\n",
      "val: bce: 0.000296, dice: 0.028731, loss: 0.014514\n",
      "1m 18s\n",
      "Epoch 84/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000128, dice: 0.013834, loss: 0.006981\n",
      "val: bce: 0.000408, dice: 0.034896, loss: 0.017652\n",
      "1m 18s\n",
      "Epoch 85/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000135, dice: 0.016239, loss: 0.008187\n",
      "val: bce: 0.000426, dice: 0.036691, loss: 0.018559\n",
      "1m 18s\n",
      "Epoch 86/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000111, dice: 0.011951, loss: 0.006031\n",
      "val: bce: 0.000334, dice: 0.033056, loss: 0.016695\n",
      "1m 18s\n",
      "Epoch 87/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000127, dice: 0.013441, loss: 0.006784\n",
      "val: bce: 0.000287, dice: 0.031436, loss: 0.015861\n",
      "1m 18s\n",
      "Epoch 88/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000131, dice: 0.014098, loss: 0.007114\n",
      "val: bce: 0.000454, dice: 0.038174, loss: 0.019314\n",
      "1m 18s\n",
      "Epoch 89/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000137, dice: 0.015160, loss: 0.007648\n",
      "val: bce: 0.000356, dice: 0.032420, loss: 0.016388\n",
      "1m 18s\n",
      "Epoch 90/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000103, dice: 0.010770, loss: 0.005437\n",
      "val: bce: 0.000578, dice: 0.046019, loss: 0.023298\n",
      "1m 18s\n",
      "Epoch 91/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000134, dice: 0.015270, loss: 0.007702\n",
      "val: bce: 0.000304, dice: 0.030975, loss: 0.015639\n",
      "1m 18s\n",
      "Epoch 92/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000096, dice: 0.010439, loss: 0.005267\n",
      "val: bce: 0.000363, dice: 0.031226, loss: 0.015794\n",
      "1m 18s\n",
      "Epoch 93/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000089, dice: 0.009786, loss: 0.004938\n",
      "val: bce: 0.000300, dice: 0.028821, loss: 0.014561\n",
      "1m 18s\n",
      "Epoch 94/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000128, dice: 0.014598, loss: 0.007363\n",
      "val: bce: 0.000284, dice: 0.031468, loss: 0.015876\n",
      "1m 18s\n",
      "Epoch 95/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000122, dice: 0.013614, loss: 0.006868\n",
      "val: bce: 0.000305, dice: 0.029592, loss: 0.014949\n",
      "1m 18s\n",
      "Epoch 96/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000109, dice: 0.011338, loss: 0.005724\n",
      "val: bce: 0.000306, dice: 0.031395, loss: 0.015850\n",
      "1m 18s\n",
      "Epoch 97/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000108, dice: 0.012126, loss: 0.006117\n",
      "val: bce: 0.000155, dice: 0.019535, loss: 0.009845\n",
      "saving best model\n",
      "1m 18s\n",
      "Epoch 98/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000089, dice: 0.010716, loss: 0.005402\n",
      "val: bce: 0.000363, dice: 0.034468, loss: 0.017416\n",
      "1m 18s\n",
      "Epoch 99/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000095, dice: 0.010044, loss: 0.005070\n",
      "val: bce: 0.000327, dice: 0.029699, loss: 0.015013\n",
      "1m 18s\n",
      "Epoch 100/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000091, dice: 0.009263, loss: 0.004677\n",
      "val: bce: 0.000370, dice: 0.030450, loss: 0.015410\n",
      "1m 18s\n",
      "Epoch 101/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000080, dice: 0.007857, loss: 0.003969\n",
      "val: bce: 0.000317, dice: 0.029162, loss: 0.014739\n",
      "1m 17s\n",
      "Epoch 102/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000083, dice: 0.009325, loss: 0.004704\n",
      "val: bce: 0.000339, dice: 0.033303, loss: 0.016821\n",
      "1m 17s\n",
      "Epoch 103/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000092, dice: 0.009730, loss: 0.004911\n",
      "val: bce: 0.000336, dice: 0.035029, loss: 0.017683\n",
      "1m 17s\n",
      "Epoch 104/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000103, dice: 0.012404, loss: 0.006253\n",
      "val: bce: 0.000473, dice: 0.031586, loss: 0.016030\n",
      "1m 17s\n",
      "Epoch 105/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000153, dice: 0.013808, loss: 0.006980\n",
      "val: bce: 0.000769, dice: 0.046513, loss: 0.023641\n",
      "1m 17s\n",
      "Epoch 106/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000161, dice: 0.013833, loss: 0.006997\n",
      "val: bce: 0.000449, dice: 0.037126, loss: 0.018788\n",
      "1m 17s\n",
      "Epoch 107/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000119, dice: 0.011695, loss: 0.005907\n",
      "val: bce: 0.000313, dice: 0.029084, loss: 0.014698\n",
      "1m 17s\n",
      "Epoch 108/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000137, dice: 0.013349, loss: 0.006743\n",
      "val: bce: 0.000310, dice: 0.033523, loss: 0.016917\n",
      "1m 17s\n",
      "Epoch 109/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000114, dice: 0.011692, loss: 0.005903\n",
      "val: bce: 0.000465, dice: 0.039891, loss: 0.020178\n",
      "1m 17s\n",
      "Epoch 110/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000097, dice: 0.010078, loss: 0.005088\n",
      "val: bce: 0.000469, dice: 0.041615, loss: 0.021042\n",
      "1m 17s\n",
      "Epoch 111/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000104, dice: 0.010494, loss: 0.005299\n",
      "val: bce: 0.000314, dice: 0.030642, loss: 0.015478\n",
      "1m 17s\n",
      "Epoch 112/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000105, dice: 0.010137, loss: 0.005121\n",
      "val: bce: 0.000312, dice: 0.026628, loss: 0.013470\n",
      "1m 17s\n",
      "Epoch 113/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000110, dice: 0.010899, loss: 0.005505\n",
      "val: bce: 0.000370, dice: 0.034340, loss: 0.017355\n",
      "1m 17s\n",
      "Epoch 114/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000094, dice: 0.009863, loss: 0.004979\n",
      "val: bce: 0.000469, dice: 0.036772, loss: 0.018621\n",
      "1m 17s\n",
      "Epoch 115/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000078, dice: 0.008654, loss: 0.004366\n",
      "val: bce: 0.000427, dice: 0.042263, loss: 0.021345\n",
      "1m 17s\n",
      "Epoch 116/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000086, dice: 0.010463, loss: 0.005274\n",
      "val: bce: 0.000301, dice: 0.030611, loss: 0.015456\n",
      "1m 17s\n",
      "Epoch 117/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000107, dice: 0.011588, loss: 0.005847\n",
      "val: bce: 0.000217, dice: 0.024206, loss: 0.012211\n",
      "1m 17s\n",
      "Epoch 118/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000101, dice: 0.012456, loss: 0.006279\n",
      "val: bce: 0.000425, dice: 0.039274, loss: 0.019849\n",
      "1m 17s\n",
      "Epoch 119/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000098, dice: 0.009573, loss: 0.004836\n",
      "val: bce: 0.000276, dice: 0.033477, loss: 0.016876\n",
      "1m 17s\n",
      "Epoch 120/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000078, dice: 0.008492, loss: 0.004285\n",
      "val: bce: 0.000315, dice: 0.030736, loss: 0.015525\n",
      "1m 17s\n",
      "Epoch 121/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000083, dice: 0.008154, loss: 0.004119\n",
      "val: bce: 0.000313, dice: 0.028432, loss: 0.014372\n",
      "1m 18s\n",
      "Epoch 122/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000105, dice: 0.009279, loss: 0.004692\n",
      "val: bce: 0.000311, dice: 0.033046, loss: 0.016678\n",
      "1m 18s\n",
      "Epoch 123/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000088, dice: 0.009133, loss: 0.004611\n",
      "val: bce: 0.000247, dice: 0.031786, loss: 0.016017\n",
      "1m 18s\n",
      "Epoch 124/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000124, dice: 0.010675, loss: 0.005400\n",
      "val: bce: 0.000431, dice: 0.028711, loss: 0.014571\n",
      "1m 18s\n",
      "Epoch 125/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000084, dice: 0.008375, loss: 0.004229\n",
      "val: bce: 0.000292, dice: 0.027249, loss: 0.013770\n",
      "1m 18s\n",
      "Epoch 126/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000079, dice: 0.007998, loss: 0.004038\n",
      "val: bce: 0.000250, dice: 0.027318, loss: 0.013784\n",
      "1m 18s\n",
      "Epoch 127/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000070, dice: 0.007606, loss: 0.003838\n",
      "val: bce: 0.000289, dice: 0.028069, loss: 0.014179\n",
      "1m 18s\n",
      "Epoch 128/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000077, dice: 0.008127, loss: 0.004102\n",
      "val: bce: 0.000307, dice: 0.032134, loss: 0.016221\n",
      "1m 18s\n",
      "Epoch 129/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000070, dice: 0.007565, loss: 0.003818\n",
      "val: bce: 0.000276, dice: 0.030669, loss: 0.015473\n",
      "1m 18s\n",
      "Epoch 130/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000078, dice: 0.008751, loss: 0.004414\n",
      "val: bce: 0.000394, dice: 0.033560, loss: 0.016977\n",
      "1m 18s\n",
      "Epoch 131/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000081, dice: 0.008386, loss: 0.004234\n",
      "val: bce: 0.000384, dice: 0.035573, loss: 0.017979\n",
      "1m 18s\n",
      "Epoch 132/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000078, dice: 0.008712, loss: 0.004395\n",
      "val: bce: 0.000324, dice: 0.038434, loss: 0.019379\n",
      "1m 18s\n",
      "Epoch 133/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000081, dice: 0.009078, loss: 0.004579\n",
      "val: bce: 0.000340, dice: 0.030301, loss: 0.015320\n",
      "1m 18s\n",
      "Epoch 134/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000087, dice: 0.008582, loss: 0.004334\n",
      "val: bce: 0.000496, dice: 0.037254, loss: 0.018875\n",
      "1m 18s\n",
      "Epoch 135/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000091, dice: 0.009529, loss: 0.004810\n",
      "val: bce: 0.000277, dice: 0.026835, loss: 0.013556\n",
      "1m 18s\n",
      "Epoch 136/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000111, dice: 0.009432, loss: 0.004771\n",
      "val: bce: 0.000326, dice: 0.030159, loss: 0.015242\n",
      "1m 18s\n",
      "Epoch 137/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000073, dice: 0.006661, loss: 0.003367\n",
      "val: bce: 0.000309, dice: 0.029726, loss: 0.015018\n",
      "1m 18s\n",
      "Epoch 138/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000088, dice: 0.009069, loss: 0.004578\n",
      "val: bce: 0.000257, dice: 0.027400, loss: 0.013829\n",
      "1m 18s\n",
      "Epoch 139/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000085, dice: 0.008154, loss: 0.004119\n",
      "val: bce: 0.000288, dice: 0.027468, loss: 0.013878\n",
      "1m 18s\n",
      "Epoch 140/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000090, dice: 0.010116, loss: 0.005103\n",
      "val: bce: 0.000357, dice: 0.027184, loss: 0.013770\n",
      "1m 18s\n",
      "Epoch 141/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000071, dice: 0.006795, loss: 0.003433\n",
      "val: bce: 0.000330, dice: 0.031704, loss: 0.016017\n",
      "1m 18s\n",
      "Epoch 142/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000078, dice: 0.007859, loss: 0.003969\n",
      "val: bce: 0.000302, dice: 0.028416, loss: 0.014359\n",
      "1m 18s\n",
      "Epoch 143/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000081, dice: 0.008587, loss: 0.004334\n",
      "val: bce: 0.000273, dice: 0.023975, loss: 0.012124\n",
      "1m 18s\n",
      "Epoch 144/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000072, dice: 0.006540, loss: 0.003306\n",
      "val: bce: 0.000345, dice: 0.028950, loss: 0.014647\n",
      "1m 18s\n",
      "Epoch 145/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000071, dice: 0.006532, loss: 0.003302\n",
      "val: bce: 0.000269, dice: 0.027825, loss: 0.014047\n",
      "1m 18s\n",
      "Epoch 146/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000075, dice: 0.006498, loss: 0.003287\n",
      "val: bce: 0.000364, dice: 0.027181, loss: 0.013772\n",
      "1m 18s\n",
      "Epoch 147/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000064, dice: 0.005712, loss: 0.002888\n",
      "val: bce: 0.000307, dice: 0.025958, loss: 0.013132\n",
      "1m 18s\n",
      "Epoch 148/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000068, dice: 0.006243, loss: 0.003156\n",
      "val: bce: 0.000440, dice: 0.029116, loss: 0.014778\n",
      "1m 18s\n",
      "Epoch 149/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000077, dice: 0.007502, loss: 0.003790\n",
      "val: bce: 0.000292, dice: 0.028932, loss: 0.014612\n",
      "1m 18s\n",
      "Epoch 150/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000092, dice: 0.009035, loss: 0.004563\n",
      "val: bce: 0.000366, dice: 0.031967, loss: 0.016167\n",
      "1m 18s\n",
      "Epoch 151/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000074, dice: 0.008927, loss: 0.004501\n",
      "val: bce: 0.000223, dice: 0.025810, loss: 0.013016\n",
      "1m 18s\n",
      "Epoch 152/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000061, dice: 0.005928, loss: 0.002995\n",
      "val: bce: 0.000264, dice: 0.027601, loss: 0.013933\n",
      "1m 18s\n",
      "Epoch 153/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000056, dice: 0.006047, loss: 0.003051\n",
      "val: bce: 0.000299, dice: 0.026876, loss: 0.013588\n",
      "1m 18s\n",
      "Epoch 154/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000068, dice: 0.006853, loss: 0.003461\n",
      "val: bce: 0.000305, dice: 0.028043, loss: 0.014174\n",
      "1m 18s\n",
      "Epoch 155/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000071, dice: 0.006017, loss: 0.003044\n",
      "val: bce: 0.000280, dice: 0.028164, loss: 0.014222\n",
      "1m 18s\n",
      "Epoch 156/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000060, dice: 0.005109, loss: 0.002584\n",
      "val: bce: 0.000293, dice: 0.028262, loss: 0.014278\n",
      "1m 18s\n",
      "Epoch 157/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000061, dice: 0.005207, loss: 0.002634\n",
      "val: bce: 0.000281, dice: 0.026347, loss: 0.013314\n",
      "1m 18s\n",
      "Epoch 158/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000074, dice: 0.006621, loss: 0.003347\n",
      "val: bce: 0.000422, dice: 0.039164, loss: 0.019793\n",
      "1m 18s\n",
      "Epoch 159/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000065, dice: 0.005642, loss: 0.002854\n",
      "val: bce: 0.000359, dice: 0.030872, loss: 0.015615\n",
      "1m 18s\n",
      "Epoch 160/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000071, dice: 0.006479, loss: 0.003275\n",
      "val: bce: 0.000311, dice: 0.034594, loss: 0.017452\n",
      "1m 18s\n",
      "Epoch 161/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000066, dice: 0.005616, loss: 0.002841\n",
      "val: bce: 0.000518, dice: 0.038153, loss: 0.019336\n",
      "1m 18s\n",
      "Epoch 162/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000072, dice: 0.005891, loss: 0.002982\n",
      "val: bce: 0.000384, dice: 0.035215, loss: 0.017799\n",
      "1m 18s\n",
      "Epoch 163/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000078, dice: 0.008339, loss: 0.004209\n",
      "val: bce: 0.000493, dice: 0.037529, loss: 0.019011\n",
      "1m 18s\n",
      "Epoch 164/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000077, dice: 0.007125, loss: 0.003601\n",
      "val: bce: 0.000442, dice: 0.032700, loss: 0.016571\n",
      "1m 18s\n",
      "Epoch 165/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000141, dice: 0.014845, loss: 0.007493\n",
      "val: bce: 0.000420, dice: 0.034548, loss: 0.017484\n",
      "1m 18s\n",
      "Epoch 166/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000075, dice: 0.007315, loss: 0.003695\n",
      "val: bce: 0.000386, dice: 0.029427, loss: 0.014906\n",
      "1m 18s\n",
      "Epoch 167/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000063, dice: 0.005148, loss: 0.002605\n",
      "val: bce: 0.000337, dice: 0.027416, loss: 0.013877\n",
      "1m 18s\n",
      "Epoch 168/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000062, dice: 0.006891, loss: 0.003477\n",
      "val: bce: 0.000336, dice: 0.028819, loss: 0.014577\n",
      "1m 18s\n",
      "Epoch 169/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000061, dice: 0.005659, loss: 0.002860\n",
      "val: bce: 0.000334, dice: 0.031553, loss: 0.015943\n",
      "1m 18s\n",
      "Epoch 170/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000062, dice: 0.005468, loss: 0.002765\n",
      "val: bce: 0.000314, dice: 0.030264, loss: 0.015289\n",
      "1m 18s\n",
      "Epoch 171/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000057, dice: 0.005129, loss: 0.002593\n",
      "val: bce: 0.000318, dice: 0.027202, loss: 0.013760\n",
      "1m 18s\n",
      "Epoch 172/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000073, dice: 0.006339, loss: 0.003206\n",
      "val: bce: 0.000307, dice: 0.036966, loss: 0.018637\n",
      "1m 18s\n",
      "Epoch 173/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000091, dice: 0.007590, loss: 0.003840\n",
      "val: bce: 0.000398, dice: 0.027720, loss: 0.014059\n",
      "1m 18s\n",
      "Epoch 174/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000072, dice: 0.005819, loss: 0.002946\n",
      "val: bce: 0.000412, dice: 0.031385, loss: 0.015898\n",
      "1m 18s\n",
      "Epoch 175/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000061, dice: 0.005667, loss: 0.002864\n",
      "val: bce: 0.000429, dice: 0.031113, loss: 0.015771\n",
      "1m 18s\n",
      "Epoch 176/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000053, dice: 0.004333, loss: 0.002193\n",
      "val: bce: 0.000416, dice: 0.031446, loss: 0.015931\n",
      "1m 18s\n",
      "Epoch 177/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000054, dice: 0.004430, loss: 0.002242\n",
      "val: bce: 0.000405, dice: 0.033974, loss: 0.017190\n",
      "1m 18s\n",
      "Epoch 178/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000052, dice: 0.004496, loss: 0.002274\n",
      "val: bce: 0.000358, dice: 0.031744, loss: 0.016051\n",
      "1m 18s\n",
      "Epoch 179/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000085, dice: 0.007857, loss: 0.003971\n",
      "val: bce: 0.000409, dice: 0.034872, loss: 0.017640\n",
      "1m 18s\n",
      "Epoch 180/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000109, dice: 0.010989, loss: 0.005549\n",
      "val: bce: 0.000461, dice: 0.042265, loss: 0.021363\n",
      "1m 18s\n",
      "Epoch 181/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000093, dice: 0.008997, loss: 0.004545\n",
      "val: bce: 0.000344, dice: 0.027730, loss: 0.014037\n",
      "1m 18s\n",
      "Epoch 182/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000078, dice: 0.006897, loss: 0.003487\n",
      "val: bce: 0.000386, dice: 0.030383, loss: 0.015384\n",
      "1m 18s\n",
      "Epoch 183/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000066, dice: 0.005207, loss: 0.002637\n",
      "val: bce: 0.000275, dice: 0.024618, loss: 0.012447\n",
      "1m 18s\n",
      "Epoch 184/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000059, dice: 0.004610, loss: 0.002335\n",
      "val: bce: 0.000298, dice: 0.025255, loss: 0.012777\n",
      "1m 18s\n",
      "Epoch 185/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000063, dice: 0.005086, loss: 0.002574\n",
      "val: bce: 0.000420, dice: 0.032390, loss: 0.016405\n",
      "1m 18s\n",
      "Epoch 186/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000070, dice: 0.005588, loss: 0.002829\n",
      "val: bce: 0.000348, dice: 0.027464, loss: 0.013906\n",
      "1m 18s\n",
      "Epoch 187/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000059, dice: 0.004476, loss: 0.002267\n",
      "val: bce: 0.000341, dice: 0.027323, loss: 0.013832\n",
      "1m 18s\n",
      "Epoch 188/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000065, dice: 0.006226, loss: 0.003145\n",
      "val: bce: 0.000306, dice: 0.027346, loss: 0.013826\n",
      "1m 18s\n",
      "Epoch 189/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000055, dice: 0.004690, loss: 0.002373\n",
      "val: bce: 0.000402, dice: 0.029371, loss: 0.014886\n",
      "1m 17s\n",
      "Epoch 190/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000070, dice: 0.006599, loss: 0.003334\n",
      "val: bce: 0.000314, dice: 0.028584, loss: 0.014449\n",
      "1m 17s\n",
      "Epoch 191/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000058, dice: 0.005582, loss: 0.002820\n",
      "val: bce: 0.000459, dice: 0.036347, loss: 0.018403\n",
      "1m 18s\n",
      "Epoch 192/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000057, dice: 0.005206, loss: 0.002631\n",
      "val: bce: 0.000301, dice: 0.028075, loss: 0.014188\n",
      "1m 17s\n",
      "Epoch 193/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000047, dice: 0.003893, loss: 0.001970\n",
      "val: bce: 0.000267, dice: 0.026076, loss: 0.013172\n",
      "1m 18s\n",
      "Epoch 194/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000047, dice: 0.003964, loss: 0.002005\n",
      "val: bce: 0.000297, dice: 0.030748, loss: 0.015522\n",
      "1m 18s\n",
      "Epoch 195/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000045, dice: 0.003930, loss: 0.001987\n",
      "val: bce: 0.000288, dice: 0.027149, loss: 0.013719\n",
      "1m 18s\n",
      "Epoch 196/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000077, dice: 0.007198, loss: 0.003638\n",
      "val: bce: 0.000450, dice: 0.030249, loss: 0.015349\n",
      "1m 18s\n",
      "Epoch 197/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000075, dice: 0.006366, loss: 0.003221\n",
      "val: bce: 0.000435, dice: 0.026979, loss: 0.013707\n",
      "1m 18s\n",
      "Epoch 198/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000069, dice: 0.006623, loss: 0.003346\n",
      "val: bce: 0.000370, dice: 0.024759, loss: 0.012565\n",
      "1m 18s\n",
      "Epoch 199/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000066, dice: 0.005565, loss: 0.002816\n",
      "val: bce: 0.000507, dice: 0.036736, loss: 0.018621\n",
      "1m 18s\n",
      "Epoch 200/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000055, dice: 0.004675, loss: 0.002365\n",
      "val: bce: 0.000476, dice: 0.034908, loss: 0.017692\n",
      "1m 18s\n",
      "Epoch 201/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000055, dice: 0.004336, loss: 0.002195\n",
      "val: bce: 0.000504, dice: 0.032913, loss: 0.016708\n",
      "1m 17s\n",
      "Epoch 202/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000082, dice: 0.006538, loss: 0.003310\n",
      "val: bce: 0.000458, dice: 0.029838, loss: 0.015148\n",
      "1m 17s\n",
      "Epoch 203/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000079, dice: 0.006173, loss: 0.003126\n",
      "val: bce: 0.000416, dice: 0.032151, loss: 0.016284\n",
      "1m 17s\n",
      "Epoch 204/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000066, dice: 0.005226, loss: 0.002646\n",
      "val: bce: 0.000564, dice: 0.033315, loss: 0.016939\n",
      "1m 18s\n",
      "Epoch 205/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000063, dice: 0.005261, loss: 0.002662\n",
      "val: bce: 0.000637, dice: 0.038368, loss: 0.019503\n",
      "1m 18s\n",
      "Epoch 206/599\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000075, dice: 0.006663, loss: 0.003369\n",
      "val: bce: 0.000575, dice: 0.035902, loss: 0.018239\n",
      "1m 18s\n",
      "Epoch 207/599\n",
      "----------\n",
      "LR 0.0001\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-1a007f8957b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mexp_lr_scheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStepLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_lr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m600\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-22abf4db1873>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m                 \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "num_class = 4\n",
    "model = ResNetUNet(num_class).to(device)\n",
    "\n",
    "# freeze backbone layers\n",
    "#for l in model.base_layers:\n",
    "#    for param in l.parameters():\n",
    "#        param.requires_grad = False\n",
    "\n",
    "optimizer_ft = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=300, gamma=0.1)\n",
    "\n",
    "model = train_model(model, optimizer_ft, exp_lr_scheduler, num_epochs=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNetUNet(\n",
       "  (base_model): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       "  )\n",
       "  (layer0): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (layer0_1x1): Sequential(\n",
       "    (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (layer1): Sequential(\n",
       "    (0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer1_1x1): Sequential(\n",
       "    (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2_1x1): Sequential(\n",
       "    (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3_1x1): Sequential(\n",
       "    (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4_1x1): Sequential(\n",
       "    (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (upsample): Upsample(scale_factor=2.0, mode=bilinear)\n",
       "  (conv_up3): Sequential(\n",
       "    (0): Conv2d(768, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv_up2): Sequential(\n",
       "    (0): Conv2d(640, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv_up1): Sequential(\n",
       "    (0): Conv2d(320, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv_up0): Sequential(\n",
       "    (0): Conv2d(320, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv_original_size0): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv_original_size1): Sequential(\n",
       "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv_original_size2): Sequential(\n",
       "    (0): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv_last): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://discuss.pytorch.org/t/save-and-load-model/6206/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/a/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ResNetUNet. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/a/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ResNet. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/a/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Conv2d. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/a/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BatchNorm2d. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/a/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/a/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type MaxPool2d. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/a/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Sequential. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/a/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BasicBlock. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/a/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type AdaptiveAvgPool2d. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/a/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/a/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Upsample. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(model, '/home/a/Documents/GNN-for-trans-grouping/GNN-for-trans-grouping/Unet_model_multi_seq_strict_encoding.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('/home/a/Documents/GNN-for-trans-grouping/GNN-for-trans-grouping/Unet_model_multi_seq_strict_encoding.pt')\n",
    "# model = torch.load(r'C:\\Users\\andy.knapper\\Documents\\OW\\Categorisation\\ML grouping\\GNN-for-trans-grouping\\Unet_model_1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sim_data = [group_to_image_constructors.make_an_image(*np.array(synthetic_data.make_a_group())) for _ in range(3)]\n",
    "# input_images = np.array([x[0] for  x in sim_data]).astype('uint8')\n",
    "# target_masks = np.array([x[1] for  x in sim_data]).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# output = model(torch.tensor(test_img))\n",
    "# prediction = torch.argmax(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 224, 224]           1,792\n",
      "              ReLU-2         [-1, 64, 224, 224]               0\n",
      "            Conv2d-3         [-1, 64, 224, 224]          36,928\n",
      "              ReLU-4         [-1, 64, 224, 224]               0\n",
      "            Conv2d-5         [-1, 64, 112, 112]           9,408\n",
      "            Conv2d-6         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-7         [-1, 64, 112, 112]             128\n",
      "       BatchNorm2d-8         [-1, 64, 112, 112]             128\n",
      "              ReLU-9         [-1, 64, 112, 112]               0\n",
      "             ReLU-10         [-1, 64, 112, 112]               0\n",
      "        MaxPool2d-11           [-1, 64, 56, 56]               0\n",
      "        MaxPool2d-12           [-1, 64, 56, 56]               0\n",
      "           Conv2d-13           [-1, 64, 56, 56]          36,864\n",
      "           Conv2d-14           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-15           [-1, 64, 56, 56]             128\n",
      "      BatchNorm2d-16           [-1, 64, 56, 56]             128\n",
      "             ReLU-17           [-1, 64, 56, 56]               0\n",
      "             ReLU-18           [-1, 64, 56, 56]               0\n",
      "           Conv2d-19           [-1, 64, 56, 56]          36,864\n",
      "           Conv2d-20           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-21           [-1, 64, 56, 56]             128\n",
      "      BatchNorm2d-22           [-1, 64, 56, 56]             128\n",
      "             ReLU-23           [-1, 64, 56, 56]               0\n",
      "             ReLU-24           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-25           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-26           [-1, 64, 56, 56]               0\n",
      "           Conv2d-27           [-1, 64, 56, 56]          36,864\n",
      "           Conv2d-28           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-29           [-1, 64, 56, 56]             128\n",
      "      BatchNorm2d-30           [-1, 64, 56, 56]             128\n",
      "             ReLU-31           [-1, 64, 56, 56]               0\n",
      "             ReLU-32           [-1, 64, 56, 56]               0\n",
      "           Conv2d-33           [-1, 64, 56, 56]          36,864\n",
      "           Conv2d-34           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-35           [-1, 64, 56, 56]             128\n",
      "      BatchNorm2d-36           [-1, 64, 56, 56]             128\n",
      "             ReLU-37           [-1, 64, 56, 56]               0\n",
      "             ReLU-38           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-39           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-40           [-1, 64, 56, 56]               0\n",
      "           Conv2d-41          [-1, 128, 28, 28]          73,728\n",
      "           Conv2d-42          [-1, 128, 28, 28]          73,728\n",
      "      BatchNorm2d-43          [-1, 128, 28, 28]             256\n",
      "      BatchNorm2d-44          [-1, 128, 28, 28]             256\n",
      "             ReLU-45          [-1, 128, 28, 28]               0\n",
      "             ReLU-46          [-1, 128, 28, 28]               0\n",
      "           Conv2d-47          [-1, 128, 28, 28]         147,456\n",
      "           Conv2d-48          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-49          [-1, 128, 28, 28]             256\n",
      "      BatchNorm2d-50          [-1, 128, 28, 28]             256\n",
      "           Conv2d-51          [-1, 128, 28, 28]           8,192\n",
      "           Conv2d-52          [-1, 128, 28, 28]           8,192\n",
      "      BatchNorm2d-53          [-1, 128, 28, 28]             256\n",
      "      BatchNorm2d-54          [-1, 128, 28, 28]             256\n",
      "             ReLU-55          [-1, 128, 28, 28]               0\n",
      "             ReLU-56          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-57          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-58          [-1, 128, 28, 28]               0\n",
      "           Conv2d-59          [-1, 128, 28, 28]         147,456\n",
      "           Conv2d-60          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-61          [-1, 128, 28, 28]             256\n",
      "      BatchNorm2d-62          [-1, 128, 28, 28]             256\n",
      "             ReLU-63          [-1, 128, 28, 28]               0\n",
      "             ReLU-64          [-1, 128, 28, 28]               0\n",
      "           Conv2d-65          [-1, 128, 28, 28]         147,456\n",
      "           Conv2d-66          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-67          [-1, 128, 28, 28]             256\n",
      "      BatchNorm2d-68          [-1, 128, 28, 28]             256\n",
      "             ReLU-69          [-1, 128, 28, 28]               0\n",
      "             ReLU-70          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-71          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-72          [-1, 128, 28, 28]               0\n",
      "           Conv2d-73          [-1, 256, 14, 14]         294,912\n",
      "           Conv2d-74          [-1, 256, 14, 14]         294,912\n",
      "      BatchNorm2d-75          [-1, 256, 14, 14]             512\n",
      "      BatchNorm2d-76          [-1, 256, 14, 14]             512\n",
      "             ReLU-77          [-1, 256, 14, 14]               0\n",
      "             ReLU-78          [-1, 256, 14, 14]               0\n",
      "           Conv2d-79          [-1, 256, 14, 14]         589,824\n",
      "           Conv2d-80          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-81          [-1, 256, 14, 14]             512\n",
      "      BatchNorm2d-82          [-1, 256, 14, 14]             512\n",
      "           Conv2d-83          [-1, 256, 14, 14]          32,768\n",
      "           Conv2d-84          [-1, 256, 14, 14]          32,768\n",
      "      BatchNorm2d-85          [-1, 256, 14, 14]             512\n",
      "      BatchNorm2d-86          [-1, 256, 14, 14]             512\n",
      "             ReLU-87          [-1, 256, 14, 14]               0\n",
      "             ReLU-88          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-89          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-90          [-1, 256, 14, 14]               0\n",
      "           Conv2d-91          [-1, 256, 14, 14]         589,824\n",
      "           Conv2d-92          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-93          [-1, 256, 14, 14]             512\n",
      "      BatchNorm2d-94          [-1, 256, 14, 14]             512\n",
      "             ReLU-95          [-1, 256, 14, 14]               0\n",
      "             ReLU-96          [-1, 256, 14, 14]               0\n",
      "           Conv2d-97          [-1, 256, 14, 14]         589,824\n",
      "           Conv2d-98          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-99          [-1, 256, 14, 14]             512\n",
      "     BatchNorm2d-100          [-1, 256, 14, 14]             512\n",
      "            ReLU-101          [-1, 256, 14, 14]               0\n",
      "            ReLU-102          [-1, 256, 14, 14]               0\n",
      "      BasicBlock-103          [-1, 256, 14, 14]               0\n",
      "      BasicBlock-104          [-1, 256, 14, 14]               0\n",
      "          Conv2d-105            [-1, 512, 7, 7]       1,179,648\n",
      "          Conv2d-106            [-1, 512, 7, 7]       1,179,648\n",
      "     BatchNorm2d-107            [-1, 512, 7, 7]           1,024\n",
      "     BatchNorm2d-108            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-109            [-1, 512, 7, 7]               0\n",
      "            ReLU-110            [-1, 512, 7, 7]               0\n",
      "          Conv2d-111            [-1, 512, 7, 7]       2,359,296\n",
      "          Conv2d-112            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-113            [-1, 512, 7, 7]           1,024\n",
      "     BatchNorm2d-114            [-1, 512, 7, 7]           1,024\n",
      "          Conv2d-115            [-1, 512, 7, 7]         131,072\n",
      "          Conv2d-116            [-1, 512, 7, 7]         131,072\n",
      "     BatchNorm2d-117            [-1, 512, 7, 7]           1,024\n",
      "     BatchNorm2d-118            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-119            [-1, 512, 7, 7]               0\n",
      "            ReLU-120            [-1, 512, 7, 7]               0\n",
      "      BasicBlock-121            [-1, 512, 7, 7]               0\n",
      "      BasicBlock-122            [-1, 512, 7, 7]               0\n",
      "          Conv2d-123            [-1, 512, 7, 7]       2,359,296\n",
      "          Conv2d-124            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-125            [-1, 512, 7, 7]           1,024\n",
      "     BatchNorm2d-126            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-127            [-1, 512, 7, 7]               0\n",
      "            ReLU-128            [-1, 512, 7, 7]               0\n",
      "          Conv2d-129            [-1, 512, 7, 7]       2,359,296\n",
      "          Conv2d-130            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-131            [-1, 512, 7, 7]           1,024\n",
      "     BatchNorm2d-132            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-133            [-1, 512, 7, 7]               0\n",
      "            ReLU-134            [-1, 512, 7, 7]               0\n",
      "      BasicBlock-135            [-1, 512, 7, 7]               0\n",
      "      BasicBlock-136            [-1, 512, 7, 7]               0\n",
      "          Conv2d-137            [-1, 512, 7, 7]         262,656\n",
      "            ReLU-138            [-1, 512, 7, 7]               0\n",
      "        Upsample-139          [-1, 512, 14, 14]               0\n",
      "          Conv2d-140          [-1, 256, 14, 14]          65,792\n",
      "            ReLU-141          [-1, 256, 14, 14]               0\n",
      "          Conv2d-142          [-1, 512, 14, 14]       3,539,456\n",
      "            ReLU-143          [-1, 512, 14, 14]               0\n",
      "        Upsample-144          [-1, 512, 28, 28]               0\n",
      "          Conv2d-145          [-1, 128, 28, 28]          16,512\n",
      "            ReLU-146          [-1, 128, 28, 28]               0\n",
      "          Conv2d-147          [-1, 256, 28, 28]       1,474,816\n",
      "            ReLU-148          [-1, 256, 28, 28]               0\n",
      "        Upsample-149          [-1, 256, 56, 56]               0\n",
      "          Conv2d-150           [-1, 64, 56, 56]           4,160\n",
      "            ReLU-151           [-1, 64, 56, 56]               0\n",
      "          Conv2d-152          [-1, 256, 56, 56]         737,536\n",
      "            ReLU-153          [-1, 256, 56, 56]               0\n",
      "        Upsample-154        [-1, 256, 112, 112]               0\n",
      "          Conv2d-155         [-1, 64, 112, 112]           4,160\n",
      "            ReLU-156         [-1, 64, 112, 112]               0\n",
      "          Conv2d-157        [-1, 128, 112, 112]         368,768\n",
      "            ReLU-158        [-1, 128, 112, 112]               0\n",
      "        Upsample-159        [-1, 128, 224, 224]               0\n",
      "          Conv2d-160         [-1, 64, 224, 224]         110,656\n",
      "            ReLU-161         [-1, 64, 224, 224]               0\n",
      "          Conv2d-162          [-1, 4, 224, 224]             260\n",
      "================================================================\n",
      "Total params: 28,976,516\n",
      "Trainable params: 28,976,516\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 416.88\n",
      "Params size (MB): 110.54\n",
      "Estimated Total Size (MB): 527.99\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, input_size=(3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = SimDataset(1000, transform = trans)\n",
    "val_set = SimDataset(2, transform = trans)\n",
    "\n",
    "image_datasets = {\n",
    "    'train': train_set, 'val': val_set\n",
    "}\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "dataloaders = {\n",
    "    'train': DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0),\n",
    "    'val': DataLoader(val_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.3544184673297289\n",
      "1 1.0\n",
      "2 0.0\n",
      "3 0.0\n",
      "4 0.1466375624196394\n",
      "5 0.01602959309494447\n",
      "6 0.0\n",
      "7 0.18731450172024272\n",
      "8 0.39613561576102707\n",
      "9 0.0\n",
      "10 0.0\n",
      "11 1.0\n",
      "12 1.0\n",
      "13 0.0\n",
      "14 0.0\n",
      "15 1.0\n",
      "16 0.0\n",
      "17 0.0\n",
      "18 0.17212619262594495\n",
      "19 1.0\n",
      "20 0.0\n",
      "21 1.0\n",
      "22 1.0\n",
      "23 1.0\n",
      "24 0.0\n",
      "25 0.3543277179498637\n",
      "26 0.0\n",
      "27 0.8816820680781378\n",
      "28 1.0\n",
      "29 0.0\n",
      "30 0.0\n",
      "31 0.09706281825692052\n",
      "32 0.89473449912788\n",
      "33 0.5831167363543903\n",
      "34 0.0\n",
      "35 0.0\n",
      "36 0.0\n",
      "37 0.8708692029991669\n",
      "38 0.21340273259596615\n",
      "39 0.19719866420008167\n",
      "40 0.48932190179267343\n",
      "41 0.7400158191787434\n",
      "42 0.6571258199165176\n",
      "43 0.0\n",
      "44 0.7027590784109226\n",
      "45 0.18338108882521495\n",
      "46 0.0\n",
      "47 0.0\n",
      "48 0.16304347826086954\n",
      "49 0.0\n",
      "50 1.0\n",
      "51 0.24661781285231107\n",
      "52 1.0\n",
      "53 1.0\n",
      "54 0.0\n",
      "55 0.22190201729106626\n",
      "56 0.8175892395240558\n",
      "57 0.0\n",
      "58 1.0\n",
      "59 1.0\n",
      "60 0.0\n",
      "61 1.0\n",
      "62 0.580110497237569\n",
      "63 0.56\n",
      "64 -0.01061445925227028\n",
      "65 0.0\n",
      "66 0.45996978851963743\n",
      "67 0.5815384615384616\n",
      "68 0.4324324324324324\n",
      "69 0.0\n",
      "70 0.0\n",
      "71 0.07981729070944786\n",
      "72 0.0\n",
      "73 1.0\n",
      "74 0.0\n",
      "75 0.8250565753080211\n",
      "76 1.0\n",
      "77 0.0\n",
      "78 1.0\n",
      "79 0.0\n",
      "80 0.9199693343555215\n",
      "81 0.0\n",
      "82 0.4660373733788752\n",
      "83 0.0\n",
      "84 0.0\n",
      "85 0.5663860802576747\n",
      "86 -0.01658083686698322\n",
      "87 0.0\n",
      "88 0.0\n",
      "89 0.44057377049180335\n",
      "90 1.0\n",
      "91 1.0\n",
      "92 1.0\n",
      "93 0.07529354306994167\n",
      "94 0.0\n",
      "95 0.0\n",
      "96 0.0\n",
      "97 1.0\n",
      "98 1.0\n",
      "99 0.0\n",
      "100 1.0\n",
      "101 1.0\n",
      "102 0.022372681778039406\n",
      "103 0.0\n",
      "104 0.0\n",
      "105 1.0\n",
      "106 0.0\n",
      "107 0.0\n",
      "108 0.0\n",
      "109 1.0\n",
      "110 -0.03636363636363643\n",
      "111 0.0\n",
      "112 0.29243549217084464\n",
      "113 0.0\n",
      "114 0.2709395484340859\n",
      "115 0.29714285714285715\n",
      "116 0.0\n",
      "117 0.0\n",
      "118 0.0\n",
      "119 1.0\n",
      "120 0.749299961973243\n",
      "121 -0.12338474364318469\n",
      "122 0.0\n",
      "123 -0.05248769819573546\n",
      "124 1.0\n",
      "125 0.0\n",
      "126 0.7815785741260666\n",
      "127 1.0\n",
      "128 1.0\n",
      "129 0.0\n",
      "130 1.0\n",
      "131 0.9069359991755127\n",
      "132 0.08292036195600407\n",
      "133 0.7494969070698164\n",
      "134 0.5611489667601527\n",
      "135 0.4035334088335221\n",
      "136 0.0\n",
      "137 1.0\n",
      "138 0.6057731770714578\n",
      "139 1.0\n",
      "140 0.3728115345005149\n",
      "141 1.0\n",
      "142 0.0\n",
      "143 0.0\n",
      "144 0.12660243898630605\n",
      "145 0.0\n",
      "146 0.0\n",
      "147 0.7052313141001992\n",
      "148 0.14808938187463686\n",
      "149 0.5934828198096468\n",
      "150 0.7250755287009063\n",
      "151 1.0\n",
      "152 0.0837150171540029\n",
      "153 0.507538954262478\n",
      "154 0.6547657559627091\n",
      "155 0.0\n",
      "156 0.0\n",
      "157 0.3824091778202676\n",
      "158 1.0\n",
      "159 0.0\n",
      "160 0.0\n",
      "161 -0.11428571428571425\n",
      "162 0.0\n",
      "163 0.0\n",
      "164 0.0\n",
      "165 0.3680733944954128\n",
      "166 0.0\n",
      "167 0.0\n",
      "168 1.0\n",
      "169 1.0\n",
      "170 0.47658728814516227\n",
      "171 -0.040695652173913\n",
      "172 0.0\n",
      "173 0.07933194154488513\n",
      "174 0.0\n",
      "175 0.23529411764705888\n",
      "176 1.0\n",
      "177 0.0\n",
      "178 0.07236544549977379\n",
      "179 0.0\n",
      "180 0.0\n",
      "181 -0.0985915492957745\n",
      "182 0.06235846721329005\n",
      "183 0.0\n",
      "184 0.6487889273356401\n",
      "185 0.0\n",
      "186 1.0\n",
      "187 0.4342730635086579\n",
      "188 0.0990432166952739\n",
      "189 1.0\n",
      "190 0.0\n",
      "191 0.7777113067259208\n",
      "192 0.0\n",
      "193 0.0\n",
      "194 0.21226831421006173\n",
      "195 0.0\n",
      "196 0.4514925373134328\n",
      "197 1.0\n",
      "198 1.0\n",
      "199 0.0\n",
      "200 0.23163738964230451\n",
      "201 1.0\n",
      "202 0.5724933965336324\n",
      "203 0.863398553337405\n",
      "204 0.0\n",
      "205 -0.04516129032258055\n",
      "206 0.18738016384870146\n",
      "207 0.0\n",
      "208 0.0\n",
      "209 1.0\n",
      "210 0.10381831794359864\n",
      "211 0.9130237482982907\n",
      "212 0.4347826086956521\n",
      "213 0.0\n",
      "214 0.147366782141606\n",
      "215 1.0\n",
      "216 0.021966527196652794\n",
      "217 1.0\n",
      "218 0.8928702342266649\n",
      "219 -0.01583113456464384\n",
      "220 0.0\n",
      "221 0.40785907859078596\n",
      "222 0.22808898418654522\n",
      "223 0.29569141506393043\n",
      "224 1.0\n",
      "225 0.0\n",
      "226 0.0\n",
      "227 0.0\n",
      "228 0.05163743254562617\n",
      "229 0.0\n",
      "230 0.5838503711391376\n",
      "231 0.4796163069544364\n",
      "232 0.9788444040036397\n",
      "233 0.0\n",
      "234 0.24073028752866465\n",
      "235 0.0\n",
      "236 0.35\n",
      "237 1.0\n",
      "238 0.6322580645161291\n",
      "239 1.0\n",
      "240 0.0\n",
      "241 0.17312346465289774\n",
      "242 1.0\n",
      "243 0.0\n",
      "244 -0.057901508948415066\n",
      "245 1.0\n",
      "246 0.0\n",
      "247 -0.06038368801761189\n",
      "248 0.8384852400357817\n",
      "249 0.5513342095929565\n",
      "250 0.0\n",
      "251 0.4427645788336933\n",
      "252 0.3240053747736169\n",
      "253 0.005098099799165678\n",
      "254 0.10098666393406619\n",
      "255 1.0\n",
      "256 0.0\n",
      "257 1.0\n",
      "258 1.0\n",
      "259 0.4390279923321627\n",
      "260 1.0\n",
      "261 1.0\n",
      "262 0.0\n",
      "263 0.0\n",
      "264 0.0\n",
      "265 0.0\n",
      "266 0.5362903225806451\n",
      "267 0.20891875381795966\n",
      "268 1.0\n",
      "269 0.0772469635627531\n",
      "270 0.0\n",
      "271 0.4704264099037139\n",
      "272 1.0\n",
      "273 0.0\n",
      "274 0.16857012334399268\n",
      "275 0.0\n",
      "276 0.2877697841726619\n",
      "277 -0.11378259397223157\n",
      "278 0.0\n",
      "279 0.0\n",
      "280 0.5264557530962845\n",
      "281 1.0\n",
      "282 1.0\n",
      "283 0.0\n",
      "284 0.6912785080656966\n",
      "285 0.0\n",
      "286 0.0\n",
      "287 0.5854956753160345\n",
      "288 0.0\n",
      "289 1.0\n",
      "290 0.0\n",
      "291 0.0\n",
      "292 1.0\n",
      "293 0.0\n",
      "294 0.0\n",
      "295 -0.14084507042253516\n",
      "296 0.0\n",
      "297 1.0\n",
      "298 1.0\n",
      "299 1.0\n",
      "300 1.0\n",
      "301 1.0\n",
      "302 0.0\n",
      "303 0.0\n",
      "304 0.0\n",
      "305 0.8748860825413358\n",
      "306 0.0\n",
      "307 0.22161046111493457\n",
      "308 0.0\n",
      "309 -0.11897679952409264\n",
      "310 0.2432432432432432\n",
      "311 0.0\n",
      "312 0.23927178153446038\n",
      "313 0.5019041892162758\n",
      "314 0.6046496094335297\n",
      "315 1.0\n",
      "316 0.0\n",
      "317 0.0\n",
      "318 1.0\n",
      "319 0.0\n",
      "320 0.0\n",
      "321 0.6311123750212262\n",
      "322 0.0\n",
      "323 0.7485314685314685\n",
      "324 1.0\n",
      "325 0.7219801219648125\n",
      "326 0.6471844199538465\n",
      "327 0.027490469220004126\n",
      "328 0.12124095926912833\n",
      "329 0.0\n",
      "330 1.0\n",
      "331 0.24753843496286057\n",
      "332 0.4378417587322843\n",
      "333 1.0\n",
      "334 -0.025240783792759788\n",
      "335 0.6472376762841248\n",
      "336 0.0\n",
      "337 1.0\n",
      "338 0.0\n",
      "339 0.0\n",
      "340 0.0\n",
      "341 0.0\n",
      "342 0.0\n",
      "343 0.0\n",
      "344 0.0\n",
      "345 0.0\n",
      "346 0.0\n",
      "347 0.22768714011516322\n",
      "348 1.0\n",
      "349 0.031020846930548594\n",
      "350 1.0\n",
      "351 0.0\n",
      "352 1.0\n",
      "353 0.0\n",
      "354 0.33182076006806577\n",
      "355 0.9130042531254028\n",
      "356 0.0\n",
      "357 0.04957185927671019\n",
      "358 0.4543514116619773\n",
      "359 0.0\n",
      "360 0.0\n",
      "361 0.2575476164463382\n",
      "362 0.3751820565103408\n",
      "363 0.0\n",
      "364 0.07778377961262439\n",
      "365 1.0\n",
      "366 0.43921568627450985\n",
      "367 0.0\n",
      "368 1.0\n",
      "369 0.0\n",
      "370 1.0\n",
      "371 0.0\n",
      "372 0.021399488684783606\n",
      "373 0.08904710571836849\n",
      "374 -0.0731735164644642\n",
      "375 0.34333821376281104\n",
      "376 0.5311420013823823\n",
      "377 0.0\n",
      "378 0.11854103343465042\n",
      "379 0.2\n",
      "380 1.0\n",
      "381 1.0\n",
      "382 1.0\n",
      "383 0.0\n",
      "384 1.0\n",
      "385 0.08631319358816272\n",
      "386 0.0\n",
      "387 0.48196070377216405\n",
      "388 0.0\n",
      "389 1.0\n",
      "390 0.0\n",
      "391 0.3553839606007174\n",
      "392 0.06266218279669897\n",
      "393 0.0\n",
      "394 0.0\n",
      "395 -0.015894039735099362\n",
      "396 0.5041196216051266\n",
      "397 1.0\n",
      "398 0.08147266660123657\n",
      "399 0.41316025067144135\n",
      "400 -0.08870967741935497\n",
      "401 0.3065465881537928\n",
      "402 1.0\n",
      "403 0.0\n",
      "404 0.17085427135678397\n",
      "405 0.0\n",
      "406 0.6249578541190575\n",
      "407 0.4959278880366885\n",
      "408 0.0\n",
      "409 1.0\n",
      "410 0.0\n",
      "411 0.0\n",
      "412 0.0\n",
      "413 0.7138873396542108\n",
      "414 0.0\n",
      "415 -0.03874528355014923\n",
      "416 -0.006059458435902256\n",
      "417 0.0\n",
      "418 0.0\n",
      "419 1.0\n",
      "420 -0.013628620102214602\n",
      "421 0.36211104331909705\n",
      "422 1.0\n",
      "423 0.0\n",
      "424 1.0\n",
      "425 0.0\n",
      "426 0.3166930333391926\n",
      "427 1.0\n",
      "428 0.9130179006639213\n",
      "429 0.8505154639175257\n",
      "430 0.3989229081480004\n",
      "431 0.0\n",
      "432 0.0\n",
      "433 1.0\n",
      "434 1.0\n",
      "435 0.0\n",
      "436 0.0\n",
      "437 -0.014440195173414144\n",
      "438 1.0\n",
      "439 0.09067357512953357\n",
      "440 -0.04832104832104837\n",
      "441 0.8436488519076839\n",
      "442 0.0\n",
      "443 0.21650638914504017\n",
      "444 0.0\n",
      "445 0.48804851944345345\n",
      "446 0.4613778705636743\n",
      "447 0.6966278281240368\n",
      "448 0.0\n",
      "449 -0.003861003861003847\n",
      "450 0.3343248178617821\n",
      "451 0.0\n",
      "452 0.0\n",
      "453 0.6929133858267716\n",
      "454 0.909094972266793\n",
      "455 0.3721824643396135\n",
      "456 0.0\n",
      "457 0.0\n",
      "458 0.8543018966103844\n",
      "459 1.0\n",
      "460 0.8460736951785546\n",
      "461 1.0\n",
      "462 0.0\n",
      "463 1.0\n",
      "464 0.0\n",
      "465 0.0\n",
      "466 1.0\n",
      "467 0.30626450116009274\n",
      "468 1.0\n",
      "469 0.0\n",
      "470 0.6955503512880562\n",
      "471 0.5220980918564707\n",
      "472 0.0\n",
      "473 1.0\n",
      "474 0.8368625083709831\n",
      "475 0.26547243227369505\n",
      "476 0.0\n",
      "477 -0.005573324044459888\n",
      "478 0.8477274244143362\n",
      "479 0.024206245080031656\n",
      "480 0.0\n",
      "481 0.0\n",
      "482 0.04087193460490472\n",
      "483 0.0\n",
      "484 1.0\n",
      "485 0.0\n",
      "486 0.4780349551251772\n",
      "487 0.0\n",
      "488 0.4163362410785091\n",
      "489 0.0\n",
      "490 1.0\n",
      "491 1.0\n",
      "492 0.0\n",
      "493 -0.1367781155015197\n",
      "494 0.0\n",
      "495 0.0\n",
      "496 1.0\n",
      "497 0.0\n",
      "498 1.0\n",
      "499 1.0\n",
      "500 0.8620856990343088\n",
      "501 -0.04198473282442751\n",
      "502 0.7184466019417476\n",
      "503 0.6994678452241363\n",
      "504 0.0\n",
      "505 0.0\n",
      "506 0.4101694915254237\n",
      "507 0.30626450116009274\n",
      "508 0.28668941979522206\n",
      "509 1.0\n",
      "510 0.0\n",
      "511 0.0\n",
      "512 0.0\n",
      "513 0.0\n",
      "514 0.0\n",
      "515 0.677583228909134\n",
      "516 0.4357885915210698\n",
      "517 0.0006412341433948827\n",
      "518 0.6504845351249264\n",
      "519 0.5906517465151\n",
      "520 1.0\n",
      "521 0.0\n",
      "522 0.0\n",
      "523 1.0\n",
      "524 0.0\n",
      "525 1.0\n",
      "526 0.0\n",
      "527 0.0\n",
      "528 0.17060876308646758\n",
      "529 1.0\n",
      "530 0.126679462571977\n",
      "531 1.0\n",
      "532 1.0\n",
      "533 1.0\n",
      "534 1.0\n",
      "535 0.0\n",
      "536 0.0\n",
      "537 1.0\n",
      "538 1.0\n",
      "539 0.0\n",
      "540 0.6545117428924598\n",
      "541 0.3156905731287191\n",
      "542 0.0\n",
      "543 0.7605268786090807\n",
      "544 0.5881980007054233\n",
      "545 0.3014705882352941\n",
      "546 0.7724924164138804\n",
      "547 0.5958076953603101\n",
      "548 0.0\n",
      "549 0.0\n",
      "550 0.0\n",
      "551 0.5464752438799927\n",
      "552 0.0\n",
      "553 -0.02166653091146038\n",
      "554 0.30061971970362306\n",
      "555 1.0\n",
      "556 0.0\n",
      "557 1.0\n",
      "558 1.0\n",
      "559 0.0\n",
      "560 0.0\n",
      "561 0.10592788755347325\n",
      "562 0.0\n",
      "563 0.18866837302172249\n",
      "564 1.0\n",
      "565 0.0\n",
      "566 0.0\n",
      "567 0.19041506204535733\n",
      "568 1.0\n",
      "569 1.0\n",
      "570 1.0\n",
      "571 1.0\n",
      "572 1.0\n",
      "573 0.6608695652173914\n",
      "574 0.0\n",
      "575 1.0\n",
      "576 0.01577199858319815\n",
      "577 0.0\n",
      "578 0.0\n",
      "579 1.0\n",
      "580 1.0\n",
      "581 0.0\n",
      "582 -0.09978308026030358\n",
      "583 0.0052338265715610125\n",
      "584 1.0\n",
      "585 0.777978082123552\n",
      "586 0.0\n",
      "587 0.5489306545690215\n",
      "588 1.0\n",
      "589 0.6196403872752421\n",
      "590 0.18870354287083346\n",
      "591 0.0\n",
      "592 -0.0581395348837209\n",
      "593 0.7267093426438995\n",
      "594 0.0\n",
      "595 0.37169743032935215\n",
      "596 1.0\n",
      "597 0.0\n",
      "598 1.0\n",
      "599 1.0\n",
      "600 0.0\n",
      "601 1.0\n",
      "602 0.6333333333333333\n",
      "603 0.0\n",
      "604 0.0\n",
      "605 0.0\n",
      "606 1.0\n",
      "607 0.1666191968100256\n",
      "608 0.5618378519778022\n",
      "609 0.2699386503067484\n",
      "610 0.17317356010823354\n",
      "611 0.0\n",
      "612 0.16822429906542055\n",
      "613 0.0\n",
      "614 0.0\n",
      "615 0.19349396633852012\n",
      "616 0.4760008383986586\n",
      "617 1.0\n",
      "618 0.0\n",
      "619 0.022158520507517144\n",
      "620 0.23854305565302825\n",
      "621 0.7701904136572555\n",
      "622 1.0\n",
      "623 0.0\n",
      "624 0.0\n",
      "625 0.08295372812275231\n",
      "626 0.0\n",
      "627 1.0\n",
      "628 0.4188435573818609\n",
      "629 1.0\n",
      "630 0.22768714011516322\n",
      "631 0.62486602357985\n",
      "632 0.09067357512953357\n",
      "633 0.0\n",
      "634 0.0\n",
      "635 0.0\n",
      "636 1.0\n",
      "637 1.0\n",
      "638 0.4413558360516239\n",
      "639 1.0\n",
      "640 0.0\n",
      "641 0.23960495764684525\n",
      "642 0.2507645259938837\n",
      "643 1.0\n",
      "644 1.0\n",
      "645 0.10486874166704985\n",
      "646 0.8816820680781378\n",
      "647 0.6930058729311266\n",
      "648 0.37208619360522743\n",
      "649 1.0\n",
      "650 0.0\n",
      "651 0.11688311688311684\n",
      "652 0.026209677419354885\n",
      "653 1.0\n",
      "654 0.0\n",
      "655 1.0\n",
      "656 0.0\n",
      "657 1.0\n",
      "658 0.0\n",
      "659 1.0\n",
      "660 0.7718456927390506\n",
      "661 -0.021073585941790297\n",
      "662 1.0\n",
      "663 1.0\n",
      "664 0.0\n",
      "665 0.0\n",
      "666 1.0\n",
      "667 0.29994442634574475\n",
      "668 0.0\n",
      "669 0.0\n",
      "670 0.0\n",
      "671 0.3529411764705882\n",
      "672 0.0\n",
      "673 0.012820512820512933\n",
      "674 -0.0023563293624819332\n",
      "675 1.0\n",
      "676 0.6490984743411928\n",
      "677 0.0\n",
      "678 -0.16666666666666657\n",
      "679 0.08496732026143802\n",
      "680 0.0\n",
      "681 -0.057901508948415066\n",
      "682 0.0\n",
      "683 0.0\n",
      "684 0.06778919746337195\n",
      "685 1.0\n",
      "686 0.0\n",
      "687 1.0\n",
      "688 1.0\n",
      "689 0.0\n",
      "690 0.0\n",
      "691 0.013333333333333334\n",
      "692 0.8999644844323429\n",
      "693 0.09037540553066577\n",
      "694 1.0\n",
      "695 1.0\n",
      "696 1.0\n",
      "697 0.0\n",
      "698 0.1904120365817681\n",
      "699 0.0\n",
      "700 0.4106412005457026\n",
      "701 0.49064843824171656\n",
      "702 0.2847008170654142\n",
      "703 1.0\n",
      "704 0.5468924741243723\n",
      "705 0.0\n",
      "706 0.1455696202531647\n",
      "707 0.0\n",
      "708 0.0\n",
      "709 0.5276208411801632\n",
      "710 0.189873417721519\n",
      "711 0.0\n",
      "712 -0.06329113924050643\n",
      "713 0.0\n",
      "714 1.0\n",
      "715 0.17426273458445038\n",
      "716 0.0\n",
      "717 0.0\n",
      "718 0.7776739841195702\n",
      "719 1.0\n",
      "720 0.7332457293035479\n",
      "721 0.5593922651933702\n",
      "722 1.0\n",
      "723 0.0\n",
      "724 0.25909878682842286\n",
      "725 -0.05180684362008313\n",
      "726 0.47856193437279676\n",
      "727 0.0\n",
      "728 0.4554973821989529\n",
      "729 0.5338634494104744\n",
      "730 0.5821099351314442\n",
      "731 0.1579254079254079\n",
      "732 1.0\n",
      "733 1.0\n",
      "734 0.0\n",
      "735 0.0\n",
      "736 0.397480755773268\n",
      "737 1.0\n",
      "738 0.38852633176506307\n",
      "739 0.4780349551251772\n",
      "740 0.7084313996954175\n",
      "741 1.0\n",
      "742 0.37324982349692776\n",
      "743 1.0\n",
      "744 0.6578947368421053\n",
      "745 0.0\n",
      "746 0.22989377845220027\n",
      "747 0.0\n",
      "748 1.0\n",
      "749 0.07043964945872305\n",
      "750 0.0\n",
      "751 1.0\n",
      "752 0.0\n",
      "753 0.0\n",
      "754 0.0\n",
      "755 0.0\n",
      "756 1.0\n",
      "757 0.0\n",
      "758 0.7798482950534721\n",
      "759 0.23333333333333334\n",
      "760 1.0\n",
      "761 0.0\n",
      "762 0.0\n",
      "763 0.0\n",
      "764 0.7400782560089435\n",
      "765 0.0\n",
      "766 0.0\n",
      "767 0.0\n",
      "768 0.3970404943117028\n",
      "769 1.0\n",
      "770 0.0\n",
      "771 1.0\n",
      "772 -0.11897679952409264\n",
      "773 1.0\n",
      "774 1.0\n",
      "775 1.0\n",
      "776 0.0\n",
      "777 1.0\n",
      "778 0.6684555382215288\n",
      "779 0.0\n",
      "780 0.5557491289198606\n",
      "781 0.0\n",
      "782 1.0\n",
      "783 0.0\n",
      "784 0.0\n",
      "785 1.0\n",
      "786 0.19821745442813357\n",
      "787 0.026422400668513712\n",
      "788 1.0\n",
      "789 0.3078661316086804\n",
      "790 0.0\n",
      "791 0.0\n",
      "792 1.0\n",
      "793 0.17426273458445038\n",
      "794 0.0\n",
      "795 0.0\n",
      "796 0.0\n",
      "797 0.0\n",
      "798 1.0\n",
      "799 0.23553894504374417\n",
      "800 -0.16666666666666657\n",
      "801 0.567656273310458\n",
      "802 -0.0985915492957745\n",
      "803 1.0\n",
      "804 0.0\n",
      "805 0.0\n",
      "806 0.686197149484267\n",
      "807 0.0\n",
      "808 0.0\n",
      "809 0.104136048708797\n",
      "810 1.0\n",
      "811 0.2000794416387675\n",
      "812 1.0\n",
      "813 0.0\n",
      "814 0.0\n",
      "815 0.6774193548387097\n",
      "816 0.71782111994007\n",
      "817 1.0\n",
      "818 0.6817823964729963\n",
      "819 1.0\n",
      "820 0.41363636363636364\n",
      "821 1.0\n",
      "822 0.10830324909747292\n",
      "823 0.0\n",
      "824 0.0\n",
      "825 0.37733951836853336\n",
      "826 0.0\n",
      "827 0.7516484749852689\n",
      "828 1.0\n",
      "829 1.0\n",
      "830 0.18685985510866857\n",
      "831 0.6704753143949939\n",
      "832 0.0\n",
      "833 1.0\n",
      "834 1.0\n",
      "835 0.8918597098939164\n",
      "836 1.0\n",
      "837 0.5162811929397443\n",
      "838 0.46975315847602966\n",
      "839 0.5281424985518439\n",
      "840 0.0\n",
      "841 0.0\n",
      "842 0.0\n",
      "843 0.29280397022332505\n",
      "844 1.0\n",
      "845 0.5241228070175439\n",
      "846 0.0\n",
      "847 0.0\n",
      "848 0.0\n",
      "849 0.5557491289198606\n",
      "850 0.0\n",
      "851 0.10384504985710548\n",
      "852 0.22190201729106626\n",
      "853 0.8668570902054732\n",
      "854 -0.08399040109701746\n",
      "855 1.0\n",
      "856 0.776300010901559\n",
      "857 0.0\n",
      "858 0.21052631578947364\n",
      "859 0.25763062596999475\n",
      "860 1.0\n",
      "861 1.0\n",
      "862 0.0\n",
      "863 0.0\n",
      "864 1.0\n",
      "865 0.2699386503067484\n",
      "866 0.0\n",
      "867 0.20481927710843387\n",
      "868 1.0\n",
      "869 1.0\n",
      "870 0.0\n",
      "871 0.4005449591280655\n",
      "872 0.5009540313370685\n",
      "873 1.0\n",
      "874 0.7281896116994454\n",
      "875 0.07644731710035105\n",
      "876 1.0\n",
      "877 0.0\n",
      "878 0.5083219283916824\n",
      "879 0.2981345769487008\n",
      "880 1.0\n",
      "881 1.0\n",
      "882 0.5154386356036116\n",
      "883 0.7920792079207921\n",
      "884 1.0\n",
      "885 0.0\n",
      "886 0.0\n",
      "887 0.0\n",
      "888 1.0\n",
      "889 0.6938775510204082\n",
      "890 0.0\n",
      "891 0.7517517559847258\n",
      "892 0.0\n",
      "893 0.0\n",
      "894 1.0\n",
      "895 0.23826661225095536\n",
      "896 0.0\n",
      "897 1.0\n",
      "898 1.0\n",
      "899 0.34333821376281104\n",
      "900 0.4160039153309678\n",
      "901 0.3414245373057964\n",
      "902 -0.005830903790087443\n",
      "903 1.0\n",
      "904 0.0\n",
      "905 1.0\n",
      "906 0.03433476394849796\n",
      "907 0.32346862763823897\n",
      "908 0.3156905731287191\n",
      "909 1.0\n",
      "910 0.6033300685602351\n",
      "911 -0.004091025313219111\n",
      "912 1.0\n",
      "913 1.0\n",
      "914 0.0\n",
      "915 0.0\n",
      "916 0.611430625449317\n",
      "917 1.0\n",
      "918 -0.010794140323824204\n",
      "919 1.0\n",
      "920 0.0\n",
      "921 0.2709395484340859\n",
      "922 0.0\n",
      "923 1.0\n",
      "924 -0.11740473738414002\n",
      "925 0.0\n",
      "926 0.5909016258827393\n",
      "927 1.0\n",
      "928 0.0\n",
      "929 0.0\n",
      "930 1.0\n",
      "931 0.3164556962025316\n",
      "932 0.0\n",
      "933 0.676056338028169\n",
      "934 0.0\n",
      "935 0.0\n",
      "936 0.22956710073460315\n",
      "937 1.0\n",
      "938 0.0\n",
      "939 0.24379811804961507\n",
      "940 0.0\n",
      "941 1.0\n",
      "942 0.0\n",
      "943 0.4067796610169492\n",
      "944 1.0\n",
      "945 0.5626417782449928\n",
      "946 0.8519141775347077\n",
      "947 1.0\n",
      "948 0.0\n",
      "949 0.256880733944954\n",
      "950 0.0\n",
      "951 0.0\n",
      "952 0.34374999999999994\n",
      "953 0.0\n",
      "954 0.1774874371859296\n",
      "955 0.18103448275862077\n",
      "956 1.0\n",
      "957 0.6119631901840492\n",
      "958 0.19630484988452662\n",
      "959 0.6520534483464588\n",
      "960 1.0\n",
      "961 0.2831858407079646\n",
      "962 0.0\n",
      "963 0.0\n",
      "964 0.1684730121257645\n",
      "965 1.0\n",
      "966 1.0\n",
      "967 0.6049731192071718\n",
      "968 0.0\n",
      "969 -0.10344827586206896\n",
      "970 -0.0398834864440959\n",
      "971 0.13024850042844907\n",
      "972 0.0\n",
      "973 -0.1260669730794483\n",
      "974 0.031585965497124745\n",
      "975 0.5247359644246804\n",
      "976 1.0\n",
      "977 0.43301955104996415\n",
      "978 0.0\n",
      "979 0.0\n",
      "980 -0.05100019173004401\n",
      "981 0.0\n",
      "982 0.012763241863433217\n",
      "983 0.0\n",
      "984 1.0\n",
      "985 0.0\n",
      "986 0.7473967415674356\n",
      "987 0.0\n",
      "988 0.6855875384382719\n",
      "989 0.0\n",
      "990 0.725242433147223\n",
      "991 0.26113533952668017\n",
      "992 0.11854103343465042\n",
      "993 0.7105513307984791\n",
      "994 0.4565154579601271\n",
      "995 1.0\n",
      "996 0.20815498986258169\n",
      "997 -0.09979633401221984\n",
      "998 0.0\n",
      "999 0.5822578689348138\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.eval()\n",
    "for i, (inputs, labels) in enumerate(dataloaders['train']):\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    a_arr, d_arr = np.nonzero(np.max(reverse_transform(inputs[0].cpu()), axis = 2))\n",
    "    \n",
    "    p = torch.nn.functional.softmax(outputs[0], dim=1)\n",
    "    p_arr = p.cpu().data.numpy()\n",
    "    \n",
    "    pred_labels = np.argmax(p_arr, axis = 0)[a_arr, d_arr]\n",
    "    target_labels = np.argmax(labels[0].cpu().data.numpy(), axis = 0)[a_arr, d_arr]\n",
    "    \n",
    "    ari = metrics.adjusted_rand_score(pred_labels, target_labels)\n",
    "      \n",
    "    plot_preds_and_targets(d_arr, a_arr, pred_labels, target_labels, ari, i)\n",
    "\n",
    "    \n",
    "    print(i, ari)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_preds_and_targets(inp_d_arr, inp_a_arr, inp_pred_arr, inp_targets_arr, inp_ari, i):\n",
    "    colour_list = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'tab:brown', 'tab:pink', 'tab:gray', 'tab:olive', 'tab:cyan']\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(10,16), sharex=True)\n",
    "    \n",
    "    for p in inp_pred_arr:\n",
    "        mask = (inp_pred_arr == p)\n",
    "        ax1.scatter(inp_d_arr[mask], inp_a_arr[mask], s=10, c=colour_list[p%10], marker='x')\n",
    "\n",
    "    for t in inp_targets_arr:\n",
    "        mask = (inp_targets_arr == t)\n",
    "        ax2.scatter(inp_d_arr[mask], inp_a_arr[mask], s=10, c=colour_list[t%10], marker='x')\n",
    "        \n",
    "    for ax in fig.axes:\n",
    "        matplotlib.pyplot.sca(ax)\n",
    "        plt.xticks(rotation=90)\n",
    "        #plt.legend(bbox_to_anchor=(1.02, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "    plt.savefig('/home/a/Documents/GNN-for-trans-grouping/GNN-for-trans-grouping/Charts/'+str(ari)+'_'+str(i)+'.png')\n",
    "        \n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 4, 224, 224)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloaders['train'].dataset.target_masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = dataloaders['train'].dataset.target_masks[999,:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(*args, **kw)>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = 3\n",
    "a_arr, d_arr = np.nonzero(np.max(reverse_transform(inputs[idx].cpu()), axis = 2))\n",
    "# t_arr = (p_arr[idx, :, a_arr, d_arr][:,0]<0.5).astype(int)\n",
    "\n",
    "t_arr = np.argmax(target, axis = 0)[a_arr, d_arr]\n",
    "\n",
    "colour_list = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'tab:brown', 'tab:pink', 'tab:gray', 'tab:olive', 'tab:cyan']\n",
    "\n",
    "\n",
    "fig, (ax1) = plt.subplots(nrows=1, ncols=1, figsize=(10,8), sharex=True)\n",
    "for t in t_arr:\n",
    "    mask = (t_arr == t)\n",
    "    ax1.scatter(d_arr[mask], a_arr[mask], s=10, c=colour_list[t%10], marker='x')\n",
    "    \n",
    "for ax1 in fig.axes:\n",
    "    matplotlib.pyplot.sca(ax1)\n",
    "    plt.xticks(rotation=90)\n",
    "    #plt.legend(bbox_to_anchor=(1.02, 1), loc=2, borderaxespad=0.)\n",
    "    \n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 224, 224])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = torch.nn.functional.softmax(outputs, dim=1)\n",
    "p_arr = p.cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(*args, **kw)>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = 3\n",
    "a_arr, d_arr = np.nonzero(np.max(reverse_transform(inputs[idx].cpu()), axis = 2))\n",
    "# t_arr = (p_arr[idx, :, a_arr, d_arr][:,0]<0.5).astype(int)\n",
    "\n",
    "t_arr = np.argmax(p_arr[0,:,:,:], axis = 0)[a_arr, d_arr]\n",
    "\n",
    "colour_list = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'tab:brown', 'tab:pink', 'tab:gray', 'tab:olive', 'tab:cyan']\n",
    "\n",
    "\n",
    "fig, (ax1) = plt.subplots(nrows=1, ncols=1, figsize=(10,8), sharex=True)\n",
    "for t in t_arr:\n",
    "    mask = (t_arr == t)\n",
    "    ax1.scatter(d_arr[mask], a_arr[mask], s=10, c=colour_list[t%10], marker='x')\n",
    "    \n",
    "for ax1 in fig.axes:\n",
    "    matplotlib.pyplot.sca(ax1)\n",
    "    plt.xticks(rotation=90)\n",
    "    #plt.legend(bbox_to_anchor=(1.02, 1), loc=2, borderaxespad=0.)\n",
    "    \n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 224, 224)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_arr[0,:,:,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.1298797e-37"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(p_arr[0,2,a_arr, d_arr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(p_arr[0,:,:,:], axis = 0)[a_arr, d_arr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
