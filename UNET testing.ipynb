{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Unet CNN Pixel classification model for grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import timeit\n",
    "import statistics\n",
    "import time\n",
    "import torch\n",
    "import torch_geometric\n",
    "import importlib\n",
    "\n",
    "from data_utils import synthetic_data\n",
    "from data_utils import graph_constructors\n",
    "from data_utils import group_to_image_constructors\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import importlib\n",
    "\n",
    "from Unet import helper\n",
    "from Unet import simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'data_utils.group_to_image_constructors' from '/home/a/Documents/GNN-for-trans-grouping/GNN-for-trans-grouping/data_utils/group_to_image_constructors.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(synthetic_data)\n",
    "importlib.reload(graph_constructors)\n",
    "importlib.reload(group_to_image_constructors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colour_list = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'tab:brown', 'tab:pink', 'tab:gray', 'tab:olive', 'tab:cyan']\n",
    "\n",
    "for i in range(2):\n",
    "    d_lst, a_lst, g_lst = synthetic_data.make_a_group()\n",
    "    \n",
    "    d_arr = np.array(d_lst)\n",
    "    a_arr = np.array(a_lst)\n",
    "    g_arr = np.array(g_lst)    \n",
    "    \n",
    "    fig, (ax1) = plt.subplots(nrows=1, ncols=1, figsize=(10,8), sharex=True)\n",
    "    for g in g_lst:\n",
    "        mask = (g_arr == g)\n",
    "        \n",
    "        ax1.scatter(d_arr[mask], a_arr[mask], s=10, c=colour_list[g%10], marker='x')\n",
    "        ax1.set_title(str(i))\n",
    "        #ax1.legend(loc=\"upper right\")\n",
    "    \n",
    "    for ax1 in fig.axes:\n",
    "        matplotlib.pyplot.sca(ax1)\n",
    "        plt.xticks(rotation=90)\n",
    "        #plt.legend(bbox_to_anchor=(1.02, 1), loc=2, borderaxespad=0.)\n",
    "    \n",
    "    plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_lst, a_lst, g_lst = synthetic_data.make_a_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_arr = np.array(d_lst)\n",
    "a_arr = np.array(a_lst)\n",
    "g_arr = np.array(g_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_a_arr = graph_constructors.normalise_amounts(a_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unet demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 192, 192, 3)\n",
      "0 255\n",
      "(3, 6, 192, 192)\n",
      "0.0 1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x864 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Generate some random images\n",
    "input_images, target_masks = simulation.generate_random_data(192, 192, count=3)\n",
    "\n",
    "for x in [input_images, target_masks]:\n",
    "    print(x.shape)\n",
    "    print(x.min(), x.max())\n",
    "\n",
    "# Change channel-order and make 3 channels for matplot\n",
    "input_images_rgb = [x.astype(np.uint8) for x in input_images]\n",
    "\n",
    "# Map each channel (i.e. class) to each color\n",
    "target_masks_rgb = [helper.masks_to_colorimg(x) for x in target_masks]\n",
    "\n",
    "# Left: Input image (black and white), Right: Target mask (6ch)\n",
    "helper.plot_side_by_side([input_images_rgb, target_masks_rgb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 192, 192, 3)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 6, 192, 192)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
       "       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
       "       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
       "       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
       "       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
       "       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
       "       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
       "       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
       "       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
       "       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
       "       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
       "       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
       "       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
       "       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
       "       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
       "       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
       "       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
       "       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
       "       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
       "       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
       "       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
       "       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
       "       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
       "       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
       "       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
       "       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
       "       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
       "       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
       "       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
       "       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
       "       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
       "       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
       "       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
       "       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
       "       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
       "       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
       "       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
       "       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
       "       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
       "       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
       "       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
       "       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
       "       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
       "       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
       "       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
       "       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
       "       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
       "       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
       "       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
       "       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
       "       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
       "       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
       "       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
       "       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
       "       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
       "       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
       "       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
       "       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
       "       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
       "       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
       "       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
       "       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
       "       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
       "       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
       "       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
       "       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255],\n",
       "      dtype=uint8)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_images[0,:,:,2][input_images[0,:,:,2] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_masks[0,3,:,:][input_images[0,:,:,2] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218535"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum((input_images[0,:,:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets, models\n",
    "\n",
    "class SimDataset(Dataset):\n",
    "    def __init__(self, count, transform=None):\n",
    "        self.input_images, self.target_masks = simulation.generate_random_data(192, 192, count=count)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.input_images[idx]\n",
    "        mask = self.target_masks[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return [image, mask]\n",
    "\n",
    "# use the same transformations for train/val in this example\n",
    "trans = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # imagenet\n",
    "])\n",
    "\n",
    "train_set = SimDataset(2000, transform = trans)\n",
    "val_set = SimDataset(200, transform = trans)\n",
    "\n",
    "image_datasets = {\n",
    "    'train': train_set, 'val': val_set\n",
    "}\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "dataloaders = {\n",
    "    'train': DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0),\n",
    "    'val': DataLoader(val_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': <torch.utils.data.dataloader.DataLoader at 0x1b9f57b33c8>,\n",
       " 'val': <torch.utils.data.dataloader.DataLoader at 0x1b9f57f7608>}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 192, 192]) torch.Size([4, 6, 192, 192])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1b9f5533c48>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torchvision.utils\n",
    "\n",
    "def reverse_transform(inp):\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    inp = (inp * 255).astype(np.uint8)\n",
    "\n",
    "    return inp\n",
    "\n",
    "# Get a batch of training data\n",
    "inputs, masks = next(iter(dataloaders['train']))\n",
    "\n",
    "print(inputs.shape, masks.shape)\n",
    "\n",
    "plt.imshow(reverse_transform(inputs[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "def convrelu(in_channels, out_channels, kernel, padding):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel, padding=padding),\n",
    "        nn.ReLU(inplace=True),\n",
    "    )\n",
    "\n",
    "\n",
    "class ResNetUNet(nn.Module):\n",
    "    def __init__(self, n_class):\n",
    "        super().__init__()\n",
    "\n",
    "        self.base_model = models.resnet18(pretrained=True)\n",
    "        self.base_layers = list(self.base_model.children())\n",
    "\n",
    "        self.layer0 = nn.Sequential(*self.base_layers[:3]) # size=(N, 64, x.H/2, x.W/2)\n",
    "        self.layer0_1x1 = convrelu(64, 64, 1, 0)\n",
    "        self.layer1 = nn.Sequential(*self.base_layers[3:5]) # size=(N, 64, x.H/4, x.W/4)\n",
    "        self.layer1_1x1 = convrelu(64, 64, 1, 0)\n",
    "        self.layer2 = self.base_layers[5]  # size=(N, 128, x.H/8, x.W/8)\n",
    "        self.layer2_1x1 = convrelu(128, 128, 1, 0)\n",
    "        self.layer3 = self.base_layers[6]  # size=(N, 256, x.H/16, x.W/16)\n",
    "        self.layer3_1x1 = convrelu(256, 256, 1, 0)\n",
    "        self.layer4 = self.base_layers[7]  # size=(N, 512, x.H/32, x.W/32)\n",
    "        self.layer4_1x1 = convrelu(512, 512, 1, 0)\n",
    "\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "        self.conv_up3 = convrelu(256 + 512, 512, 3, 1)\n",
    "        self.conv_up2 = convrelu(128 + 512, 256, 3, 1)\n",
    "        self.conv_up1 = convrelu(64 + 256, 256, 3, 1)\n",
    "        self.conv_up0 = convrelu(64 + 256, 128, 3, 1)\n",
    "\n",
    "        self.conv_original_size0 = convrelu(3, 64, 3, 1)\n",
    "        self.conv_original_size1 = convrelu(64, 64, 3, 1)\n",
    "        self.conv_original_size2 = convrelu(64 + 128, 64, 3, 1)\n",
    "\n",
    "        self.conv_last = nn.Conv2d(64, n_class, 1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x_original = self.conv_original_size0(input)\n",
    "        x_original = self.conv_original_size1(x_original)\n",
    "\n",
    "        layer0 = self.layer0(input)\n",
    "        layer1 = self.layer1(layer0)\n",
    "        layer2 = self.layer2(layer1)\n",
    "        layer3 = self.layer3(layer2)\n",
    "        layer4 = self.layer4(layer3)\n",
    "\n",
    "        layer4 = self.layer4_1x1(layer4)\n",
    "        x = self.upsample(layer4)\n",
    "        layer3 = self.layer3_1x1(layer3)\n",
    "        x = torch.cat([x, layer3], dim=1)\n",
    "        x = self.conv_up3(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer2 = self.layer2_1x1(layer2)\n",
    "        x = torch.cat([x, layer2], dim=1)\n",
    "        x = self.conv_up2(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer1 = self.layer1_1x1(layer1)\n",
    "        x = torch.cat([x, layer1], dim=1)\n",
    "        x = self.conv_up1(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer0 = self.layer0_1x1(layer0)\n",
    "        x = torch.cat([x, layer0], dim=1)\n",
    "        x = self.conv_up0(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, x_original], dim=1)\n",
    "        x = self.conv_original_size2(x)\n",
    "\n",
    "        out = self.conv_last(x)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 512, 512]           1,792\n",
      "              ReLU-2         [-1, 64, 512, 512]               0\n",
      "            Conv2d-3         [-1, 64, 512, 512]          36,928\n",
      "              ReLU-4         [-1, 64, 512, 512]               0\n",
      "            Conv2d-5         [-1, 64, 256, 256]           9,408\n",
      "            Conv2d-6         [-1, 64, 256, 256]           9,408\n",
      "       BatchNorm2d-7         [-1, 64, 256, 256]             128\n",
      "       BatchNorm2d-8         [-1, 64, 256, 256]             128\n",
      "              ReLU-9         [-1, 64, 256, 256]               0\n",
      "             ReLU-10         [-1, 64, 256, 256]               0\n",
      "        MaxPool2d-11         [-1, 64, 128, 128]               0\n",
      "        MaxPool2d-12         [-1, 64, 128, 128]               0\n",
      "           Conv2d-13         [-1, 64, 128, 128]          36,864\n",
      "           Conv2d-14         [-1, 64, 128, 128]          36,864\n",
      "      BatchNorm2d-15         [-1, 64, 128, 128]             128\n",
      "      BatchNorm2d-16         [-1, 64, 128, 128]             128\n",
      "             ReLU-17         [-1, 64, 128, 128]               0\n",
      "             ReLU-18         [-1, 64, 128, 128]               0\n",
      "           Conv2d-19         [-1, 64, 128, 128]          36,864\n",
      "           Conv2d-20         [-1, 64, 128, 128]          36,864\n",
      "      BatchNorm2d-21         [-1, 64, 128, 128]             128\n",
      "      BatchNorm2d-22         [-1, 64, 128, 128]             128\n",
      "             ReLU-23         [-1, 64, 128, 128]               0\n",
      "             ReLU-24         [-1, 64, 128, 128]               0\n",
      "       BasicBlock-25         [-1, 64, 128, 128]               0\n",
      "       BasicBlock-26         [-1, 64, 128, 128]               0\n",
      "           Conv2d-27         [-1, 64, 128, 128]          36,864\n",
      "           Conv2d-28         [-1, 64, 128, 128]          36,864\n",
      "      BatchNorm2d-29         [-1, 64, 128, 128]             128\n",
      "      BatchNorm2d-30         [-1, 64, 128, 128]             128\n",
      "             ReLU-31         [-1, 64, 128, 128]               0\n",
      "             ReLU-32         [-1, 64, 128, 128]               0\n",
      "           Conv2d-33         [-1, 64, 128, 128]          36,864\n",
      "           Conv2d-34         [-1, 64, 128, 128]          36,864\n",
      "      BatchNorm2d-35         [-1, 64, 128, 128]             128\n",
      "      BatchNorm2d-36         [-1, 64, 128, 128]             128\n",
      "             ReLU-37         [-1, 64, 128, 128]               0\n",
      "             ReLU-38         [-1, 64, 128, 128]               0\n",
      "       BasicBlock-39         [-1, 64, 128, 128]               0\n",
      "       BasicBlock-40         [-1, 64, 128, 128]               0\n",
      "           Conv2d-41          [-1, 128, 64, 64]          73,728\n",
      "           Conv2d-42          [-1, 128, 64, 64]          73,728\n",
      "      BatchNorm2d-43          [-1, 128, 64, 64]             256\n",
      "      BatchNorm2d-44          [-1, 128, 64, 64]             256\n",
      "             ReLU-45          [-1, 128, 64, 64]               0\n",
      "             ReLU-46          [-1, 128, 64, 64]               0\n",
      "           Conv2d-47          [-1, 128, 64, 64]         147,456\n",
      "           Conv2d-48          [-1, 128, 64, 64]         147,456\n",
      "      BatchNorm2d-49          [-1, 128, 64, 64]             256\n",
      "      BatchNorm2d-50          [-1, 128, 64, 64]             256\n",
      "           Conv2d-51          [-1, 128, 64, 64]           8,192\n",
      "           Conv2d-52          [-1, 128, 64, 64]           8,192\n",
      "      BatchNorm2d-53          [-1, 128, 64, 64]             256\n",
      "      BatchNorm2d-54          [-1, 128, 64, 64]             256\n",
      "             ReLU-55          [-1, 128, 64, 64]               0\n",
      "             ReLU-56          [-1, 128, 64, 64]               0\n",
      "       BasicBlock-57          [-1, 128, 64, 64]               0\n",
      "       BasicBlock-58          [-1, 128, 64, 64]               0\n",
      "           Conv2d-59          [-1, 128, 64, 64]         147,456\n",
      "           Conv2d-60          [-1, 128, 64, 64]         147,456\n",
      "      BatchNorm2d-61          [-1, 128, 64, 64]             256\n",
      "      BatchNorm2d-62          [-1, 128, 64, 64]             256\n",
      "             ReLU-63          [-1, 128, 64, 64]               0\n",
      "             ReLU-64          [-1, 128, 64, 64]               0\n",
      "           Conv2d-65          [-1, 128, 64, 64]         147,456\n",
      "           Conv2d-66          [-1, 128, 64, 64]         147,456\n",
      "      BatchNorm2d-67          [-1, 128, 64, 64]             256\n",
      "      BatchNorm2d-68          [-1, 128, 64, 64]             256\n",
      "             ReLU-69          [-1, 128, 64, 64]               0\n",
      "             ReLU-70          [-1, 128, 64, 64]               0\n",
      "       BasicBlock-71          [-1, 128, 64, 64]               0\n",
      "       BasicBlock-72          [-1, 128, 64, 64]               0\n",
      "           Conv2d-73          [-1, 256, 32, 32]         294,912\n",
      "           Conv2d-74          [-1, 256, 32, 32]         294,912\n",
      "      BatchNorm2d-75          [-1, 256, 32, 32]             512\n",
      "      BatchNorm2d-76          [-1, 256, 32, 32]             512\n",
      "             ReLU-77          [-1, 256, 32, 32]               0\n",
      "             ReLU-78          [-1, 256, 32, 32]               0\n",
      "           Conv2d-79          [-1, 256, 32, 32]         589,824\n",
      "           Conv2d-80          [-1, 256, 32, 32]         589,824\n",
      "      BatchNorm2d-81          [-1, 256, 32, 32]             512\n",
      "      BatchNorm2d-82          [-1, 256, 32, 32]             512\n",
      "           Conv2d-83          [-1, 256, 32, 32]          32,768\n",
      "           Conv2d-84          [-1, 256, 32, 32]          32,768\n",
      "      BatchNorm2d-85          [-1, 256, 32, 32]             512\n",
      "      BatchNorm2d-86          [-1, 256, 32, 32]             512\n",
      "             ReLU-87          [-1, 256, 32, 32]               0\n",
      "             ReLU-88          [-1, 256, 32, 32]               0\n",
      "       BasicBlock-89          [-1, 256, 32, 32]               0\n",
      "       BasicBlock-90          [-1, 256, 32, 32]               0\n",
      "           Conv2d-91          [-1, 256, 32, 32]         589,824\n",
      "           Conv2d-92          [-1, 256, 32, 32]         589,824\n",
      "      BatchNorm2d-93          [-1, 256, 32, 32]             512\n",
      "      BatchNorm2d-94          [-1, 256, 32, 32]             512\n",
      "             ReLU-95          [-1, 256, 32, 32]               0\n",
      "             ReLU-96          [-1, 256, 32, 32]               0\n",
      "           Conv2d-97          [-1, 256, 32, 32]         589,824\n",
      "           Conv2d-98          [-1, 256, 32, 32]         589,824\n",
      "      BatchNorm2d-99          [-1, 256, 32, 32]             512\n",
      "     BatchNorm2d-100          [-1, 256, 32, 32]             512\n",
      "            ReLU-101          [-1, 256, 32, 32]               0\n",
      "            ReLU-102          [-1, 256, 32, 32]               0\n",
      "      BasicBlock-103          [-1, 256, 32, 32]               0\n",
      "      BasicBlock-104          [-1, 256, 32, 32]               0\n",
      "          Conv2d-105          [-1, 512, 16, 16]       1,179,648\n",
      "          Conv2d-106          [-1, 512, 16, 16]       1,179,648\n",
      "     BatchNorm2d-107          [-1, 512, 16, 16]           1,024\n",
      "     BatchNorm2d-108          [-1, 512, 16, 16]           1,024\n",
      "            ReLU-109          [-1, 512, 16, 16]               0\n",
      "            ReLU-110          [-1, 512, 16, 16]               0\n",
      "          Conv2d-111          [-1, 512, 16, 16]       2,359,296\n",
      "          Conv2d-112          [-1, 512, 16, 16]       2,359,296\n",
      "     BatchNorm2d-113          [-1, 512, 16, 16]           1,024\n",
      "     BatchNorm2d-114          [-1, 512, 16, 16]           1,024\n",
      "          Conv2d-115          [-1, 512, 16, 16]         131,072\n",
      "          Conv2d-116          [-1, 512, 16, 16]         131,072\n",
      "     BatchNorm2d-117          [-1, 512, 16, 16]           1,024\n",
      "     BatchNorm2d-118          [-1, 512, 16, 16]           1,024\n",
      "            ReLU-119          [-1, 512, 16, 16]               0\n",
      "            ReLU-120          [-1, 512, 16, 16]               0\n",
      "      BasicBlock-121          [-1, 512, 16, 16]               0\n",
      "      BasicBlock-122          [-1, 512, 16, 16]               0\n",
      "          Conv2d-123          [-1, 512, 16, 16]       2,359,296\n",
      "          Conv2d-124          [-1, 512, 16, 16]       2,359,296\n",
      "     BatchNorm2d-125          [-1, 512, 16, 16]           1,024\n",
      "     BatchNorm2d-126          [-1, 512, 16, 16]           1,024\n",
      "            ReLU-127          [-1, 512, 16, 16]               0\n",
      "            ReLU-128          [-1, 512, 16, 16]               0\n",
      "          Conv2d-129          [-1, 512, 16, 16]       2,359,296\n",
      "          Conv2d-130          [-1, 512, 16, 16]       2,359,296\n",
      "     BatchNorm2d-131          [-1, 512, 16, 16]           1,024\n",
      "     BatchNorm2d-132          [-1, 512, 16, 16]           1,024\n",
      "            ReLU-133          [-1, 512, 16, 16]               0\n",
      "            ReLU-134          [-1, 512, 16, 16]               0\n",
      "      BasicBlock-135          [-1, 512, 16, 16]               0\n",
      "      BasicBlock-136          [-1, 512, 16, 16]               0\n",
      "          Conv2d-137          [-1, 512, 16, 16]         262,656\n",
      "            ReLU-138          [-1, 512, 16, 16]               0\n",
      "        Upsample-139          [-1, 512, 32, 32]               0\n",
      "          Conv2d-140          [-1, 256, 32, 32]          65,792\n",
      "            ReLU-141          [-1, 256, 32, 32]               0\n",
      "          Conv2d-142          [-1, 512, 32, 32]       3,539,456\n",
      "            ReLU-143          [-1, 512, 32, 32]               0\n",
      "        Upsample-144          [-1, 512, 64, 64]               0\n",
      "          Conv2d-145          [-1, 128, 64, 64]          16,512\n",
      "            ReLU-146          [-1, 128, 64, 64]               0\n",
      "          Conv2d-147          [-1, 256, 64, 64]       1,474,816\n",
      "            ReLU-148          [-1, 256, 64, 64]               0\n",
      "        Upsample-149        [-1, 256, 128, 128]               0\n",
      "          Conv2d-150         [-1, 64, 128, 128]           4,160\n",
      "            ReLU-151         [-1, 64, 128, 128]               0\n",
      "          Conv2d-152        [-1, 256, 128, 128]         737,536\n",
      "            ReLU-153        [-1, 256, 128, 128]               0\n",
      "        Upsample-154        [-1, 256, 256, 256]               0\n",
      "          Conv2d-155         [-1, 64, 256, 256]           4,160\n",
      "            ReLU-156         [-1, 64, 256, 256]               0\n",
      "          Conv2d-157        [-1, 128, 256, 256]         368,768\n",
      "            ReLU-158        [-1, 128, 256, 256]               0\n",
      "        Upsample-159        [-1, 128, 512, 512]               0\n",
      "          Conv2d-160         [-1, 64, 512, 512]         110,656\n",
      "            ReLU-161         [-1, 64, 512, 512]               0\n",
      "          Conv2d-162          [-1, 6, 512, 512]             390\n",
      "================================================================\n",
      "Total params: 28,976,646\n",
      "Trainable params: 28,976,646\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 3.00\n",
      "Forward/backward pass size (MB): 2182.00\n",
      "Params size (MB): 110.54\n",
      "Estimated Total Size (MB): 2295.54\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = ResNetUNet(n_class=6)\n",
    "model = model.to(device)\n",
    "\n",
    "# check keras-like model summary using torchsummary\n",
    "from torchsummary import summary\n",
    "summary(model, input_size=(3, 224, 224))\n",
    "# summary(model, input_size=(3, 512, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import torch.nn.functional as F\n",
    "from Unet.loss import dice_loss\n",
    "\n",
    "def calc_loss(pred, target, metrics, bce_weight=0.5):\n",
    "    bce = F.binary_cross_entropy_with_logits(pred, target)\n",
    "\n",
    "    pred = F.sigmoid(pred)\n",
    "    dice = dice_loss(pred, target)\n",
    "\n",
    "    loss = bce * bce_weight + dice * (1 - bce_weight)\n",
    "\n",
    "    metrics['bce'] += bce.data.cpu().numpy() * target.size(0)\n",
    "    metrics['dice'] += dice.data.cpu().numpy() * target.size(0)\n",
    "    metrics['loss'] += loss.data.cpu().numpy() * target.size(0)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def print_metrics(metrics, epoch_samples, phase):\n",
    "    outputs = []\n",
    "    for k in metrics.keys():\n",
    "        outputs.append(\"{}: {:4f}\".format(k, metrics[k] / epoch_samples))\n",
    "\n",
    "    print(\"{}: {}\".format(phase, \", \".join(outputs)))\n",
    "\n",
    "def train_model(model, optimizer, scheduler, num_epochs=25):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 1e10\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        since = time.time()\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    print(\"LR\", param_group['lr'])\n",
    "\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            metrics = defaultdict(float)\n",
    "            epoch_samples = 0\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = calc_loss(outputs, labels, metrics)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                epoch_samples += inputs.size(0)\n",
    "\n",
    "            print_metrics(metrics, epoch_samples, phase)\n",
    "            epoch_loss = metrics['loss'] / epoch_samples\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                print(\"saving best model\")\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print('{:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    print('Best val loss: {:4f}'.format(best_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Epoch 0/59\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andy.knapper\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:100: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andy.knapper\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-d51f522a29b9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mexp_lr_scheduler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStepLR\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer_ft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer_ft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexp_lr_scheduler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-14-17fc489d1e47>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[0;32m     59\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphase\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m                     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalc_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m                     \u001b[1;31m# backward + optimize only if in training phase\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-17fc489d1e47>\u001b[0m in \u001b[0;36mcalc_loss\u001b[1;34m(pred, target, metrics, bce_weight)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbce\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mbce_weight\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdice\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbce_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mmetrics\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'bce'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mbce\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mmetrics\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'dice'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mdice\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mmetrics\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "import copy\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "num_class = 6\n",
    "model = ResNetUNet(num_class).to(device)\n",
    "\n",
    "# freeze backbone layers\n",
    "#for l in model.base_layers:\n",
    "#    for param in l.parameters():\n",
    "#        param.requires_grad = False\n",
    "\n",
    "optimizer_ft = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=30, gamma=0.1)\n",
    "\n",
    "model = train_model(model, optimizer_ft, exp_lr_scheduler, num_epochs=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1500"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amt_range = [-1500, 0]\n",
    "min_amt, max_amt = amt_range\n",
    "\n",
    "min_amt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_lst, a_lst, g_lst, t_lst = synthetic_data.make_a_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_arr = np.array(d_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0, 14, 21,  6, 16, 17, 19, 26, 41, 11, 50, 28, 24, 10,  4])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([115, 135, 167, 140, 160, 160, 130, 143, 160, 175, 137, 131, 142,\n",
       "       147, 188], dtype=int64)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_idx = np.array([(np.abs(np.arange(-1500,0, 1500/212) - x)).argmin() for x in a_lst])\n",
    "a_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datum = np.zeros((212, 212,3))\n",
    "test_datum[a_idx, d_arr, :] = 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n",
       "       255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n",
       "       255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n",
       "       255., 255., 255., 255., 255., 255., 255.])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_datum[a_idx, d_arr,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_target = np.zeros((2, 212, 212))\n",
    "\n",
    "for t_type in [0,1]:\n",
    "    t_type_mask = np.array([t == t_type for t in t_lst])\n",
    "    \n",
    "    t_type_a_idx = a_idx[t_type_mask]\n",
    "    t_type_d_arr = d_arr[t_type_mask]\n",
    "    \n",
    "    test_target[t_type,t_type_a_idx,t_type_d_arr] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_target[0, a_idx, d_arr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_target[1, a_idx, d_arr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'data_utils.group_to_image_constructors' from 'C:\\\\Users\\\\andy.knapper\\\\Documents\\\\OW\\\\Categorisation\\\\ML grouping\\\\GNN-for-trans-grouping\\\\data_utils\\\\group_to_image_constructors.py'>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(group_to_image_constructors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   7,  14,  21,  28,  42,  49,  56,  63,  70,  77,  84,  91,\n",
       "        98, 105, 112, 119, 126,  51,  80,  39,  58,  39,  31,  76,  90,\n",
       "        49,  57,  80,  75])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_day_arr, inp_amt_arr, inp_group_arr, inp_type_arr = np.array(synthetic_data.make_a_group())\n",
    "\n",
    "img_size = 212\n",
    "amt_range = [-1500, 0]\n",
    "\n",
    "min_amt, max_amt = amt_range\n",
    "amt_spread = max_amt - min_amt\n",
    "\n",
    "a_idx = np.array([(np.abs(np.arange(min_amt,max_amt, amt_spread/img_size) - x)).argmin() for x in inp_amt_arr])\n",
    "\n",
    "out_img = np.zeros((img_size, img_size,3))\n",
    "\n",
    "inp_day_arr.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, target = group_to_image_constructors.make_an_image(*np.array(synthetic_data.make_a_group()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(212, 212, 3)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 212, 212)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
