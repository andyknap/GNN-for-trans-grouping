{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Unet CNN Pixel classification model for grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import timeit\n",
    "import statistics\n",
    "import time\n",
    "import torch\n",
    "import torch_geometric\n",
    "import importlib\n",
    "\n",
    "from data_utils import synthetic_data\n",
    "from data_utils import graph_constructors\n",
    "from data_utils import group_to_image_constructors\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import importlib\n",
    "\n",
    "from Unet import helper\n",
    "from Unet import simulation\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(synthetic_data)\n",
    "importlib.reload(graph_constructors)\n",
    "importlib.reload(group_to_image_constructors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colour_list = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'tab:brown', 'tab:pink', 'tab:gray', 'tab:olive', 'tab:cyan']\n",
    "\n",
    "for i in range(2):\n",
    "    d_lst, a_lst, g_lst, t_lst = synthetic_data.make_a_group()\n",
    "    \n",
    "    d_arr = np.array(d_lst)\n",
    "    a_arr = np.array(a_lst)\n",
    "    g_arr = np.array(g_lst)    \n",
    "    \n",
    "    fig, (ax1) = plt.subplots(nrows=1, ncols=1, figsize=(10,8), sharex=True)\n",
    "    for g in g_lst:\n",
    "        mask = (g_arr == g)\n",
    "        \n",
    "        ax1.scatter(d_arr[mask], a_arr[mask], s=10, c=colour_list[g%10], marker='x')\n",
    "        ax1.set_title(str(i))\n",
    "        #ax1.legend(loc=\"upper right\")\n",
    "    \n",
    "    for ax1 in fig.axes:\n",
    "        matplotlib.pyplot.sca(ax1)\n",
    "        plt.xticks(rotation=90)\n",
    "        #plt.legend(bbox_to_anchor=(1.02, 1), loc=2, borderaxespad=0.)\n",
    "    \n",
    "    plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_lst, a_lst, g_lst = synthetic_data.make_a_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_arr = np.array(d_lst)\n",
    "a_arr = np.array(a_lst)\n",
    "g_arr = np.array(g_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_a_arr = graph_constructors.normalise_amounts(a_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unet demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Generate some random images\n",
    "input_images, target_masks = simulation.generate_random_data(192, 192, count=3)\n",
    "\n",
    "for x in [input_images, target_masks]:\n",
    "    print(x.shape)\n",
    "    print(x.min(), x.max())\n",
    "\n",
    "# Change channel-order and make 3 channels for matplot\n",
    "input_images_rgb = [x.astype(np.uint8) for x in input_images]\n",
    "\n",
    "# Map each channel (i.e. class) to each color\n",
    "target_masks_rgb = [helper.masks_to_colorimg(x) for x in target_masks]\n",
    "\n",
    "# Left: Input image (black and white), Right: Target mask (6ch)\n",
    "helper.plot_side_by_side([input_images_rgb, target_masks_rgb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets, models\n",
    "\n",
    "class SimDataset(Dataset):\n",
    "    def __init__(self, count, transform=None):\n",
    "        self.input_images, self.target_masks = simulation.generate_random_data(192, 192, count=count)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.input_images[idx]\n",
    "        mask = self.target_masks[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return [image, mask]\n",
    "\n",
    "# use the same transformations for train/val in this example\n",
    "trans = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # imagenet\n",
    "])\n",
    "\n",
    "train_set = SimDataset(2000, transform = trans)\n",
    "val_set = SimDataset(200, transform = trans)\n",
    "\n",
    "image_datasets = {\n",
    "    'train': train_set, 'val': val_set\n",
    "}\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "dataloaders_orig = {\n",
    "    'train': DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0),\n",
    "    'val': DataLoader(val_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "192/8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders_orig['train'].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders_orig['train'].dataset.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders_orig['train'].dataset.input_images.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders_orig['val'].dataset.input_images.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders_orig['val'].dataset.input_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders_orig['val'].dataset.target_masks.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### my dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets, models\n",
    "\n",
    "class SimDataset(Dataset):\n",
    "    def __init__(self, count, transform=None):\n",
    "        self.sim_data = [group_to_image_constructors.make_an_image(*np.array(synthetic_data.make_a_group())) for _ in range(count)]\n",
    "        self.input_images = np.array([x[0] for  x in self.sim_data]).astype('uint8')\n",
    "        self.target_masks = np.array([x[1] for  x in self.sim_data]).astype('float32')\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.input_images[idx]\n",
    "        mask = self.target_masks[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return [image, mask]\n",
    "\n",
    "# use the same transformations for train/val in this example\n",
    "trans = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # imagenet\n",
    "])\n",
    "\n",
    "train_set = SimDataset(2000, transform = trans)\n",
    "val_set = SimDataset(200, transform = trans)\n",
    "\n",
    "image_datasets = {\n",
    "    'train': train_set, 'val': val_set\n",
    "}\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "dataloaders = {\n",
    "    'train': DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0),\n",
    "    'val': DataLoader(val_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 224, 224]) torch.Size([4, 2, 224, 224])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f3134b81290>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD8CAYAAAB3lxGOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAOk0lEQVR4nO3df8ydZX3H8fdHFJKpCaBCSKlrIdUMzVKBIIlK3A8VyGJhia5kmY0jqyaQaOKSVU02sv/mRBOjw9RILIsD3RRpjE6bxuj+GErRWsAKFKxS2rQTF2DTqIXv/jjXM4/PD/r0Oef0nCfX+5WcnPtc933O/T057af3fZ/T65uqQlK/njftAiRNlyEgdc4QkDpnCEidMwSkzhkCUucmFgJJrkzyYJIDSbZNaj+SRpNJ/E4gyWnAQ8CbgEPAPcB1VfWDse9M0kgmdSRwGXCgqh6tql8BdwCbJrQvSSN4/oRedw3w2NDjQ8Brl9o4iT9blCbvp1X1svmDkwqBLDL2W3/Rk2wFtk5o/5IW+vFig5MKgUPA2qHH5wOHhzeoqu3AdvBIQJqmSV0TuAfYkGR9ktOBzcDOCe1L0ggmciRQVceT3Ah8DTgNuLWqHpjEviSNZiJfEZ50EZ4OSKfCvVV16fxBfzEodc4QkDpnCEidMwSkzhkCUucMAalzhoDUOUNA6pwhIHXOEJA6ZwhInTMEpM4ZAlLnDAGpc4aA1LkVh0CStUm+kWR/kgeSvKeN35Tk8SR72+3q8ZUradxGmVnoOPC+qvpukhcD9ybZ1dZ9tKo+PHp5kiZtxSFQVUeAI2356ST7GUw1LmkVGcs1gSTrgNcA325DNybZl+TWJGeNYx+SJmPkEEjyIuALwHur6ingFuBCYCODI4Wbl3je1iR7kuwZtQZJKzfSRKNJXgB8GfhaVX1kkfXrgC9X1atP8DpONCpN3ngnGk0S4NPA/uEASHLe0GbXAvevdB+SJm+UbwdeB/wFcF+SvW3sA8B1STYyaDt2EHjXSBVKmij7Dkj9sO+ApIUMAalzhoDUOUNA6pwhIHXOEJA6ZwhInTMEpM4ZAlLnDAGpc4aA1DlDQOqcISB1zhCQOmcISJ0zBKTOjTKzEABJDgJPA88Ax6vq0iRnA58D1jGYXejtVfXfo+5L0viN60jgD6pq49CsJduA3VW1AdjdHkuaQZM6HdgE7GjLO4BrJrQfSSMaRwgU8PUk9ybZ2sbObR2K5joVnTP/SfYdkGbDyNcEgNdV1eEk5wC7kvxwOU+qqu3AdnCiUWmaRj4SqKrD7f4YcCdwGXB0rv9Auz826n4kTcZIIZDkha0jMUleCLyZQbORncCWttkW4K5R9iNpckY9HTgXuHPQjIjnA/9SVf+e5B7g80muB34CvG3E/UiaEJuPSP2w+YikhQwBqXOGgNQ5Q0DqnCEgdc4QkDpnCEidMwSkzhkCUucMAalzhoDUOUNA6pwhIHXOEJA6ZwhInVvxpCJJXsmgt8CcC4C/Bc4E/gr4rzb+gar6yoorlDRRY5lUJMlpwOPAa4F3Av9TVR8+iec7qYg0eROdVOSPgEeq6sdjej1Jp8i4QmAzcPvQ4xuT7Etya5KzxrQPSRMwcggkOR14K/CvbegW4EJgI3AEuHmJ59l8RJoBI18TSLIJuKGq3rzIunXAl6vq1Sd4Da8JSJM3sWsC1zF0KjDXdKS5lkEfAkkzaqS+A0l+B3gT8K6h4Q8l2cigR+HBeeskzRj7Dkj9sO+ApIUMAalzhoDUOUNA6pwhIHXOEJA6ZwhInTMEpM4ZAlLnDAGpc4aA1DlDQOqcISB1zhCQOmcISJ1bVgi0CUOPJbl/aOzsJLuSPNzuz2rjSfKxJAfaZKMXT6p4SaNb7pHAZ4Ar541tA3ZX1QZgd3sMcBWwod22Mph4VNKMWlYIVNW3gJ/NG94E7GjLO4BrhsZvq4G7gTPnzTsoaYaMck3g3Ko6AtDuz2nja4DHhrY71MYkzaCRJhpdQhYZWzCHYJKtDE4XJE3RKEcCR+cO89v9sTZ+CFg7tN35wOH5T66q7VV16WITH0o6dUYJgZ3Alra8BbhraPwd7VuCy4En504bJM2gqjrhjUFzkSPArxn8S3898BIG3wo83O7PbtsG+ATwCHAfcOkyXr+8efM28duexf7+2XdA6od9ByQtZAhInTMEpM4ZAlLnDAGpc4aA1DlDQOqcISB1zhCQOmcISJ0zBKTOGQJS5wwBqXOGgNQ5Q0DqnCEgde6EIbBE45F/TPLD1lzkziRntvF1SX6RZG+7fXKSxUsa3XKOBD7DwsYju4BXV9XvAw8B7x9a90hVbWy3d4+nTEmTcsIQWKzxSFV9vaqOt4d3M5hRWNIqNI5rAn8JfHXo8fok30vyzSRvWOpJSbYm2ZNkzxhqkLRCIzUfSfJB4Djw2TZ0BHh5VT2R5BLgS0leVVVPzX9uVW0HtrfXcaJRaUpWfCSQZAvwJ8Cf19y84VW/rKon2vK9DKYdf8U4CpU0GSsKgSRXAn8DvLWqfj40/rIkp7XlCxh0Jn50HIVKmowTng4kuR14I/DSJIeAv2PwbcAZwK4kAHe3bwKuAP4+yXHgGeDdVTW/m7GkGWLzEakfNh+RtJAhIHXOEJA6ZwhInTMEpM4ZAlLnDAGpc4aA1DlDQOqcISB1zhCQOmcISJ0zBKTOGQJS5wwBqXMr7TtwU5LHh/oLXD207v1JDiR5MMlbJlW4pPFYad8BgI8O9Rf4CkCSi4DNwKvac/5pbroxSbNpRX0HnsMm4I424eiPgAPAZSPUJ2nCRrkmcGNrQ3ZrkrPa2BrgsaFtDrWxBew7IM2GlYbALcCFwEYGvQZubuNZZNtF5w+squ1Vdelic55JOnVWFAJVdbSqnqmqZ4FP8ZtD/kPA2qFNzwcOj1aipElaad+B84YeXgvMfXOwE9ic5Iwk6xn0HfjOaCVKmqSV9h14Y5KNDA71DwLvAqiqB5J8HvgBg/ZkN1TVM5MpXdI42HdA6od9ByQtZAhInTMEpM4ZAlLnDAGpc4aA1DlDQOqcISB1zhCQOmcISJ0zBKTOGQJS5wwBqXOGgNQ5Q0Dq3Er7DnxuqOfAwSR72/i6JL8YWvfJSRYvaXQnnFmIQd+BjwO3zQ1U1Z/NLSe5GXhyaPtHqmrjuAqUNFknDIGq+laSdYutSxLg7cAfjrcsSafKqNcE3gAcraqHh8bWJ/lekm8mecOIry9pwpZzOvBcrgNuH3p8BHh5VT2R5BLgS0leVVVPzX9ikq3A1hH3L2lEKz4SSPJ84E+Bz82NtfZjT7Tle4FHgFcs9nybj0izYZTTgT8GflhVh+YGkrxsrgFpkgsY9B14dLQSJU3Scr4ivB34T+CVSQ4lub6t2sxvnwoAXAHsS/J94N+Ad1fVcpuZSpoC+w5I/bDvgKSFDAGpc4aA1DlDQOqcISB1zhCQOmcIdOLZJb4KXmr8VK+blTp6ZAh04nnJSY2f6nWzUkePDAGpc4aA1DlDQOqcISB1biZC4JJLLlly3Wq+0jwrdUjPxf9FKPXD/0UoaaHlTCqyNsk3kuxP8kCS97Txs5PsSvJwuz+rjSfJx5IcSLIvycWTfhOSVm45RwLHgfdV1e8BlwM3JLkI2AbsrqoNwO72GOAqBtOKbWAwkegtY69a0ticMASq6khVfbctPw3sB9YAm4AdbbMdwDVteRNwWw3cDZyZ5LyxVy5pLE7qmkBrQvIa4NvAuVV1BAZBAZzTNlsDPDb0tENtTNIMWnbfgSQvAr4AvLeqnsrSv71ebMWCq//2HZBmw7KOBJK8gEEAfLaqvtiGj84d5rf7Y238ELB26OnnA4fnv6Z9B6TZsJxvBwJ8GthfVR8ZWrUT2NKWtwB3DY2/o31LcDnw5Nxpg6TZc8IfCyV5PfAfwH3As234AwyuC3weeDnwE+BtVfWzFhofB64Efg68s6r2nGAf/lhImrxFfyzkLwalfviLQUkLGQJS5wwBqXOGgNQ5Q0DqnCEgdc4QkDpnCEidMwSkzhkCUucMAalzhoDUOUNA6pwhIHXOEJA6ZwhInTMEpM4ZAlLnlj3l+IT9FPjfdr9avZTVXT+s/vew2uuHyb6H311scCbmGARIsmc1Tz++2uuH1f8eVnv9MJ334OmA1DlDQOrcLIXA9mkXMKLVXj+s/vew2uuHKbyHmbkmIGk6ZulIQNIUTD0EklyZ5MEkB5Jsm3Y9y5XkYJL7kuxNsqeNnZ1kV5KH2/1Z065zWJJbkxxLcv/Q2KI1t16SH2ufy74kF0+v8v+vdbH6b0ryePsc9ia5emjd+1v9DyZ5y3Sq/o0ka5N8I8n+JA8keU8bn+5nUFVTuwGnAY8AFwCnA98HLppmTSdR+0HgpfPGPgRsa8vbgH+Ydp3z6rsCuBi4/0Q1A1cDX2XQav5y4NszWv9NwF8vsu1F7c/TGcD69ufstCnXfx5wcVt+MfBQq3Oqn8G0jwQuAw5U1aNV9SvgDmDTlGsaxSZgR1veAVwzxVoWqKpvAT+bN7xUzZuA22rgbuDMuVb007JE/UvZBNxRVb+sqh8BBxj8eZuaqjpSVd9ty08D+4E1TPkzmHYIrAEeG3p8qI2tBgV8Pcm9Sba2sXOrtWFv9+dMrbrlW6rm1fTZ3NgOl28dOgWb6fqTrANew6C791Q/g2mHQBYZWy1fV7yuqi4GrgJuSHLFtAsas9Xy2dwCXAhsBI4AN7fxma0/yYuALwDvraqnnmvTRcbG/h6mHQKHgLVDj88HDk+plpNSVYfb/THgTgaHmkfnDtfa/bHpVbhsS9W8Kj6bqjpaVc9U1bPAp/jNIf9M1p/kBQwC4LNV9cU2PNXPYNohcA+wIcn6JKcDm4GdU67phJK8MMmL55aBNwP3M6h9S9tsC3DXdCo8KUvVvBN4R7tCfTnw5Nwh6yyZd458LYPPAQb1b05yRpL1wAbgO6e6vmFJAnwa2F9VHxlaNd3PYJpXS4eugD7E4OrtB6ddzzJrvoDBlefvAw/M1Q28BNgNPNzuz552rfPqvp3BIfOvGfwrc/1SNTM4FP1E+1zuAy6d0fr/udW3r/2lOW9o+w+2+h8ErpqB+l/P4HB+H7C33a6e9mfgLwalzk37dEDSlBkCUucMAalzhoDUOUNA6pwhIHXOEJA6ZwhInfs/G7V9H9fuLSQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torchvision.utils\n",
    "\n",
    "def reverse_transform(inp):\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    inp = (inp * 255).astype(np.uint8)\n",
    "\n",
    "    return inp\n",
    "\n",
    "# Get a batch of training data\n",
    "inputs, masks = next(iter(dataloaders['train']))\n",
    "\n",
    "print(inputs.shape, masks.shape)\n",
    "\n",
    "plt.imshow(reverse_transform(inputs[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "def convrelu(in_channels, out_channels, kernel, padding):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel, padding=padding),\n",
    "        nn.ReLU(inplace=True),\n",
    "    )\n",
    "\n",
    "\n",
    "class ResNetUNet(nn.Module):\n",
    "    def __init__(self, n_class):\n",
    "        super().__init__()\n",
    "\n",
    "        self.base_model = models.resnet18(pretrained=True)\n",
    "        self.base_layers = list(self.base_model.children())\n",
    "\n",
    "        self.layer0 = nn.Sequential(*self.base_layers[:3]) # size=(N, 64, x.H/2, x.W/2)\n",
    "        self.layer0_1x1 = convrelu(64, 64, 1, 0)\n",
    "        self.layer1 = nn.Sequential(*self.base_layers[3:5]) # size=(N, 64, x.H/4, x.W/4)\n",
    "        self.layer1_1x1 = convrelu(64, 64, 1, 0)\n",
    "        self.layer2 = self.base_layers[5]  # size=(N, 128, x.H/8, x.W/8)\n",
    "        self.layer2_1x1 = convrelu(128, 128, 1, 0)\n",
    "        self.layer3 = self.base_layers[6]  # size=(N, 256, x.H/16, x.W/16)\n",
    "        self.layer3_1x1 = convrelu(256, 256, 1, 0)\n",
    "        self.layer4 = self.base_layers[7]  # size=(N, 512, x.H/32, x.W/32)\n",
    "        self.layer4_1x1 = convrelu(512, 512, 1, 0)\n",
    "\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "        self.conv_up3 = convrelu(256 + 512, 512, 3, 1)\n",
    "        self.conv_up2 = convrelu(128 + 512, 256, 3, 1)\n",
    "        self.conv_up1 = convrelu(64 + 256, 256, 3, 1)\n",
    "        self.conv_up0 = convrelu(64 + 256, 128, 3, 1)\n",
    "\n",
    "        self.conv_original_size0 = convrelu(3, 64, 3, 1)\n",
    "        self.conv_original_size1 = convrelu(64, 64, 3, 1)\n",
    "        self.conv_original_size2 = convrelu(64 + 128, 64, 3, 1)\n",
    "\n",
    "        self.conv_last = nn.Conv2d(64, n_class, 1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x_original = self.conv_original_size0(input)\n",
    "        x_original = self.conv_original_size1(x_original)\n",
    "\n",
    "        layer0 = self.layer0(input)\n",
    "        layer1 = self.layer1(layer0)\n",
    "        layer2 = self.layer2(layer1)\n",
    "        layer3 = self.layer3(layer2)\n",
    "        layer4 = self.layer4(layer3)\n",
    "\n",
    "        layer4 = self.layer4_1x1(layer4)\n",
    "        x = self.upsample(layer4)\n",
    "        layer3 = self.layer3_1x1(layer3)\n",
    "        x = torch.cat([x, layer3], dim=1)\n",
    "        x = self.conv_up3(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer2 = self.layer2_1x1(layer2)\n",
    "        x = torch.cat([x, layer2], dim=1)\n",
    "        x = self.conv_up2(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer1 = self.layer1_1x1(layer1)\n",
    "        x = torch.cat([x, layer1], dim=1)\n",
    "        x = self.conv_up1(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer0 = self.layer0_1x1(layer0)\n",
    "        x = torch.cat([x, layer0], dim=1)\n",
    "        x = self.conv_up0(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, x_original], dim=1)\n",
    "        x = self.conv_original_size2(x)\n",
    "\n",
    "        out = self.conv_last(x)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = ResNetUNet(n_class=2)\n",
    "model = model.to(device)\n",
    "\n",
    "# check keras-like model summary using torchsummary\n",
    "# from torchsummary import summary\n",
    "# summary(model, input_size=(3, 224, 224))\n",
    "# summary(model, input_size=(3, 512, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import torch.nn.functional as F\n",
    "from Unet.loss import dice_loss\n",
    "\n",
    "def calc_loss(pred, target, metrics, bce_weight=0.5):\n",
    "    bce = F.binary_cross_entropy_with_logits(pred, target)\n",
    "\n",
    "    pred = F.sigmoid(pred)\n",
    "    dice = dice_loss(pred, target)\n",
    "\n",
    "    loss = bce * bce_weight + dice * (1 - bce_weight)\n",
    "\n",
    "    metrics['bce'] += bce.data.cpu().numpy() * target.size(0)\n",
    "    metrics['dice'] += dice.data.cpu().numpy() * target.size(0)\n",
    "    metrics['loss'] += loss.data.cpu().numpy() * target.size(0)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def print_metrics(metrics, epoch_samples, phase):\n",
    "    outputs = []\n",
    "    for k in metrics.keys():\n",
    "        outputs.append(\"{}: {:4f}\".format(k, metrics[k] / epoch_samples))\n",
    "\n",
    "    print(\"{}: {}\".format(phase, \", \".join(outputs)))\n",
    "\n",
    "def train_model(model, optimizer, scheduler, num_epochs=25):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 1e10\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        since = time.time()\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    print(\"LR\", param_group['lr'])\n",
    "\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            metrics = defaultdict(float)\n",
    "            epoch_samples = 0\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = calc_loss(outputs, labels, metrics)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                epoch_samples += inputs.size(0)\n",
    "\n",
    "            print_metrics(metrics, epoch_samples, phase)\n",
    "            epoch_loss = metrics['loss'] / epoch_samples\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                print(\"saving best model\")\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print('{:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    print('Best val loss: {:4f}'.format(best_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "import copy\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "num_class = 2\n",
    "model = ResNetUNet(num_class).to(device)\n",
    "\n",
    "# freeze backbone layers\n",
    "#for l in model.base_layers:\n",
    "#    for param in l.parameters():\n",
    "#        param.requires_grad = False\n",
    "\n",
    "optimizer_ft = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=30, gamma=0.1)\n",
    "\n",
    "model = train_model(model, optimizer_ft, exp_lr_scheduler, num_epochs=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://discuss.pytorch.org/t/save-and-load-model/6206/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, '/home/a/Documents/GNN-for-trans-grouping/GNN-for-trans-grouping/Unet_model_1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('/home/a/Documents/GNN-for-trans-grouping/GNN-for-trans-grouping/Unet_model_1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sim_data = [group_to_image_constructors.make_an_image(*np.array(synthetic_data.make_a_group())) for _ in range(3)]\n",
    "# input_images = np.array([x[0] for  x in sim_data]).astype('uint8')\n",
    "# target_masks = np.array([x[1] for  x in sim_data]).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# output = model(torch.tensor(test_img))\n",
    "# prediction = torch.argmax(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = SimDataset(2, transform = trans)\n",
    "val_set = SimDataset(2, transform = trans)\n",
    "\n",
    "image_datasets = {\n",
    "    'train': train_set, 'val': val_set\n",
    "}\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "dataloaders = {\n",
    "    'train': DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0),\n",
    "    'val': DataLoader(val_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.eval()\n",
    "for inputs, labels in dataloaders['train']:\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "    outputs = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = torch.nn.functional.softmax(outputs, dim=1)\n",
    "p_arr = p.cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_arr, d_arr = np.nonzero(np.max(reverse_transform(inputs[idx].cpu()), axis = 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_arr = (p_arr[idx, :, a_arr, d_arr][:,0]<0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(*args, **kw)>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "colour_list = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'tab:brown', 'tab:pink', 'tab:gray', 'tab:olive', 'tab:cyan']\n",
    "\n",
    "\n",
    "fig, (ax1) = plt.subplots(nrows=1, ncols=1, figsize=(10,8), sharex=True)\n",
    "for t in t_arr:\n",
    "    mask = (t_arr == t)\n",
    "    ax1.scatter(d_arr[mask], a_arr[mask], s=10, c=colour_list[t%10], marker='x')\n",
    "    \n",
    "for ax1 in fig.axes:\n",
    "    matplotlib.pyplot.sca(ax1)\n",
    "    plt.xticks(rotation=90)\n",
    "    #plt.legend(bbox_to_anchor=(1.02, 1), loc=2, borderaxespad=0.)\n",
    "    \n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
